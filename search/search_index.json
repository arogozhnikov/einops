{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#einops","title":"einops","text":"<p>Flexible and powerful tensor operations for readable and reliable code.  Supports numpy, pytorch, jax, mlx and others.</p>"},{"location":"#recent-updates","title":"Recent updates:","text":"<ul> <li>einops playground can run 2 of 4 example notebooks right in your browser</li> <li>0.8.2: MLX backend added</li> <li>0.8.0: tinygrad backend added, small fixes</li> <li>0.7.0: no-hassle <code>torch.compile</code>, support of array api standard and more</li> <li>10'000\ud83c\udf89: github reports that more than 10k project use einops</li> <li>einops 0.6.1: paddle backend added</li> <li>einops 0.6 introduces packing and unpacking</li> <li>einops 0.5: einsum is now a part of einops</li> <li>Einops paper is accepted for oral presentation at ICLR 2022 (yes, it worth reading).   Talk recordings are available</li> </ul> Previous updates <ul> <li>flax and oneflow backend added</li> <li>torch.jit.script is supported for pytorch layers</li> <li>powerful EinMix added to einops. Einmix tutorial notebook </li> </ul>"},{"location":"#tweets","title":"Tweets","text":"<p>In case you need convincing arguments for setting aside time to learn about einsum and einops... Tim Rockt\u00e4schel</p> <p>Writing better code with PyTorch and einops \ud83d\udc4c Andrej Karpathy</p> <p>Slowly but surely, einops is seeping in to every nook and cranny of my code. If you find yourself shuffling around bazillion dimensional tensors, this might change your life Nasim Rahaman</p> <p>More testimonials</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Installation</li> <li>Documentation</li> <li>Tutorial</li> <li>API micro-reference</li> <li>Why use einops</li> <li>Supported frameworks</li> <li>Citing</li> <li>Repository and discussions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Plain and simple:</p> <pre><code>pip install einops\n</code></pre> <p>(<code>uv pip install einops</code> works as well)</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Tutorials are the most convenient way to see <code>einops</code> in action</p> <ul> <li>part 1: einops fundamentals</li> <li>part 2: einops for deep learning</li> <li>part 3: packing and unpacking</li> <li>part 4: improve pytorch code with einops</li> </ul> <p>Kapil Sachdeva recorded a small intro to einops.</p>"},{"location":"#api","title":"API","text":"<p><code>einops</code> has a minimalistic yet powerful API.</p> <p>Three core operations provided (einops tutorial shows those cover stacking, reshape, transposition, squeeze/unsqueeze, repeat, tile, concatenate, view and numerous reductions)</p> <pre><code>from einops import rearrange, reduce, repeat\n# rearrange elements according to the pattern\noutput_tensor = rearrange(input_tensor, 't b c -&gt; b c t')\n# combine rearrangement and reduction\noutput_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -&gt; b h w c', 'mean', h2=2, w2=2)\n# copy along a new axis\noutput_tensor = repeat(input_tensor, 'h w -&gt; h w c', c=3)\n</code></pre> <p>Later additions to the family are <code>pack</code> and <code>unpack</code> functions (better than stack/split/concatenate):</p> <pre><code>from einops import pack, unpack\n# pack and unpack allow reversibly 'packing' multiple tensors into one.\n# Packed tensors may be of different dimensionality:\npacked,  ps = pack([class_token_bc, image_tokens_bhwc, text_tokens_btc], 'b * c')\nclass_emb_bc, image_emb_bhwc, text_emb_btc = unpack(transformer(packed), ps, 'b * c')\n</code></pre> <p>Finally, einops provides einsum with a support of multi-lettered names:</p> <pre><code>from einops import einsum, pack, unpack\n# einsum is like ... einsum, generic and flexible dot-product\n# but 1) axes can be multi-lettered  2) pattern goes last 3) works with multiple frameworks\nC = einsum(A, B, 'b t1 head c, b t2 head c -&gt; b head t1 t2')\n</code></pre>"},{"location":"#einmix","title":"EinMix","text":"<p><code>EinMix</code> is a generic linear layer, perfect for MLP Mixers and similar architectures.</p>"},{"location":"#layers","title":"Layers","text":"<p>Einops provides layers (<code>einops</code> keeps a separate version for each framework) that reflect corresponding functions</p> <pre><code>from einops.layers.torch      import Rearrange, Reduce\nfrom einops.layers.tensorflow import Rearrange, Reduce\nfrom einops.layers.flax       import Rearrange, Reduce\nfrom einops.layers.paddle     import Rearrange, Reduce\n</code></pre> Example of using layers within a pytorch model <p>Example given for pytorch, but code in other frameworks is almost identical</p> <pre><code>from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Rearrange\n\nmodel = Sequential(\n    ...,\n    Conv2d(6, 16, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    # flattening without need to write forward\n    Rearrange('b c h w -&gt; b (c h w)'),\n    Linear(16*5*5, 120),\n    ReLU(),\n    Linear(120, 10),\n)\n</code></pre> <p>No more flatten needed!</p> <p>Additionally, torch layers as those are script-able and compile-able. Operations are torch.compile-able,  but not script-able due to limitations of torch.jit.script.</p>"},{"location":"#naming","title":"Naming","text":"<p><code>einops</code> stands for Einstein-Inspired Notation for operations  (though \"Einstein operations\" is more attractive and easier to remember).</p> <p>Notation was loosely inspired by Einstein summation (in particular by <code>numpy.einsum</code> operation).</p>"},{"location":"#why-use-einops-notation","title":"Why use <code>einops</code> notation?!","text":""},{"location":"#semantic-information-being-verbose-in-expectations","title":"Semantic information (being verbose in expectations)","text":"<pre><code>y = x.view(x.shape[0], -1)\ny = rearrange(x, 'b c h w -&gt; b (c h w)')\n</code></pre> <p>While these two lines are doing the same job in some context, the second one provides information about the input and output. In other words, <code>einops</code> focuses on interface: what is the input and output, not how the output is computed.</p> <p>The next operation looks similar:</p> <pre><code>y = rearrange(x, 'time c h w -&gt; time (c h w)')\n</code></pre> <p>but it gives the reader a hint: this is not an independent batch of images we are processing, but rather a sequence (video).</p> <p>Semantic information makes the code easier to read and maintain.</p>"},{"location":"#convenient-checks","title":"Convenient checks","text":"<p>Reconsider the same example:</p> <pre><code>y = x.view(x.shape[0], -1) # x: (batch, 256, 19, 19)\ny = rearrange(x, 'b c h w -&gt; b (c h w)')\n</code></pre> <p>The second line checks that the input has four dimensions, but you can also specify particular dimensions. That's opposed to just writing comments about shapes since comments don't prevent mistakes, not tested, and without code review tend to be outdated</p> <pre><code>y = x.view(x.shape[0], -1) # x: (batch, 256, 19, 19)\ny = rearrange(x, 'b c h w -&gt; b (c h w)', c=256, h=19, w=19)\n</code></pre>"},{"location":"#result-is-strictly-determined","title":"Result is strictly determined","text":"<p>Below we have at least two ways to define the depth-to-space operation</p> <pre><code># depth-to-space\nrearrange(x, 'b c (h h2) (w w2) -&gt; b (c h2 w2) h w', h2=2, w2=2)\nrearrange(x, 'b c (h h2) (w w2) -&gt; b (h2 w2 c) h w', h2=2, w2=2)\n</code></pre> <p>There are at least four more ways to do it. Which one is used by the framework?</p> <p>These details are ignored, since usually it makes no difference, but it can make a big difference (e.g. if you use grouped convolutions in the next stage), and you'd like to specify this in your code.</p>"},{"location":"#uniformity","title":"Uniformity","text":"<pre><code>reduce(x, 'b c (x dx) -&gt; b c x', 'max', dx=2)\nreduce(x, 'b c (x dx) (y dy) -&gt; b c x y', 'max', dx=2, dy=3)\nreduce(x, 'b c (x dx) (y dy) (z dz) -&gt; b c x y z', 'max', dx=2, dy=3, dz=4)\n</code></pre> <p>These examples demonstrated that we don't use separate operations for 1d/2d/3d pooling, those are all defined in a uniform way.</p> <p>Space-to-depth and depth-to space are defined in many frameworks but how about width-to-height? Here you go:</p> <pre><code>rearrange(x, 'b c h (w w2) -&gt; b c (h w2) w', w2=2)\n</code></pre>"},{"location":"#framework-independent-behavior","title":"Framework independent behavior","text":"<p>Even simple functions are defined differently by different frameworks</p> <pre><code>y = x.flatten() # or flatten(x)\n</code></pre> <p>Suppose <code>x</code>'s shape was <code>(3, 4, 5)</code>, then <code>y</code> has shape ...</p> <ul> <li>numpy, pytorch, cupy, chainer, jax: <code>(60,)</code></li> <li>keras, tensorflow.layers, gluon: <code>(3, 20)</code></li> </ul> <p><code>einops</code> works the same way in all frameworks.</p>"},{"location":"#independence-of-framework-terminology","title":"Independence of framework terminology","text":"<p>Example: <code>tile</code> vs <code>repeat</code> causes lots of confusion. To copy image along width:</p> <pre><code>np.tile(image, (1, 2))    # in numpy\nimage.repeat(1, 2)        # pytorch's repeat ~ numpy's tile\n</code></pre> <p>With einops you don't need to decipher which axis was repeated:</p> <pre><code>repeat(image, 'h w -&gt; h (tile w)', tile=2)  # in numpy\nrepeat(image, 'h w -&gt; h (tile w)', tile=2)  # in pytorch\nrepeat(image, 'h w -&gt; h (tile w)', tile=2)  # in tf\nrepeat(image, 'h w -&gt; h (tile w)', tile=2)  # in jax\nrepeat(image, 'h w -&gt; h (tile w)', tile=2)  # in cupy\n... (etc.)\n</code></pre> <p>Testimonials provide users' perspective on the same question.</p>"},{"location":"#supported-frameworks","title":"Supported frameworks","text":"<p>Einops works with ...</p> <ul> <li>numpy</li> <li>pytorch</li> <li>tensorflow</li> <li>jax</li> <li>cupy</li> <li>flax (community)</li> <li>paddle (community)</li> <li>oneflow (community)</li> <li>tinygrad (community)</li> <li>pytensor (community)</li> </ul> <pre><code>from einops import rearrange  \n=&gt; from einops.array_api import rearrange\n</code></pre> <p>But actually it is even better: einops can be used with any framework that supports Python array API standard, to name a few:</p> <ul> <li>numpy &gt;= 2.0</li> <li>MLX  # yes, einops works with apple's framework</li> <li>pydata/sparse &gt;= 0.15 # and works with sparse tensors</li> <li>cubed # and with distributed tensors too</li> <li>quantco/ndonnx</li> <li>jax</li> <li>cupy</li> <li>dask is supported via array-api-compat</li> </ul>"},{"location":"#development","title":"Development","text":"<p>Devcontainer is provided, this environment can be used locally, or on your server, or within github codespaces.  To start with devcontainers in vs code, clone repo, and click 'Reopen in Devcontainer'. </p> <p>Starting from einops 0.8.1, einops distributes tests as a part of package.</p> <pre><code># pip install einops pytest\npython -m einops.tests.run_tests numpy pytorch jax --pip-install\n</code></pre> <p><code>numpy pytorch jax</code> is an example, any subset of testable frameworks can be provided. Every framework is tested against numpy, so it is a requirement for tests.</p> <p>Specifying <code>--pip-install</code> will install requirements in current virtualenv, and should be omitted if dependencies are installed locally.</p> <p>To build/test docs:</p> <pre><code>hatch run docs:serve  # Serving on http://localhost:8000/\n</code></pre>"},{"location":"#citing-einops","title":"Citing einops","text":"<p>Please use the following bibtex record</p> <pre><code>@inproceedings{\n    rogozhnikov2022einops,\n    title={Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},\n    author={Alex Rogozhnikov},\n    booktitle={International Conference on Learning Representations},\n    year={2022},\n    url={https://openreview.net/forum?id=oapKSVM2bcj}\n}\n</code></pre>"},{"location":"#supported-python-versions","title":"Supported python versions","text":"<p><code>einops</code> works with python 3.10 or later.</p>"},{"location":"1-einops-basics/","title":"Einops tutorial, part 1: basics","text":"In\u00a0[1]: Copied! <pre># we need some libraries for this demo\n%pip install einops numpy pillow -q\n</pre> # we need some libraries for this demo %pip install einops numpy pillow -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre># Examples are given for numpy. This code also setups ipython/jupyter/jupyterlite\n# so that numpy arrays in the output are displayed as images\nimport numpy\nfrom utils import display_np_arrays_as_images\n\ndisplay_np_arrays_as_images()\n</pre> # Examples are given for numpy. This code also setups ipython/jupyter/jupyterlite # so that numpy arrays in the output are displayed as images import numpy from utils import display_np_arrays_as_images  display_np_arrays_as_images() In\u00a0[3]: Copied! <pre>ims = numpy.load(\"./resources/test_images.npy\", allow_pickle=False)\n# There are 6 images of shape 96x96 with 3 color channels packed into tensor\nprint(ims.shape, ims.dtype)\n</pre> ims = numpy.load(\"./resources/test_images.npy\", allow_pickle=False) # There are 6 images of shape 96x96 with 3 color channels packed into tensor print(ims.shape, ims.dtype) <pre>(6, 96, 96, 3) float64\n</pre> In\u00a0[4]: Copied! <pre># display the first image (whole 4d tensor can't be rendered)\nims[0]\n</pre> # display the first image (whole 4d tensor can't be rendered) ims[0] Out[4]: In\u00a0[5]: Copied! <pre># second image in a batch\nims[1]\n</pre> # second image in a batch ims[1] Out[5]: In\u00a0[6]: Copied! <pre># we'll use three operations\nfrom einops import rearrange, reduce, repeat\n</pre> # we'll use three operations from einops import rearrange, reduce, repeat In\u00a0[7]: Copied! <pre># rearrange, as the name suggests, rearranges elements\n# below we swapped height and width.\n# In other words, transposed first two axes (dimensions)\nrearrange(ims[0], \"h w c -&gt; w h c\")\n</pre> # rearrange, as the name suggests, rearranges elements # below we swapped height and width. # In other words, transposed first two axes (dimensions) rearrange(ims[0], \"h w c -&gt; w h c\") Out[7]: In\u00a0[8]: Copied! <pre># we could use more verbose names for axes, and result is the same:\nrearrange(ims[0], \"height width color -&gt; width height color\")\n# when you operate on same set of axes many times,\n# you usually come up with short names.\n# That's what we do throughout tutorial - we'll use b (for batch), h, w, and c\n</pre> # we could use more verbose names for axes, and result is the same: rearrange(ims[0], \"height width color -&gt; width height color\") # when you operate on same set of axes many times, # you usually come up with short names. # That's what we do throughout tutorial - we'll use b (for batch), h, w, and c Out[8]: In\u00a0[9]: Copied! <pre># einops allows seamlessly composing batch and height to a new height dimension\n# We just rendered all images by collapsing to 3d tensor!\nrearrange(ims, \"b h w c -&gt; (b h) w c\")\n</pre> # einops allows seamlessly composing batch and height to a new height dimension # We just rendered all images by collapsing to 3d tensor! rearrange(ims, \"b h w c -&gt; (b h) w c\") Out[9]: In\u00a0[10]: Copied! <pre># or compose a new dimension of batch and width\nrearrange(ims, \"b h w c -&gt; h (b w) c\")\n</pre> # or compose a new dimension of batch and width rearrange(ims, \"b h w c -&gt; h (b w) c\") Out[10]: In\u00a0[11]: Copied! <pre># resulting dimensions are computed very simply\n# length of newly composed axis is a product of components\n# [6, 96, 96, 3] -&gt; [96, (6 * 96), 3]\nrearrange(ims, \"b h w c -&gt; h (b w) c\").shape\n</pre> # resulting dimensions are computed very simply # length of newly composed axis is a product of components # [6, 96, 96, 3] -&gt; [96, (6 * 96), 3] rearrange(ims, \"b h w c -&gt; h (b w) c\").shape Out[11]: <pre>(96, 576, 3)</pre> In\u00a0[12]: Copied! <pre># we can compose more than two axes.\n# let's flatten 4d array into 1d, resulting array has as many elements as the original\nrearrange(ims, \"b h w c -&gt; (b h w c)\").shape\n</pre> # we can compose more than two axes. # let's flatten 4d array into 1d, resulting array has as many elements as the original rearrange(ims, \"b h w c -&gt; (b h w c)\").shape Out[12]: <pre>(165888,)</pre> In\u00a0[13]: Copied! <pre># decomposition is the inverse process - represent an axis as a combination of new axes\n# several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3\nrearrange(ims, \"(b1 b2) h w c -&gt; b1 b2 h w c \", b1=2).shape\n</pre> # decomposition is the inverse process - represent an axis as a combination of new axes # several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3 rearrange(ims, \"(b1 b2) h w c -&gt; b1 b2 h w c \", b1=2).shape Out[13]: <pre>(2, 3, 96, 96, 3)</pre> In\u00a0[14]: Copied! <pre># finally, combine composition and decomposition:\nrearrange(ims, \"(b1 b2) h w c -&gt; (b1 h) (b2 w) c \", b1=2)\n</pre> # finally, combine composition and decomposition: rearrange(ims, \"(b1 b2) h w c -&gt; (b1 h) (b2 w) c \", b1=2) Out[14]: In\u00a0[15]: Copied! <pre># slightly different composition: b1 is merged with width, b2 with height\n# ... so letters are ordered by w then by h\nrearrange(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w) c \", b1=2)\n</pre> # slightly different composition: b1 is merged with width, b2 with height # ... so letters are ordered by w then by h rearrange(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w) c \", b1=2) Out[15]: In\u00a0[16]: Copied! <pre># move part of width dimension to height.\n# we should call this width-to-height as image width shrunk by 2 and height doubled.\n# but all pixels are the same!\n# Can you write reverse operation (height-to-width)?\nrearrange(ims, \"b h (w w2) c -&gt; (h w2) (b w) c\", w2=2)\n</pre> # move part of width dimension to height. # we should call this width-to-height as image width shrunk by 2 and height doubled. # but all pixels are the same! # Can you write reverse operation (height-to-width)? rearrange(ims, \"b h (w w2) c -&gt; (h w2) (b w) c\", w2=2) Out[16]: In\u00a0[17]: Copied! <pre># compare with the next example\nrearrange(ims, \"b h w c -&gt; h (b w) c\")\n</pre> # compare with the next example rearrange(ims, \"b h w c -&gt; h (b w) c\") Out[17]: In\u00a0[18]: Copied! <pre># order of axes in composition is different\n# rule is just as for digits in the number: leftmost digit is the most significant,\n# while neighboring numbers differ in the rightmost axis.\n\n# you can also think of this as lexicographic sort\nrearrange(ims, \"b h w c -&gt; h (w b) c\")\n</pre> # order of axes in composition is different # rule is just as for digits in the number: leftmost digit is the most significant, # while neighboring numbers differ in the rightmost axis.  # you can also think of this as lexicographic sort rearrange(ims, \"b h w c -&gt; h (w b) c\") Out[18]: In\u00a0[19]: Copied! <pre># what if b1 and b2 are reordered before composing to width?\nrearrange(ims, \"(b1 b2) h w c -&gt; h (b1 b2 w) c \", b1=2)  # produces 'einops'\nrearrange(ims, \"(b1 b2) h w c -&gt; h (b2 b1 w) c \", b1=2)  # produces 'eoipns'\n</pre> # what if b1 and b2 are reordered before composing to width? rearrange(ims, \"(b1 b2) h w c -&gt; h (b1 b2 w) c \", b1=2)  # produces 'einops' rearrange(ims, \"(b1 b2) h w c -&gt; h (b2 b1 w) c \", b1=2)  # produces 'eoipns' Out[19]: In\u00a0[20]: Copied! <pre># average over batch\nreduce(ims, \"b h w c -&gt; h w c\", \"mean\")\n</pre> # average over batch reduce(ims, \"b h w c -&gt; h w c\", \"mean\") Out[20]: In\u00a0[21]: Copied! <pre># the previous is identical to familiar:\nims.mean(axis=0)\n# but is so much more readable\n</pre> # the previous is identical to familiar: ims.mean(axis=0) # but is so much more readable Out[21]: In\u00a0[22]: Copied! <pre># Example of reducing of several axes\n# besides mean, there are also min, max, sum, prod\nreduce(ims, \"b h w c -&gt; h w\", \"min\")\n</pre> # Example of reducing of several axes # besides mean, there are also min, max, sum, prod reduce(ims, \"b h w c -&gt; h w\", \"min\") Out[22]: In\u00a0[23]: Copied! <pre># this is mean-pooling with 2x2 kernel\n# image is split into 2x2 patches, each patch is averaged\nreduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"mean\", h2=2, w2=2)\n</pre> # this is mean-pooling with 2x2 kernel # image is split into 2x2 patches, each patch is averaged reduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"mean\", h2=2, w2=2) Out[23]: In\u00a0[24]: Copied! <pre># max-pooling is similar\n# result is not as smooth as for mean-pooling\nreduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"max\", h2=2, w2=2)\n</pre> # max-pooling is similar # result is not as smooth as for mean-pooling reduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"max\", h2=2, w2=2) Out[24]: In\u00a0[25]: Copied! <pre># yet another example. Can you compute result shape?\nreduce(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w)\", \"mean\", b1=2)\n</pre> # yet another example. Can you compute result shape? reduce(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w)\", \"mean\", b1=2) Out[25]: In\u00a0[26]: Copied! <pre># rearrange can also take care of lists of arrays with the same shape\nx = list(ims)\nprint(type(x), \"with\", len(x), \"tensors of shape\", x[0].shape)\n# that's how we can stack inputs\n# \"list axis\" becomes first (\"b\" in this case), and we left it there\nrearrange(x, \"b h w c -&gt; b h w c\").shape\n</pre> # rearrange can also take care of lists of arrays with the same shape x = list(ims) print(type(x), \"with\", len(x), \"tensors of shape\", x[0].shape) # that's how we can stack inputs # \"list axis\" becomes first (\"b\" in this case), and we left it there rearrange(x, \"b h w c -&gt; b h w c\").shape <pre>&lt;class 'list'&gt; with 6 tensors of shape (96, 96, 3)\n</pre> Out[26]: <pre>(6, 96, 96, 3)</pre> In\u00a0[27]: Copied! <pre># but new axis can appear in the other place:\nrearrange(x, \"b h w c -&gt; h w c b\").shape\n</pre> # but new axis can appear in the other place: rearrange(x, \"b h w c -&gt; h w c b\").shape Out[27]: <pre>(96, 96, 3, 6)</pre> In\u00a0[28]: Copied! <pre># that's equivalent to numpy stacking, but written more explicitly\nnumpy.array_equal(rearrange(x, \"b h w c -&gt; h w c b\"), numpy.stack(x, axis=3))\n</pre> # that's equivalent to numpy stacking, but written more explicitly numpy.array_equal(rearrange(x, \"b h w c -&gt; h w c b\"), numpy.stack(x, axis=3)) Out[28]: <pre>True</pre> In\u00a0[29]: Copied! <pre># ... or we can concatenate along axes\nrearrange(x, \"b h w c -&gt; h (b w) c\").shape\n</pre> # ... or we can concatenate along axes rearrange(x, \"b h w c -&gt; h (b w) c\").shape Out[29]: <pre>(96, 576, 3)</pre> In\u00a0[30]: Copied! <pre># which is equivalent to concatenation\nnumpy.array_equal(rearrange(x, \"b h w c -&gt; h (b w) c\"), numpy.concatenate(x, axis=1))\n</pre> # which is equivalent to concatenation numpy.array_equal(rearrange(x, \"b h w c -&gt; h (b w) c\"), numpy.concatenate(x, axis=1)) Out[30]: <pre>True</pre> In\u00a0[31]: Copied! <pre>x = rearrange(ims, \"b h w c -&gt; b 1 h w 1 c\")  # functionality of numpy.expand_dims\nprint(x.shape)\nprint(rearrange(x, \"b 1 h w 1 c -&gt; b h w c\").shape)  # functionality of numpy.squeeze\n</pre> x = rearrange(ims, \"b h w c -&gt; b 1 h w 1 c\")  # functionality of numpy.expand_dims print(x.shape) print(rearrange(x, \"b 1 h w 1 c -&gt; b h w c\").shape)  # functionality of numpy.squeeze <pre>(6, 1, 96, 96, 1, 3)\n(6, 96, 96, 3)\n</pre> In\u00a0[32]: Copied! <pre># compute max in each image individually, then show a difference\nx = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims\nrearrange(x, \"b h w c -&gt; h (b w) c\")\n</pre> # compute max in each image individually, then show a difference x = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims rearrange(x, \"b h w c -&gt; h (b w) c\") Out[32]: In\u00a0[33]: Copied! <pre># repeat along a new axis. New axis can be placed anywhere\nrepeat(ims[0], \"h w c -&gt; h new_axis w c\", new_axis=5).shape\n</pre> # repeat along a new axis. New axis can be placed anywhere repeat(ims[0], \"h w c -&gt; h new_axis w c\", new_axis=5).shape Out[33]: <pre>(96, 5, 96, 3)</pre> In\u00a0[34]: Copied! <pre># shortcut\nrepeat(ims[0], \"h w c -&gt; h 5 w c\").shape\n</pre> # shortcut repeat(ims[0], \"h w c -&gt; h 5 w c\").shape Out[34]: <pre>(96, 5, 96, 3)</pre> In\u00a0[35]: Copied! <pre># repeat along w (existing axis)\nrepeat(ims[0], \"h w c -&gt; h (repeat w) c\", repeat=3)\n</pre> # repeat along w (existing axis) repeat(ims[0], \"h w c -&gt; h (repeat w) c\", repeat=3) Out[35]: In\u00a0[36]: Copied! <pre># repeat along two existing axes\nrepeat(ims[0], \"h w c -&gt; (2 h) (2 w) c\")\n</pre> # repeat along two existing axes repeat(ims[0], \"h w c -&gt; (2 h) (2 w) c\") Out[36]: In\u00a0[37]: Copied! <pre># order of axes matters as usual - you can repeat each element (pixel) 3 times\n# by changing order in parenthesis\nrepeat(ims[0], \"h w c -&gt; h (w repeat) c\", repeat=3)\n</pre> # order of axes matters as usual - you can repeat each element (pixel) 3 times # by changing order in parenthesis repeat(ims[0], \"h w c -&gt; h (w repeat) c\", repeat=3) Out[37]: <p>Note: <code>repeat</code> operation covers functionality identical to <code>numpy.repeat</code>, <code>numpy.tile</code> and actually more than that.</p> In\u00a0[38]: Copied! <pre>repeated = repeat(ims, \"b h w c -&gt; b h new_axis w c\", new_axis=2)\nreduced = reduce(repeated, \"b h new_axis w c -&gt; b h w c\", \"min\")\nassert numpy.array_equal(ims, reduced)\n</pre> repeated = repeat(ims, \"b h w c -&gt; b h new_axis w c\", new_axis=2) reduced = reduce(repeated, \"b h new_axis w c -&gt; b h w c\", \"min\") assert numpy.array_equal(ims, reduced) In\u00a0[39]: Copied! <pre># interweaving pixels of different pictures\n# all letters are observable\nrearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (w b2) c \", b1=2)\n</pre> # interweaving pixels of different pictures # all letters are observable rearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (w b2) c \", b1=2) Out[39]: In\u00a0[40]: Copied! <pre># interweaving along vertical for couples of images\nrearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (b2 w) c\", b1=2)\n</pre> # interweaving along vertical for couples of images rearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (b2 w) c\", b1=2) Out[40]: In\u00a0[41]: Copied! <pre># interweaving lines for couples of images\n# exercise: achieve the same result without einops in your favourite framework\nreduce(ims, \"(b1 b2) h w c -&gt; h (b2 w) c\", \"max\", b1=2)\n</pre> # interweaving lines for couples of images # exercise: achieve the same result without einops in your favourite framework reduce(ims, \"(b1 b2) h w c -&gt; h (b2 w) c\", \"max\", b1=2) Out[41]: In\u00a0[42]: Copied! <pre># color can be also composed into dimension\n# ... while image is downsampled\nreduce(ims, \"b (h 2) (w 2) c -&gt; (c h) (b w)\", \"mean\")\n</pre> # color can be also composed into dimension # ... while image is downsampled reduce(ims, \"b (h 2) (w 2) c -&gt; (c h) (b w)\", \"mean\") Out[42]: In\u00a0[43]: Copied! <pre># disproportionate resize\nreduce(ims, \"b (h 4) (w 3) c -&gt; (h) (b w)\", \"mean\")\n</pre> # disproportionate resize reduce(ims, \"b (h 4) (w 3) c -&gt; (h) (b w)\", \"mean\") Out[43]: In\u00a0[44]: Copied! <pre># spilt each image in two halves, compute mean of the two\nreduce(ims, \"b (h1 h2) w c -&gt; h2 (b w)\", \"mean\", h1=2)\n</pre> # spilt each image in two halves, compute mean of the two reduce(ims, \"b (h1 h2) w c -&gt; h2 (b w)\", \"mean\", h1=2) Out[44]: In\u00a0[45]: Copied! <pre># split in small patches and transpose each patch\nrearrange(ims, \"b (h1 h2) (w1 w2) c -&gt; (h1 w2) (b w1 h2) c\", h2=8, w2=8)\n</pre> # split in small patches and transpose each patch rearrange(ims, \"b (h1 h2) (w1 w2) c -&gt; (h1 w2) (b w1 h2) c\", h2=8, w2=8) Out[45]: In\u00a0[46]: Copied! <pre># stop me someone!\nrearrange(ims, \"b (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w2 h3) (b w1 h2 w3) c\", h2=2, w2=2, w3=2, h3=2)\n</pre> # stop me someone! rearrange(ims, \"b (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w2 h3) (b w1 h2 w3) c\", h2=2, w2=2, w3=2, h3=2) Out[46]: In\u00a0[47]: Copied! <pre>rearrange(ims, \"(b1 b2) (h1 h2) (w1 w2) c -&gt; (h1 b1 h2) (w1 b2 w2) c\", h1=3, w1=3, b2=3)\n</pre> rearrange(ims, \"(b1 b2) (h1 h2) (w1 w2) c -&gt; (h1 b1 h2) (w1 b2 w2) c\", h1=3, w1=3, b2=3) Out[47]: In\u00a0[48]: Copied! <pre># patterns can be arbitrarily complicated\nreduce(ims, \"(b1 b2) (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w1 h3) (b1 w2 h2 w3 b2) c\", \"mean\", h2=2, w1=2, w3=2, h3=2, b2=2)\n</pre> # patterns can be arbitrarily complicated reduce(ims, \"(b1 b2) (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w1 h3) (b1 w2 h2 w3 b2) c\", \"mean\", h2=2, w1=2, w3=2, h3=2, b2=2) Out[48]: In\u00a0[49]: Copied! <pre># subtract background in each image individually and normalize\n# pay attention to () - this is composition of 0 axis, a dummy axis with 1 element.\nim2 = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims\nim2 /= reduce(im2, \"b h w c -&gt; b () () c\", \"max\")\nrearrange(im2, \"b h w c -&gt; h (b w) c\")\n</pre> # subtract background in each image individually and normalize # pay attention to () - this is composition of 0 axis, a dummy axis with 1 element. im2 = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims im2 /= reduce(im2, \"b h w c -&gt; b () () c\", \"max\") rearrange(im2, \"b h w c -&gt; h (b w) c\") Out[49]: In\u00a0[50]: Copied! <pre># pixelate: first downscale by averaging, then upscale back using the same pattern\naveraged = reduce(ims, \"b (h h2) (w w2) c -&gt; b h w c\", \"mean\", h2=6, w2=8)\nrepeat(averaged, \"b h w c -&gt; (h h2) (b w w2) c\", h2=6, w2=8)\n</pre> # pixelate: first downscale by averaging, then upscale back using the same pattern averaged = reduce(ims, \"b (h h2) (w w2) c -&gt; b h w c\", \"mean\", h2=6, w2=8) repeat(averaged, \"b h w c -&gt; (h h2) (b w w2) c\", h2=6, w2=8) Out[50]: In\u00a0[51]: Copied! <pre>rearrange(ims, \"b h w c -&gt; w (b h) c\")\n</pre> rearrange(ims, \"b h w c -&gt; w (b h) c\") Out[51]: In\u00a0[52]: Copied! <pre># let's bring color dimension as part of horizontal axis\n# at the same time horizontal axis is downsampled by 2x\nreduce(ims, \"b (h h2) (w w2) c -&gt; (h w2) (b w c)\", \"mean\", h2=3, w2=3)\n</pre> # let's bring color dimension as part of horizontal axis # at the same time horizontal axis is downsampled by 2x reduce(ims, \"b (h h2) (w w2) c -&gt; (h w2) (b w c)\", \"mean\", h2=3, w2=3) Out[52]:"},{"location":"1-einops-basics/#einops-tutorial-part-1-basics","title":"Einops tutorial, part 1: basics\u00b6","text":""},{"location":"1-einops-basics/#welcome-to-einops-land","title":"Welcome to einops-land!\u00b6","text":"<p>We don't write</p> <pre>y = x.transpose(0, 2, 3, 1)\n</pre> <p>We write comprehensible code</p> <pre>y = rearrange(x, 'b c h w -&gt; b h w c')\n</pre> <p><code>einops</code> supports widely used tensor packages (such as <code>numpy</code>, <code>pytorch</code>, <code>jax</code>, <code>tensorflow</code>), and extends them.</p>"},{"location":"1-einops-basics/#whats-in-this-tutorial","title":"What's in this tutorial?\u00b6","text":"<ul> <li>fundamentals: reordering, composition and decomposition of axes</li> <li>operations: <code>rearrange</code>, <code>reduce</code>, <code>repeat</code></li> <li>how much you can do with a single operation!</li> </ul>"},{"location":"1-einops-basics/#preparations","title":"Preparations\u00b6","text":""},{"location":"1-einops-basics/#load-a-batch-of-images-to-play-with","title":"Load a batch of images to play with\u00b6","text":""},{"location":"1-einops-basics/#composition-of-axes","title":"Composition of axes\u00b6","text":"<p>transposition is very common and useful, but let's move to other capabilities provided by einops</p>"},{"location":"1-einops-basics/#decomposition-of-axis","title":"Decomposition of axis\u00b6","text":""},{"location":"1-einops-basics/#order-of-axes-matters","title":"Order of axes matters\u00b6","text":""},{"location":"1-einops-basics/#meet-einopsreduce","title":"Meet einops.reduce\u00b6","text":"<p>In einops-land you don't need to guess what happened</p> <pre>x.mean(-1)\n</pre> <p>Because you write what the operation does</p> <pre>reduce(x, 'b h w c -&gt; b h w', 'mean')\n</pre> <p>if axis is not present in the output \u2014 you guessed it \u2014 axis was reduced.</p>"},{"location":"1-einops-basics/#stack-and-concatenate","title":"Stack and concatenate\u00b6","text":""},{"location":"1-einops-basics/#addition-or-removal-of-axes","title":"Addition or removal of axes\u00b6","text":"<p>You can write 1 to create a new axis of length 1. Similarly you can remove such axis.</p> <p>There is also a synonym <code>()</code> that you can use. That's a composition of zero axes and it also has a unit length.</p>"},{"location":"1-einops-basics/#repeating-elements","title":"Repeating elements\u00b6","text":"<p>Third operation we introduce is <code>repeat</code></p>"},{"location":"1-einops-basics/#reduce-repeat","title":"Reduce \u21c6 repeat\u00b6","text":"<p>reduce and repeat are like opposite of each other: first one reduces amount of elements, second one increases.</p> <p>In the following example each image is repeated first, then we reduce over new axis to get back original tensor. Notice that operation patterns are \"reverse\" of each other</p>"},{"location":"1-einops-basics/#fancy-examples-in-random-order","title":"Fancy examples in random order\u00b6","text":"<p>(a.k.a. mad designer gallery)</p>"},{"location":"1-einops-basics/#ok-numpy-is-fun-but-how-do-i-use-einops-with-some-other-framework","title":"Ok, numpy is fun, but how do I use einops with some other framework?\u00b6","text":"<p>If that's what you've done with <code>ims</code> being numpy array:</p> <pre>rearrange(ims, 'b h w c -&gt; w (b h) c')\n</pre> <p>That's how you adapt the code for other frameworks:</p> <pre># pytorch:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# tensorflow:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# gluon:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# cupy:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# jax:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# paddle:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n\n...well, you got the idea.\n</pre> <p>Einops allows backpropagation as if all operations were native to framework. Operations do not change when moving to another framework - einops notation is universal</p>"},{"location":"1-einops-basics/#summary","title":"Summary\u00b6","text":"<ul> <li><p><code>rearrange</code> doesn't change number of elements and covers different numpy functions (like <code>transpose</code>, <code>reshape</code>, <code>stack</code>, <code>concatenate</code>,  <code>squeeze</code> and <code>expand_dims</code>)</p> </li> <li><p><code>reduce</code> combines same reordering syntax with reductions (<code>mean</code>, <code>min</code>, <code>max</code>, <code>sum</code>, <code>prod</code>, and any others)</p> </li> <li><p><code>repeat</code> additionally covers repeating and tiling</p> </li> <li><p>composition and decomposition of axes are a corner stone, they can and should be used together</p> </li> <li><p>Second part of tutorial shows how einops works with other frameworks</p> </li> <li><p>Third part of tutorial shows how to improve your DL code with einops</p> </li> </ul>"},{"location":"2-einops-for-deep-learning/","title":"Einops tutorial, part 2: deep learning","text":"In\u00a0[1]: Copied! <pre>from einops import rearrange, reduce\n</pre> from einops import rearrange, reduce In\u00a0[2]: Copied! <pre>import numpy as np\n\nx = np.random.RandomState(42).normal(size=[10, 32, 100, 200])\n</pre> import numpy as np  x = np.random.RandomState(42).normal(size=[10, 32, 100, 200]) In\u00a0[3]: Copied! <pre># utility to hide answers\nfrom utils import guess\n</pre> # utility to hide answers from utils import guess In\u00a0[4]: Copied! <pre># select \"tensorflow\" or \"pytorch\"\nflavour = \"pytorch\"\n</pre> # select \"tensorflow\" or \"pytorch\" flavour = \"pytorch\" In\u00a0[5]: Copied! <pre>print(f\"selected {flavour} backend\")\nif flavour == \"tensorflow\":\n    import tensorflow as tf\n\n    tape = tf.GradientTape(persistent=True)\n    tape.__enter__()\n    x = tf.Variable(x) + 0\nelse:\n    assert flavour == \"pytorch\"\n    import torch\n\n    x = torch.from_numpy(x)\n    x.requires_grad = True\n</pre> print(f\"selected {flavour} backend\") if flavour == \"tensorflow\":     import tensorflow as tf      tape = tf.GradientTape(persistent=True)     tape.__enter__()     x = tf.Variable(x) + 0 else:     assert flavour == \"pytorch\"     import torch      x = torch.from_numpy(x)     x.requires_grad = True <pre>selected pytorch backend\n</pre> In\u00a0[6]: Copied! <pre>type(x), x.shape\n</pre> type(x), x.shape Out[6]: <pre>(torch.Tensor, torch.Size([10, 32, 100, 200]))</pre> In\u00a0[7]: Copied! <pre>y = rearrange(x, \"b c h w -&gt; b h w c\")\nguess(y.shape)\n</pre> y = rearrange(x, \"b c h w -&gt; b h w c\") guess(y.shape) Answer is: (10, 100, 200, 32) (hover to see) In\u00a0[8]: Copied! <pre>y0 = x\ny1 = reduce(y0, \"b c h w -&gt; b c\", \"max\")\ny2 = rearrange(y1, \"b c -&gt; c b\")\ny3 = reduce(y2, \"c b -&gt; \", \"sum\")\n\nif flavour == \"tensorflow\":\n    print(reduce(tape.gradient(y3, x), \"b c h w -&gt; \", \"sum\"))\nelse:\n    y3.backward()\n    print(reduce(x.grad, \"b c h w -&gt; \", \"sum\"))\n</pre> y0 = x y1 = reduce(y0, \"b c h w -&gt; b c\", \"max\") y2 = rearrange(y1, \"b c -&gt; c b\") y3 = reduce(y2, \"c b -&gt; \", \"sum\")  if flavour == \"tensorflow\":     print(reduce(tape.gradient(y3, x), \"b c h w -&gt; \", \"sum\")) else:     y3.backward()     print(reduce(x.grad, \"b c h w -&gt; \", \"sum\")) <pre>tensor(320., dtype=torch.float64)\n</pre> In\u00a0[9]: Copied! <pre>from einops import asnumpy\n\ny3_numpy = asnumpy(y3)\n\nprint(type(y3_numpy))\n</pre> from einops import asnumpy  y3_numpy = asnumpy(y3)  print(type(y3_numpy)) <pre>&lt;class 'numpy.ndarray'&gt;\n</pre> In\u00a0[10]: Copied! <pre>y = rearrange(x, \"b c h w -&gt; b (c h w)\")\nguess(y.shape)\n</pre> y = rearrange(x, \"b c h w -&gt; b (c h w)\") guess(y.shape) Answer is: (10, 640000) (hover to see) <p>space-to-depth</p> In\u00a0[11]: Copied! <pre>y = rearrange(x, \"b c (h h1) (w w1) -&gt; b (h1 w1 c) h w\", h1=2, w1=2)\nguess(y.shape)\n</pre> y = rearrange(x, \"b c (h h1) (w w1) -&gt; b (h1 w1 c) h w\", h1=2, w1=2) guess(y.shape) Answer is: (10, 128, 50, 100) (hover to see) <p>depth-to-space (notice that it's reverse of the previous)</p> In\u00a0[12]: Copied! <pre>y = rearrange(x, \"b (h1 w1 c) h w -&gt; b c (h h1) (w w1)\", h1=2, w1=2)\nguess(y.shape)\n</pre> y = rearrange(x, \"b (h1 w1 c) h w -&gt; b c (h h1) (w w1)\", h1=2, w1=2) guess(y.shape) Answer is: (10, 8, 200, 400) (hover to see) In\u00a0[13]: Copied! <pre>y = reduce(x, \"b c h w -&gt; b c\", reduction=\"mean\")\nguess(y.shape)\n</pre> y = reduce(x, \"b c h w -&gt; b c\", reduction=\"mean\") guess(y.shape) Answer is: (10, 32) (hover to see) <p>max-pooling with a kernel 2x2</p> In\u00a0[14]: Copied! <pre>y = reduce(x, \"b c (h h1) (w w1) -&gt; b c h w\", reduction=\"max\", h1=2, w1=2)\nguess(y.shape)\n</pre> y = reduce(x, \"b c (h h1) (w w1) -&gt; b c h w\", reduction=\"max\", h1=2, w1=2) guess(y.shape) Answer is: (10, 32, 50, 100) (hover to see) In\u00a0[15]: Copied! <pre># you can skip names for reduced axes\ny = reduce(x, \"b c (h 2) (w 2) -&gt; b c h w\", reduction=\"max\")\nguess(y.shape)\n</pre> # you can skip names for reduced axes y = reduce(x, \"b c (h 2) (w 2) -&gt; b c h w\", reduction=\"max\") guess(y.shape) Answer is: (10, 32, 50, 100) (hover to see) In\u00a0[16]: Copied! <pre># models typically work only with batches,\n# so to predict a single image ...\nimage = rearrange(x[0, :3], \"c h w -&gt; h w c\")\n# ... create a dummy 1-element axis ...\ny = rearrange(image, \"h w c -&gt; () c h w\")\n# ... imagine you predicted this with a convolutional network for classification,\n# we'll just flatten axes ...\npredictions = rearrange(y, \"b c h w -&gt; b (c h w)\")\n# ... finally, decompose (remove) dummy axis\npredictions = rearrange(predictions, \"() classes -&gt; classes\")\n</pre> # models typically work only with batches, # so to predict a single image ... image = rearrange(x[0, :3], \"c h w -&gt; h w c\") # ... create a dummy 1-element axis ... y = rearrange(image, \"h w c -&gt; () c h w\") # ... imagine you predicted this with a convolutional network for classification, # we'll just flatten axes ... predictions = rearrange(y, \"b c h w -&gt; b (c h w)\") # ... finally, decompose (remove) dummy axis predictions = rearrange(predictions, \"() classes -&gt; classes\") In\u00a0[17]: Copied! <pre>y = x - reduce(x, \"b c h w -&gt; b c 1 1\", \"mean\")\nguess(y.shape)\n</pre> y = x - reduce(x, \"b c h w -&gt; b c 1 1\", \"mean\") guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) <p>per-channel mean-normalization for whole batch:</p> In\u00a0[18]: Copied! <pre>y = x - reduce(y, \"b c h w -&gt; 1 c 1 1\", \"mean\")\nguess(y.shape)\n</pre> y = x - reduce(y, \"b c h w -&gt; 1 c 1 1\", \"mean\") guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) In\u00a0[19]: Copied! <pre>list_of_tensors = list(x)\n</pre> list_of_tensors = list(x) <p>New axis (one that enumerates tensors) appears first on the left side of expression. Just as if you were indexing list - first you'd get tensor by index</p> In\u00a0[20]: Copied! <pre>tensors = rearrange(list_of_tensors, \"b c h w -&gt; b h w c\")\nguess(tensors.shape)\n</pre> tensors = rearrange(list_of_tensors, \"b c h w -&gt; b h w c\") guess(tensors.shape) Answer is: (10, 100, 200, 32) (hover to see) In\u00a0[21]: Copied! <pre># or maybe stack along last dimension?\ntensors = rearrange(list_of_tensors, \"b c h w -&gt; h w c b\")\nguess(tensors.shape)\n</pre> # or maybe stack along last dimension? tensors = rearrange(list_of_tensors, \"b c h w -&gt; h w c b\") guess(tensors.shape) Answer is: (100, 200, 32, 10) (hover to see) In\u00a0[22]: Copied! <pre>tensors = rearrange(list_of_tensors, \"b c h w -&gt; (b h) w c\")\nguess(tensors.shape)\n</pre> tensors = rearrange(list_of_tensors, \"b c h w -&gt; (b h) w c\") guess(tensors.shape) Answer is: (1000, 200, 32) (hover to see) <p>or maybe concatenate along last dimension?</p> In\u00a0[23]: Copied! <pre>tensors = rearrange(list_of_tensors, \"b c h w -&gt; h w (b c)\")\nguess(tensors.shape)\n</pre> tensors = rearrange(list_of_tensors, \"b c h w -&gt; h w (b c)\") guess(tensors.shape) Answer is: (100, 200, 320) (hover to see) In\u00a0[24]: Copied! <pre>y = rearrange(x, \"b (g1 g2 c) h w-&gt; b (g2 g1 c) h w\", g1=4, g2=4)\nguess(y.shape)\n</pre> y = rearrange(x, \"b (g1 g2 c) h w-&gt; b (g2 g1 c) h w\", g1=4, g2=4) guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) <p>simpler version of channel shuffle</p> In\u00a0[25]: Copied! <pre>y = rearrange(x, \"b (g c) h w-&gt; b (c g) h w\", g=4)\nguess(y.shape)\n</pre> y = rearrange(x, \"b (g c) h w-&gt; b (c g) h w\", g=4) guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) In\u00a0[26]: Copied! <pre>bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, \"b (coord bbox) h w -&gt; coord b bbox h w\", coord=4, bbox=8)\n# now you can operate on individual variables\nmax_bbox_area = reduce(bbox_w * bbox_h, \"b bbox h w -&gt; b h w\", \"max\")\nguess(bbox_x.shape)\nguess(max_bbox_area.shape)\n</pre> bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, \"b (coord bbox) h w -&gt; coord b bbox h w\", coord=4, bbox=8) # now you can operate on individual variables max_bbox_area = reduce(bbox_w * bbox_h, \"b bbox h w -&gt; b h w\", \"max\") guess(bbox_x.shape) guess(max_bbox_area.shape) Answer is: (10, 8, 100, 200) (hover to see) Answer is: (10, 100, 200) (hover to see) In\u00a0[27]: Copied! <pre>from einops import parse_shape\n</pre> from einops import parse_shape In\u00a0[28]: Copied! <pre>def convolve_2d(x):\n    # imagine we have a simple 2d convolution with padding,\n    # so output has same shape as input.\n    # Sorry for laziness, use imagination!\n    return x\n</pre> def convolve_2d(x):     # imagine we have a simple 2d convolution with padding,     # so output has same shape as input.     # Sorry for laziness, use imagination!     return x In\u00a0[29]: Copied! <pre># imagine we are working with 3d data\nx_5d = rearrange(x, \"b c x (y z) -&gt; b c x y z\", z=20)\n# but we have only 2d convolutions.\n# That's not a problem, since we can apply\ny = rearrange(x_5d, \"b c x y z -&gt; (b z) c x y\")\ny = convolve_2d(y)\n# not just specifies additional information, but verifies that all dimensions match\ny = rearrange(y, \"(b z) c x y -&gt; b c x y z\", **parse_shape(x_5d, \"b c x y z\"))\n</pre> # imagine we are working with 3d data x_5d = rearrange(x, \"b c x (y z) -&gt; b c x y z\", z=20) # but we have only 2d convolutions. # That's not a problem, since we can apply y = rearrange(x_5d, \"b c x y z -&gt; (b z) c x y\") y = convolve_2d(y) # not just specifies additional information, but verifies that all dimensions match y = rearrange(y, \"(b z) c x y -&gt; b c x y z\", **parse_shape(x_5d, \"b c x y z\")) In\u00a0[30]: Copied! <pre>parse_shape(x_5d, \"b c x y z\")\n</pre> parse_shape(x_5d, \"b c x y z\") Out[30]: <pre>{'b': 10, 'c': 32, 'x': 100, 'y': 10, 'z': 20}</pre> In\u00a0[31]: Copied! <pre># we can skip some dimensions by writing underscore\nparse_shape(x_5d, \"batch c _ _ _\")\n</pre> # we can skip some dimensions by writing underscore parse_shape(x_5d, \"batch c _ _ _\") Out[31]: <pre>{'batch': 10, 'c': 32}</pre> In\u00a0[32]: Copied! <pre># each image is split into subgrids, each subgrid now is a separate \"image\"\ny = rearrange(x, \"b c (h hs) (w ws) -&gt; (hs ws b) c h w\", hs=2, ws=2)\ny = convolve_2d(y)\n# pack subgrids back to an image\ny = rearrange(y, \"(hs ws b) c h w -&gt; b c (h hs) (w ws)\", hs=2, ws=2)\n\nassert y.shape == x.shape\n</pre> # each image is split into subgrids, each subgrid now is a separate \"image\" y = rearrange(x, \"b c (h hs) (w ws) -&gt; (hs ws b) c h w\", hs=2, ws=2) y = convolve_2d(y) # pack subgrids back to an image y = rearrange(y, \"(hs ws b) c h w -&gt; b c (h hs) (w ws)\", hs=2, ws=2)  assert y.shape == x.shape"},{"location":"2-einops-for-deep-learning/#einops-tutorial-part-2-deep-learning","title":"Einops tutorial, part 2: deep learning\u00b6","text":"<p>Previous part of tutorial provides visual examples with numpy.</p>"},{"location":"2-einops-for-deep-learning/#whats-in-this-tutorial","title":"What's in this tutorial?\u00b6","text":"<ul> <li>working with deep learning packages</li> <li>important cases for deep learning models</li> <li><code>einops.asnumpy</code> and <code>einops.layers</code></li> </ul>"},{"location":"2-einops-for-deep-learning/#select-your-flavour","title":"Select your flavour\u00b6","text":"<p>Switch to the framework you're most comfortable with.</p>"},{"location":"2-einops-for-deep-learning/#simple-computations","title":"Simple computations\u00b6","text":"<ul> <li>converting bchw to bhwc format and back is a common operation in CV</li> <li>try to predict output shape and then check your guess!</li> </ul>"},{"location":"2-einops-for-deep-learning/#worked","title":"Worked!\u00b6","text":"<p>Did you notice? Code above worked for you backend of choice.  Einops functions work with any tensor like they are native to the framework.</p>"},{"location":"2-einops-for-deep-learning/#backpropagation","title":"Backpropagation\u00b6","text":"<ul> <li>gradients are a corner stone of deep learning</li> <li>You can back-propagate through einops operations  (just as with framework native operations)</li> </ul>"},{"location":"2-einops-for-deep-learning/#meet-einopsasnumpy","title":"Meet <code>einops.asnumpy</code>\u00b6","text":"<p>Just converts tensors to numpy (and pulls from gpu if necessary)</p>"},{"location":"2-einops-for-deep-learning/#common-building-blocks-of-deep-learning","title":"Common building blocks of deep learning\u00b6","text":"<p>Let's check how some familiar operations can be written with <code>einops</code></p> <p>Flattening is common operation, frequently appears at the boundary between convolutional layers and fully connected layers</p>"},{"location":"2-einops-for-deep-learning/#reductions","title":"Reductions\u00b6","text":"<p>Simple global average pooling.</p>"},{"location":"2-einops-for-deep-learning/#1d-2d-and-3d-pooling-are-defined-in-a-similar-way","title":"1d, 2d and 3d pooling are defined in a similar way\u00b6","text":"<p>for sequential 1-d models, you'll probably want pooling over time</p> <pre>reduce(x, '(t 2) b c -&gt; t b c', reduction='max')\n</pre> <p>for volumetric models, all three dimensions are pooled</p> <pre>reduce(x, 'b c (x 2) (y 2) (z 2) -&gt; b c x y z', reduction='max')\n</pre> <p>Uniformity is a strong point of <code>einops</code>, and you don't need specific operation for each particular case.</p>"},{"location":"2-einops-for-deep-learning/#good-exercises","title":"Good exercises\u00b6","text":"<ul> <li>write a version of space-to-depth for 1d and 3d (2d is provided above)</li> <li>write an average / max pooling for 1d models.</li> </ul>"},{"location":"2-einops-for-deep-learning/#squeeze-and-unsqueeze-expand_dims","title":"Squeeze and unsqueeze (expand_dims)\u00b6","text":""},{"location":"2-einops-for-deep-learning/#keepdims-like-behavior-for-reductions","title":"keepdims-like behavior for reductions\u00b6","text":"<ul> <li>empty composition <code>()</code> provides dimensions of length 1, which are broadcastable.</li> <li>alternatively, you can use just <code>1</code> to introduce new axis, that's a synonym to <code>()</code></li> </ul> <p>per-channel mean-normalization for each image:</p>"},{"location":"2-einops-for-deep-learning/#stacking","title":"Stacking\u00b6","text":"<p>let's take a list of tensors</p>"},{"location":"2-einops-for-deep-learning/#concatenation","title":"Concatenation\u00b6","text":"<p>concatenate over the first dimension?</p>"},{"location":"2-einops-for-deep-learning/#shuffling-within-a-dimension","title":"Shuffling within a dimension\u00b6","text":"<p>channel shuffle (as it is drawn in shufflenet paper)</p>"},{"location":"2-einops-for-deep-learning/#split-a-dimension","title":"Split a dimension\u00b6","text":"<p>Here's a super-convenient trick.</p> <p>Example: when a network predicts several bboxes for each position</p> <p>Assume we got 8 bboxes, 4 coordinates each.  To get coordinated into 4 separate variables, you move corresponding dimension to front and unpack tuple.</p>"},{"location":"2-einops-for-deep-learning/#getting-into-the-weeds-of-tensor-packing","title":"Getting into the weeds of tensor packing\u00b6","text":"<p>you can skip this part - it explains why taking a habit of defining splits and packs explicitly</p> <p>when implementing custom gated activation (like GLU), split is needed:</p> <pre>y1, y2 = rearrange(x, 'b (split c) h w -&gt; split b c h w', split=2)\nresult = y2 * sigmoid(y2) # or tanh\n</pre> <p>... but we could split differently</p> <pre>y1, y2 = rearrange(x, 'b (c split) h w -&gt; split b c h w', split=2)\n</pre> <ul> <li>first one splits channels into consequent groups: <code>y1 = x[:, :x.shape[1] // 2, :, :]</code></li> <li>while second takes channels with a step: <code>y1 = x[:, 0::2, :, :]</code></li> </ul> <p>This may drive to very surprising results when input is</p> <ul> <li>a result of group convolution</li> <li>a result of bidirectional LSTM/RNN</li> <li>multi-head attention</li> </ul> <p>Let's focus on the second case (LSTM/RNN), since it is less obvious.</p> <p>For instance, cudnn concatenates LSTM outputs for forward-in-time and backward-in-time</p> <p>Also in pytorch GLU splits channels into consequent groups (first way) So when LSTM's output comes to GLU,</p> <ul> <li>forward-in-time produces linear part, and backward-in-time produces activation ...</li> <li>and role of directions is different, and gradients coming to two parts are different<ul> <li>that's not what you expect from simple <code>GLU(BLSTM(x))</code>, right?</li> </ul> </li> </ul> <p><code>einops</code> notation makes such inconsistencies explicit and easy-detectable</p>"},{"location":"2-einops-for-deep-learning/#shape-parsing","title":"Shape parsing\u00b6","text":"<p>just a handy utility</p>"},{"location":"2-einops-for-deep-learning/#striding-anything","title":"Striding anything\u00b6","text":"<p>Finally, how to convert any operation into a strided operation?  (like convolution with strides, aka dilated/atrous convolution)</p>"},{"location":"2-einops-for-deep-learning/#layers","title":"Layers\u00b6","text":"<p>For frameworks that prefer operating with layers, layers are available.</p> <p>You'll need to import a proper one depending on your backend:</p> <pre>from einops.layers.torch import Rearrange, Reduce\nfrom einops.layers.flax import Rearrange, Reduce\nfrom einops.layers.tensorflow import Rearrange, Reduce\nfrom einops.layers.chainer import Rearrange, Reduce\n</pre> <p><code>Einops</code> layers are identical to operations, and have same parameters.  (for the exception of first argument, which should be passed during call)</p> <pre>layer = Rearrange(pattern, **axes_lengths)\nlayer = Reduce(pattern, reduction, **axes_lengths)\n\n# apply layer to tensor\nx = layer(x)\n</pre> <p>Usually it is more convenient to use layers, not operations, to build models</p> <pre># example given for pytorch, but code in other frameworks is almost identical\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Reduce\n\nmodel = Sequential(\n    Conv2d(3, 6, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    Conv2d(6, 16, kernel_size=5),\n    # combined pooling and flattening in a single step\n    Reduce('b c (h 2) (w 2) -&gt; b (c h w)', 'max'), \n    Linear(16*5*5, 120), \n    ReLU(),\n    Linear(120, 10), \n    # In flax, the {'axis': value} syntax for specifying values for axes is mandatory:\n    # Rearrange('(b1 b2) d -&gt; b1 b2 d', {'b1': 12}), \n)\n</pre>"},{"location":"2-einops-for-deep-learning/#whats-now","title":"What's now?\u00b6","text":"<ul> <li>rush through writing better code with einops+pytorch</li> </ul> <p>Use different framework? Not a big issue, most recommendations transfer well to other frameworks.  <code>einops</code> works the same way in any framework.</p> <p>Finally - just write your code with einops!</p>"},{"location":"3-einmix-layer/","title":"EinMix: universal toolkit for advanced MLP architectures","text":"In\u00a0[\u00a0]: Copied! <pre>from einops.layers.torch import EinMix as Mix\n\n# tutorial uses torch. EinMix is available for other frameworks too\nfrom torch import nn\nfrom torch.nn import functional as F\n</pre> from einops.layers.torch import EinMix as Mix  # tutorial uses torch. EinMix is available for other frameworks too from torch import nn from torch.nn import functional as F <p>Logic of EinMix is very close to the one of <code>einsum</code>. If you're not familiar with einsum, follow these guides first:</p> <ul> <li>https://rockt.github.io/2018/04/30/einsum</li> <li>https://towardsdatascience.com/einsum-an-underestimated-function-99ca96e2942e</li> </ul> <p>Einsum uniformly describes a number of operations. <code>EinMix</code> is a layer (not function) implementing a similar logic, it has some differences with <code>einsum</code>.</p> <p>Let's implement simple linear layer using einsum</p> <pre>weight = &lt;...create and initialize parameter...&gt;\nbias = &lt;...create and initialize parameter...&gt;\nresult = torch.einsum('tbc,cd-&gt;tbd', embeddings, weight) + bias\n</pre> <p>EinMix counter-part is:</p> <pre>mix_channels = Mix('t b c -&gt; t b c_out', weight_shape='c c_out', bias_shape='c_out', ...)\nresult = mix_channels(embeddings)\n</pre> <p>Main differences compared to plain <code>einsum</code> are:</p> <ul> <li>layer takes care of the parameter initialization &amp; management</li> <li>weight is not in the comprehension</li> <li>EinMix includes bias term</li> </ul> <p>We'll discuss other changes a bit later, now let's implement some elements from MLPMixer.</p> In\u00a0[2]: Copied! <pre>class MLP(nn.Module):\n    def __init__(self, num_features, expansion_factor, dropout):\n        super().__init__()\n        num_hidden = num_features * expansion_factor\n        self.fc1 = nn.Linear(num_features, num_hidden)\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(num_hidden, num_features)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.dropout1(F.gelu(self.fc1(x)))\n        x = self.dropout2(self.fc2(x))\n        return x\n\n\nclass TokenMixer(nn.Module):\n    def __init__(self, num_features, num_patches, expansion_factor, dropout):\n        super().__init__()\n        self.norm = nn.LayerNorm(num_features)\n        self.mlp = MLP(num_patches, expansion_factor, dropout)\n\n    def forward(self, x):\n        # x.shape == (batch_size, num_patches, num_features)\n        residual = x\n        x = self.norm(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_features, num_patches)\n        x = self.mlp(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_patches, num_features)\n        out = x + residual\n        return out\n</pre> class MLP(nn.Module):     def __init__(self, num_features, expansion_factor, dropout):         super().__init__()         num_hidden = num_features * expansion_factor         self.fc1 = nn.Linear(num_features, num_hidden)         self.dropout1 = nn.Dropout(dropout)         self.fc2 = nn.Linear(num_hidden, num_features)         self.dropout2 = nn.Dropout(dropout)      def forward(self, x):         x = self.dropout1(F.gelu(self.fc1(x)))         x = self.dropout2(self.fc2(x))         return x   class TokenMixer(nn.Module):     def __init__(self, num_features, num_patches, expansion_factor, dropout):         super().__init__()         self.norm = nn.LayerNorm(num_features)         self.mlp = MLP(num_patches, expansion_factor, dropout)      def forward(self, x):         # x.shape == (batch_size, num_patches, num_features)         residual = x         x = self.norm(x)         x = x.transpose(1, 2)         # x.shape == (batch_size, num_features, num_patches)         x = self.mlp(x)         x = x.transpose(1, 2)         # x.shape == (batch_size, num_patches, num_features)         out = x + residual         return out In\u00a0[3]: Copied! <pre>def TokenMixer(num_features: int, n_patches: int, expansion_factor: int, dropout: float):\n    n_hidden = n_patches * expansion_factor\n    return nn.Sequential(\n        nn.LayerNorm(num_features),\n        Mix(\"b hw c -&gt; b hid c\", weight_shape=\"hw hid\", bias_shape=\"hid\", hw=n_patches, hidden=n_hidden),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        Mix(\"b hid c -&gt; b hw c\", weight_shape=\"hid hw\", bias_shape=\"hw\",  hw=n_patches, hidden=n_hidden),\n        nn.Dropout(dropout),\n    )\n</pre> def TokenMixer(num_features: int, n_patches: int, expansion_factor: int, dropout: float):     n_hidden = n_patches * expansion_factor     return nn.Sequential(         nn.LayerNorm(num_features),         Mix(\"b hw c -&gt; b hid c\", weight_shape=\"hw hid\", bias_shape=\"hid\", hw=n_patches, hidden=n_hidden),         nn.GELU(),         nn.Dropout(dropout),         Mix(\"b hid c -&gt; b hw c\", weight_shape=\"hid hw\", bias_shape=\"hw\",  hw=n_patches, hidden=n_hidden),         nn.Dropout(dropout),     ) <p>You may also check another implementation of MLPMixer from Phil Wang.  Phil solves the issue by repurposing <code>nn.Conv1d</code> to mix on the second dimension. Hacky, but does the job</p> In\u00a0[4]: Copied! <pre>def check_sizes(image_size, patch_size):\n    sqrt_num_patches, remainder = divmod(image_size, patch_size)\n    assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"\n    num_patches = sqrt_num_patches ** 2\n    return num_patches\n\nclass Patcher(nn.Module):\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=16,\n        in_channels=3,\n        num_features=128,\n    ):\n        _num_patches = check_sizes(image_size, patch_size)\n        super().__init__()\n        # per-patch fully-connected is equivalent to strided conv2d\n        self.patcher = nn.Conv2d(\n            in_channels, num_features, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        patches = self.patcher(x)\n        batch_size, num_features, _, _ = patches.shape\n        patches = patches.permute(0, 2, 3, 1)\n        patches = patches.view(batch_size, -1, num_features)\n\n        return patches\n</pre> def check_sizes(image_size, patch_size):     sqrt_num_patches, remainder = divmod(image_size, patch_size)     assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"     num_patches = sqrt_num_patches ** 2     return num_patches  class Patcher(nn.Module):     def __init__(         self,         image_size=256,         patch_size=16,         in_channels=3,         num_features=128,     ):         _num_patches = check_sizes(image_size, patch_size)         super().__init__()         # per-patch fully-connected is equivalent to strided conv2d         self.patcher = nn.Conv2d(             in_channels, num_features, kernel_size=patch_size, stride=patch_size         )      def forward(self, x):         patches = self.patcher(x)         batch_size, num_features, _, _ = patches.shape         patches = patches.permute(0, 2, 3, 1)         patches = patches.view(batch_size, -1, num_features)          return patches In\u00a0[5]: Copied! <pre>def patcher(patch_size=16, in_channels=3, num_features=128):\n    return Mix(\"b c_in (h hp) (w wp) -&gt; b (h w) c\", weight_shape=\"c_in hp wp c\", bias_shape=\"c\",\n               c=num_features, hp=patch_size, wp=patch_size, c_in=in_channels)\n</pre> def patcher(patch_size=16, in_channels=3, num_features=128):     return Mix(\"b c_in (h hp) (w wp) -&gt; b (h w) c\", weight_shape=\"c_in hp wp c\", bias_shape=\"c\",                c=num_features, hp=patch_size, wp=patch_size, c_in=in_channels) In\u00a0[6]: Copied! <pre>class WeightedPermuteMLP(nn.Module):\n    def __init__(self, H, W, C, S):\n        super().__init__()\n\n        self.proj_h = nn.Linear(H * S, H * S)\n        self.proj_w = nn.Linear(W * S, W * S)\n        self.proj_c = nn.Linear(C, C)\n        self.proj = nn.Linear(C, C)\n        self.S = S\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        S = self.S\n        N = C // S\n        x_h = x.reshape(B, H, W, N, S).permute(0, 3, 2, 1, 4).reshape(B, N, W, H*S)\n        x_h = self.proj_h(x_h).reshape(B, N, W, H, S).permute(0, 3, 2, 1, 4).reshape(B, H, W, C)\n\n        x_w = x.reshape(B, H, W, N, S).permute(0, 1, 3, 2, 4).reshape(B, H, N, W*S)\n        x_w = self.proj_w(x_w).reshape(B, H, N, W, S).permute(0, 1, 3, 2, 4).reshape(B, H, W, C)\n\n        x_c = self.proj_c(x)\n\n        x = x_h + x_w + x_c\n        x = self.proj(x)\n        return x\n</pre> class WeightedPermuteMLP(nn.Module):     def __init__(self, H, W, C, S):         super().__init__()          self.proj_h = nn.Linear(H * S, H * S)         self.proj_w = nn.Linear(W * S, W * S)         self.proj_c = nn.Linear(C, C)         self.proj = nn.Linear(C, C)         self.S = S      def forward(self, x):         B, H, W, C = x.shape         S = self.S         N = C // S         x_h = x.reshape(B, H, W, N, S).permute(0, 3, 2, 1, 4).reshape(B, N, W, H*S)         x_h = self.proj_h(x_h).reshape(B, N, W, H, S).permute(0, 3, 2, 1, 4).reshape(B, H, W, C)          x_w = x.reshape(B, H, W, N, S).permute(0, 1, 3, 2, 4).reshape(B, H, N, W*S)         x_w = self.proj_w(x_w).reshape(B, H, N, W, S).permute(0, 1, 3, 2, 4).reshape(B, H, W, C)          x_c = self.proj_c(x)          x = x_h + x_w + x_c         x = self.proj(x)         return x <p>That didn't look readable, right?</p> <p>This code is also very inflexible: code in the paper did not support batch dimension, and multiple changes were necessary to allow batch processing.  This process is fragile and easily can result in virtually uncatchable bugs.</p> <p>Now good news: each of these long method chains can be replaced with a single <code>EinMix</code> layer:</p> In\u00a0[7]: Copied! <pre>class WeightedPermuteMLP_new(nn.Module):\n    def __init__(self, H, W, C, seg_len):\n        super().__init__()\n        assert C % seg_len == 0, f\"can't divide {C} into segments of length {seg_len}\"\n        self.mlp_c = Mix(\"b h w c -&gt; b h w c0\", weight_shape=\"c c0\", bias_shape=\"c0\",\n                         c=C, c0=C)\n        self.mlp_h = Mix(\"b h w (n c) -&gt; b h0 w (n c0)\", weight_shape=\"h c h0 c0\", bias_shape=\"h0 c0\",\n                         h=H, h0=H, c=seg_len, c0=seg_len)\n        self.mlp_w = Mix(\"b h w (n c) -&gt; b h w0 (n c0)\", weight_shape=\"w c w0 c0\", bias_shape=\"w0 c0\",\n                         w=W, w0=W, c=seg_len, c0=seg_len)\n        self.proj = nn.Linear(C, C)\n\n    def forward(self, x):\n        x = self.mlp_c(x) + self.mlp_h(x) + self.mlp_w(x)\n        return self.proj(x)\n</pre> class WeightedPermuteMLP_new(nn.Module):     def __init__(self, H, W, C, seg_len):         super().__init__()         assert C % seg_len == 0, f\"can't divide {C} into segments of length {seg_len}\"         self.mlp_c = Mix(\"b h w c -&gt; b h w c0\", weight_shape=\"c c0\", bias_shape=\"c0\",                          c=C, c0=C)         self.mlp_h = Mix(\"b h w (n c) -&gt; b h0 w (n c0)\", weight_shape=\"h c h0 c0\", bias_shape=\"h0 c0\",                          h=H, h0=H, c=seg_len, c0=seg_len)         self.mlp_w = Mix(\"b h w (n c) -&gt; b h w0 (n c0)\", weight_shape=\"w c w0 c0\", bias_shape=\"w0 c0\",                          w=W, w0=W, c=seg_len, c0=seg_len)         self.proj = nn.Linear(C, C)      def forward(self, x):         x = self.mlp_c(x) + self.mlp_h(x) + self.mlp_w(x)         return self.proj(x) In\u00a0[10]: Copied! <pre>class MultiheadAttention(nn.Module):\n    def __init__(self, dim_input, n_heads, head_dim):\n        super().__init__()\n        self.input_to_qkv = Mix(\"b t c -&gt; qkv b h t hid\", \"c qkv h hid\",\n                                c=dim_input, qkv=3, h=n_heads, hid=head_dim)\n        self.out_proj = Mix(\"b h t hid -&gt; b t c\", \"h hid c\",\n                            h=n_heads, hid=head_dim, c=dim_input)\n\n    def forward(self, x):\n        q, k, v = self.input_to_qkv(x) # fused projections, computed in one go\n        return self.out_proj(F.scaled_dot_product_attention(q, k, v)) # flash attention\n</pre> class MultiheadAttention(nn.Module):     def __init__(self, dim_input, n_heads, head_dim):         super().__init__()         self.input_to_qkv = Mix(\"b t c -&gt; qkv b h t hid\", \"c qkv h hid\",                                 c=dim_input, qkv=3, h=n_heads, hid=head_dim)         self.out_proj = Mix(\"b h t hid -&gt; b t c\", \"h hid c\",                             h=n_heads, hid=head_dim, c=dim_input)      def forward(self, x):         q, k, v = self.input_to_qkv(x) # fused projections, computed in one go         return self.out_proj(F.scaled_dot_product_attention(q, k, v)) # flash attention"},{"location":"3-einmix-layer/#einmix-universal-toolkit-for-advanced-mlp-architectures","title":"EinMix: universal toolkit for advanced MLP architectures\u00b6","text":"<p>Recent progress in MLP-based architectures demonstrated that very specific MLPs can compete with convnets and transformers (and even outperform them).</p> <p>EinMix allows writing such architectures in a more uniform and readable way.</p>"},{"location":"3-einmix-layer/#einmix-building-block-of-mlps","title":"EinMix \u2014 building block of MLPs\u00b6","text":""},{"location":"3-einmix-layer/#tokenmixer-from-mlpmixer-original-code","title":"TokenMixer from MLPMixer \u2014 original code\u00b6","text":"<p>We start from pytorch implementation of MLPMixer by Jake Tae.</p> <p>We'll focus on two components of MLPMixer that don't exist in convnets. First component is TokenMixer:</p>"},{"location":"3-einmix-layer/#tokenmixer-from-mlpmixer-reimplemented","title":"TokenMixer from MLPMixer \u2014 reimplemented\u00b6","text":"<p>We can significantly reduce amount of code by using <code>EinMix</code>.</p> <ul> <li>Main caveat addressed by original code is that <code>nn.Linear</code> mixes only last axis. <code>EinMix</code> can mix any axis (or set of axes).</li> <li>Sequential structure is always preferred as it is easier to follow</li> <li>Intentionally there is no residual connection in <code>TokenMixer</code>, because honestly it's not work of Mixer and should be done by caller</li> </ul>"},{"location":"3-einmix-layer/#mlpmixers-patch-embeddings-aka-vit-patch-embeddings-original","title":"MLPMixer's patch embeddings (aka ViT patch embeddings) \u2014 original\u00b6","text":"<p>Second interesting part of MLPMixer is derived from vision transformers.</p> <p>In the very beginning an image is split into patches, and each patch is linearly projected into embedding:</p>"},{"location":"3-einmix-layer/#mlpmixers-patch-embeddings-reimplemented","title":"MLPMixer's patch embeddings \u2014 reimplemented\u00b6","text":"<p><code>EinMix</code> does this in a single operation. This may require some training at first to understand.</p> <p>Let's go step-by-step:</p> <ul> <li><code>b c_in (h hp) (w wp) -&gt;</code> - 4-dimensional input tensor (BCHW-ordered) is split into patches of shape <code>hp x wp</code></li> <li><code>weight_shape='c_in hp wp c'</code>. Axes <code>c_in</code>, <code>hp</code> and <code>wp</code> are all absent in the output: three dimensional patch tensor was mixed to produce a vector of length <code>c</code></li> <li><code>-&gt; b (h w) c</code> - output is 3-dimensional. All patches were reorganized from <code>h x w</code> grid to one-dimensional sequence of vectors</li> </ul> <p>We don't need to provide image_size beforehead, new implementation handles images of different dimensions as long as they can be divided into patches</p>"},{"location":"3-einmix-layer/#vision-permutator","title":"Vision Permutator\u00b6","text":"<p>As a third example we consider pytorch-like code from ViP paper.</p> <p>Vision permutator is only slightly more nuanced than previous models, because</p> <ol> <li>it operates on spatial dimensions separately, while MLPMixer and its friends just pack all spatial info into one axis.</li> <li>it splits channels into groups called 'segments'</li> </ol>"},{"location":"3-einmix-layer/#multi-head-attention-once-again","title":"Multi-head attention, once again\u00b6","text":"<p>EinMix can be (mis)used to compute multiple projections and perform transpositions along the way.</p> <p>For example, F.scaled_dot_product_attention wants a specific order of axes, and an explicit head axis. We can combine linear projection with providing desired order of arguments in a single operation.</p>"},{"location":"3-einmix-layer/#exercises","title":"Exercises\u00b6","text":"<ol> <li><p>Many normalizations (batch norm, layer norm, etc) use affine scaling afterwards. Implement this scaling using <code>EinMix</code>.</p> </li> <li><p>let's assume you have an input tensor of shape <code>[b, t, n_groups, n_channels]</code>, and you want to apply a separate linear layer to every group of channels.</p> <p>This will introduce <code>n_groups</code> matrices of shape <code>[n_channels, n_channels]</code> and <code>n_groups</code> biases of shape <code>[n_channels]</code>. Can you perform this operation with just one <code>EinMix</code>?</p> </li> </ol>"},{"location":"3-einmix-layer/#final-remarks","title":"Final remarks\u00b6","text":"<p><code>EinMix</code> helps with MLPs that don't fit into a limited 'mix all in the last axis' paradigm, and specially helpful for non-1d inputs (images, videos, etc).</p> <p>However existing research does not cover real possibilities of densely connected architectures.</p> <p>Most of its systematic novelty is \"mix along spatial axes actually works\". But <code>EinMix</code> provides an astonishing amount of other possibilities!. Let me mention some examples:</p>"},{"location":"3-einmix-layer/#mixing-within-a-patch-on-a-grid","title":"Mixing within a patch on a grid\u00b6","text":"<p>What if you make mixing 'local' in space? Completely doable:</p> <pre>'b c (h hI) (w wI) -&gt; b c (h hO) (w wO)', weight_shape='c hI wI hO wO'\n</pre> <p>We split tensor into patches of shape <code>hI wI</code> and mixed per-channel.</p>"},{"location":"3-einmix-layer/#mixing-in-subgrids","title":"Mixing in subgrids\u00b6","text":"<p>Opposite question: how to collect information from the whole image (without attention)? </p> <p>Well, you can again densely connect all the tokens, but all-to-all connection can be too expensive.</p> <p>Here is EinMix-way: split the image into subgrids (each subgrid has steps <code>h</code> and <code>w</code>), and connect densely tokens within each subgrid</p> <pre>'b c (hI h) (wI w) -&gt; b c (hO h) (wO w)', weight_shape='c hI wI hO wO'\n</pre>"},{"location":"3-einmix-layer/#going-deeper","title":"Going deeper\u00b6","text":"<p>And that's very top of the iceberg.</p> <ul> <li>Want to mix part of axis? \u2014 No problems!</li> <li>... in a grid-like manner \u2014 Supported!</li> <li>... while mixing channels within group? \u2014 Welcome!</li> <li>In 2d/3d/4d? \u2014 Sure!</li> <li>Don't use pytorch? \u2014 EinMix is available for multiple frameworks!</li> </ul> <p>Hopefully this guide helped you to find MLPs more interesting and intriguing. And simpler to experiment with.</p>"},{"location":"4-pack-and-unpack/","title":"einops.pack and einops.unpack","text":"In\u00a0[1]: Copied! <pre># install necessary libraries\n%pip install numpy einops -q\n</pre> # install necessary libraries %pip install numpy einops -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre># we'll use numpy for demo purposes\n# operations work the same way with other frameworks\nimport numpy as np\n</pre> # we'll use numpy for demo purposes # operations work the same way with other frameworks import numpy as np In\u00a0[3]: Copied! <pre>from einops import pack, unpack\n\nh, w = 100, 200\n# image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w)\nimage_rgb = np.random.random([h, w, 3])\nimage_depth = np.random.random([h, w])\n# but we can stack them\nimage_rgbd, ps = pack([image_rgb, image_depth], \"h w *\")\n</pre> from einops import pack, unpack  h, w = 100, 200 # image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w) image_rgb = np.random.random([h, w, 3]) image_depth = np.random.random([h, w]) # but we can stack them image_rgbd, ps = pack([image_rgb, image_depth], \"h w *\") In\u00a0[4]: Copied! <pre># as you see, pack properly appended depth as one more layer\n# and correctly aligned axes!\n# this won't work off the shelf with np.concatenate or torch.cat or alike\nimage_rgb.shape, image_depth.shape, image_rgbd.shape\n</pre> # as you see, pack properly appended depth as one more layer # and correctly aligned axes! # this won't work off the shelf with np.concatenate or torch.cat or alike image_rgb.shape, image_depth.shape, image_rgbd.shape Out[4]: <pre>((100, 200, 3), (100, 200), (100, 200, 4))</pre> In\u00a0[5]: Copied! <pre># now let's see what PS keeps.\n# PS means Packed Shapes, not PlayStation or Post Script\nps\n</pre> # now let's see what PS keeps. # PS means Packed Shapes, not PlayStation or Post Script ps Out[5]: <pre>[(3,), ()]</pre> <p>which reads: first tensor had shape <code>h, w, *and 3*</code>, while second tensor had shape <code>h, w *and nothing more*</code>. That's just enough to reverse packing:</p> In\u00a0[6]: Copied! <pre># remove 1-axis in depth image during unpacking. Results are (h, w, 3) and (h, w)\nunpacked_rgb, unpacked_depth = unpack(image_rgbd, ps, \"h w *\")\nunpacked_rgb.shape, unpacked_depth.shape\n</pre> # remove 1-axis in depth image during unpacking. Results are (h, w, 3) and (h, w) unpacked_rgb, unpacked_depth = unpack(image_rgbd, ps, \"h w *\") unpacked_rgb.shape, unpacked_depth.shape Out[6]: <pre>((100, 200, 3), (100, 200))</pre> <p>we can unpack tensor in different ways manually:</p> In\u00a0[7]: Copied! <pre># simple unpack by splitting the axis. Results are (h, w, 3) and (h, w, 1)\nrgb, depth = unpack(image_rgbd, [[3], [1]], \"h w *\")\n# different split, both outputs have shape (h, w, 2)\nrg, bd = unpack(image_rgbd, [[2], [2]], \"h w *\")\n# unpack to 4 tensors of shape (h, w). More like 'unstack over last axis'\n[r, g, b, d] = unpack(image_rgbd, [[], [], [], []], \"h w *\")\n</pre> # simple unpack by splitting the axis. Results are (h, w, 3) and (h, w, 1) rgb, depth = unpack(image_rgbd, [[3], [1]], \"h w *\") # different split, both outputs have shape (h, w, 2) rg, bd = unpack(image_rgbd, [[2], [2]], \"h w *\") # unpack to 4 tensors of shape (h, w). More like 'unstack over last axis' [r, g, b, d] = unpack(image_rgbd, [[], [], [], []], \"h w *\") In\u00a0[8]: Copied! <pre>from einops import reduce\ndef image_classifier(images_bhwc):\n    # mock for image classifier\n    predictions = reduce(images_bhwc, \"b h w c -&gt; b c\", \"mean\", h=100, w=200, c=3)\n    return predictions\n\n\ndef universal_predict(x):\n    x_packed, ps = pack([x], \"* h w c\")\n    predictions_packed = image_classifier(x_packed)\n    [predictions] = unpack(predictions_packed, ps, \"* cls\")\n    return predictions\n</pre> from einops import reduce def image_classifier(images_bhwc):     # mock for image classifier     predictions = reduce(images_bhwc, \"b h w c -&gt; b c\", \"mean\", h=100, w=200, c=3)     return predictions   def universal_predict(x):     x_packed, ps = pack([x], \"* h w c\")     predictions_packed = image_classifier(x_packed)     [predictions] = unpack(predictions_packed, ps, \"* cls\")     return predictions In\u00a0[9]: Copied! <pre># works with a single image\nprint(universal_predict(np.zeros([h, w, 3])).shape)\n# works with a batch of images\nbatch = 5\nprint(universal_predict(np.zeros([batch, h, w, 3])).shape)\n# or even a batch of videos\nn_frames = 7\nprint(universal_predict(np.zeros([batch, n_frames, h, w, 3])).shape)\n</pre> # works with a single image print(universal_predict(np.zeros([h, w, 3])).shape) # works with a batch of images batch = 5 print(universal_predict(np.zeros([batch, h, w, 3])).shape) # or even a batch of videos n_frames = 7 print(universal_predict(np.zeros([batch, n_frames, h, w, 3])).shape) <pre>(3,)\n(5, 3)\n(5, 7, 3)\n</pre> <p>what we can learn from this example:</p> <ul> <li><code>pack</code> and <code>unpack</code> play nicely together. That's not a coincidence :)</li> <li>patterns in <code>pack</code> and <code>unpack</code> may differ, and that's quite common for applications</li> <li>unlike other operations in <code>einops</code>, <code>(un)pack</code> does not provide arbitrary reordering of axes</li> </ul> In\u00a0[10]: Copied! <pre>def transformer_mock(x_btc):\n    # imagine this is a transformer model, a very efficient one\n    assert len(x_btc.shape) == 3\n    return x_btc\n</pre> def transformer_mock(x_btc):     # imagine this is a transformer model, a very efficient one     assert len(x_btc.shape) == 3     return x_btc <p>Let's implement vision transformer (ViT) with a class token (i.e. static token, corresponding output is used to classify an image)</p> In\u00a0[11]: Copied! <pre># below it is assumed that you already\n# 1) split batch of images into patches 2) applied linear projection and 3) used positional embedding.\n\n# We'll skip that here. But hey, here is an einops-style way of doing all of that in a single shot!\n# from einops.layers.torch import EinMix\n# patcher_and_posembedder = EinMix('b (h h2) (w w2) c -&gt; b h w c_out', weight_shape='h2 w2 c c_out',\n#                                  bias_shape='h w c_out', h2=..., w2=...)\n# patch_tokens_bhwc = patcher_and_posembedder(images_bhwc)\n</pre> # below it is assumed that you already # 1) split batch of images into patches 2) applied linear projection and 3) used positional embedding.  # We'll skip that here. But hey, here is an einops-style way of doing all of that in a single shot! # from einops.layers.torch import EinMix # patcher_and_posembedder = EinMix('b (h h2) (w w2) c -&gt; b h w c_out', weight_shape='h2 w2 c c_out', #                                  bias_shape='h w c_out', h2=..., w2=...) # patch_tokens_bhwc = patcher_and_posembedder(images_bhwc) In\u00a0[12]: Copied! <pre># preparations\nbatch, height, width, c = 6, 16, 16, 256\npatch_tokens = np.random.random([batch, height, width, c])\nclass_tokens = np.zeros([batch, c])\n</pre> # preparations batch, height, width, c = 6, 16, 16, 256 patch_tokens = np.random.random([batch, height, width, c]) class_tokens = np.zeros([batch, c]) In\u00a0[13]: Copied! <pre>def vit_einops(class_tokens, patch_tokens):\n    input_packed, ps = pack([class_tokens, patch_tokens], \"b * c\")\n    output_packed = transformer_mock(input_packed)\n    return unpack(output_packed, ps, \"b * c_out\")\n\nclass_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)\n\nclass_token_emb.shape, patch_tokens_emb.shape\n</pre> def vit_einops(class_tokens, patch_tokens):     input_packed, ps = pack([class_tokens, patch_tokens], \"b * c\")     output_packed = transformer_mock(input_packed)     return unpack(output_packed, ps, \"b * c_out\")  class_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)  class_token_emb.shape, patch_tokens_emb.shape Out[13]: <pre>((6, 256), (6, 16, 16, 256))</pre> <p>At this point, let's make a small pause and understand conveniences of this pipeline, by contrasting it to more 'standard' code</p> In\u00a0[14]: Copied! <pre>def vit_vanilla(class_tokens, patch_tokens):\n    b, h, w, c = patch_tokens.shape\n    class_tokens_b1c = class_tokens[:, np.newaxis, :]\n    patch_tokens_btc = np.reshape(patch_tokens, [b, -1, c])\n    input_packed = np.concatenate([class_tokens_b1c, patch_tokens_btc], axis=1)\n    output_packed = transformer_mock(input_packed)\n    class_token_emb = np.squeeze(output_packed[:, :1, :], 1)\n    patch_tokens_emb = np.reshape(output_packed[:, 1:, :], [b, h, w, -1])\n    return class_token_emb, patch_tokens_emb\n\nclass_token_emb2, patch_tokens_emb2 = vit_vanilla(class_tokens, patch_tokens)\nassert np.allclose(class_token_emb, class_token_emb2)\nassert np.allclose(patch_tokens_emb, patch_tokens_emb2)\n</pre> def vit_vanilla(class_tokens, patch_tokens):     b, h, w, c = patch_tokens.shape     class_tokens_b1c = class_tokens[:, np.newaxis, :]     patch_tokens_btc = np.reshape(patch_tokens, [b, -1, c])     input_packed = np.concatenate([class_tokens_b1c, patch_tokens_btc], axis=1)     output_packed = transformer_mock(input_packed)     class_token_emb = np.squeeze(output_packed[:, :1, :], 1)     patch_tokens_emb = np.reshape(output_packed[:, 1:, :], [b, h, w, -1])     return class_token_emb, patch_tokens_emb  class_token_emb2, patch_tokens_emb2 = vit_vanilla(class_tokens, patch_tokens) assert np.allclose(class_token_emb, class_token_emb2) assert np.allclose(patch_tokens_emb, patch_tokens_emb2) <p>Notably, we have put all packing and unpacking, reshapes, adding and removing of dummy axes into a couple of lines.</p> In\u00a0[15]: Copied! <pre>def loss_detection(model_output_bhwc, mask_h: int, mask_w: int, n_classes: int):\n    output = model_output_bhwc\n\n    confidence = output[..., 0].sigmoid()\n    bbox_x_shift = output[..., 1].sigmoid()\n    bbox_y_shift = output[..., 2].sigmoid()\n    bbox_w = output[..., 3]\n    bbox_h = output[..., 4]\n    mask_logits = output[..., 5: 5 + mask_h * mask_w]\n    mask_logits = mask_logits.reshape([*mask_logits.shape[:-1], mask_h, mask_w])\n    class_logits = output[..., 5 + mask_h * mask_w:]\n    assert class_logits.shape[-1] == n_classes, class_logits.shape[-1]\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n</pre> def loss_detection(model_output_bhwc, mask_h: int, mask_w: int, n_classes: int):     output = model_output_bhwc      confidence = output[..., 0].sigmoid()     bbox_x_shift = output[..., 1].sigmoid()     bbox_y_shift = output[..., 2].sigmoid()     bbox_w = output[..., 3]     bbox_h = output[..., 4]     mask_logits = output[..., 5: 5 + mask_h * mask_w]     mask_logits = mask_logits.reshape([*mask_logits.shape[:-1], mask_h, mask_w])     class_logits = output[..., 5 + mask_h * mask_w:]     assert class_logits.shape[-1] == n_classes, class_logits.shape[-1]      # downstream computations     return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits <p>When the same logic is implemented in einops, there is no need to memorize offsets.  Additionally, reshapes and shape checks are automatic:</p> In\u00a0[16]: Copied! <pre>def loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):\n    confidence, bbox_x_shift, bbox_y_shift, bbox_w, bbox_h, mask_logits, class_logits \\\n        = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], \"b h w *\")\n\n    confidence = confidence.sigmoid()\n    bbox_x_shift = bbox_x_shift.sigmoid()\n    bbox_y_shift = bbox_y_shift.sigmoid()\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n</pre> def loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):     confidence, bbox_x_shift, bbox_y_shift, bbox_w, bbox_h, mask_logits, class_logits \\         = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], \"b h w *\")      confidence = confidence.sigmoid()     bbox_x_shift = bbox_x_shift.sigmoid()     bbox_y_shift = bbox_y_shift.sigmoid()      # downstream computations     return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits In\u00a0[17]: Copied! <pre># check that results are identical\nimport torch\ndims = dict(mask_h=6, mask_w=8, n_classes=19)\nmodel_output = torch.randn([3, 5, 7, 5 + dims[\"mask_h\"] * dims[\"mask_w\"] + dims[\"n_classes\"]])\nfor a, b in zip(loss_detection(model_output, **dims), loss_detection_einops(model_output, **dims)):\n    assert torch.allclose(a, b)\n</pre> # check that results are identical import torch dims = dict(mask_h=6, mask_w=8, n_classes=19) model_output = torch.randn([3, 5, 7, 5 + dims[\"mask_h\"] * dims[\"mask_w\"] + dims[\"n_classes\"]]) for a, b in zip(loss_detection(model_output, **dims), loss_detection_einops(model_output, **dims)):     assert torch.allclose(a, b) <p>Or maybe reinforcement learning is closer to your mind?</p> <p>If so, predicting multiple outputs is valuable there too:</p> <pre>action_logits, reward_expectation, q_values, expected_entropy_after_action = \\\n    unpack(predictions_btc, [[n_actions], [], [n_actions], [n_actions]], 'b step *')\n</pre>"},{"location":"4-pack-and-unpack/#einopspack-and-einopsunpack","title":"einops.pack and einops.unpack\u00b6","text":"<p>einops 0.6 introduces two more functions to the family: <code>pack</code> and <code>unpack</code>.</p> <p>Here is what they do:</p> <ul> <li><code>unpack</code> reverses <code>pack</code></li> <li><code>pack</code> reverses <code>unpack</code></li> </ul> <p>Enlightened with this exhaustive description, let's move to examples.</p>"},{"location":"4-pack-and-unpack/#stacking-data-layers","title":"Stacking data layers\u00b6","text":"<p>Assume we have RGB image along with a corresponding depth image that we want to stack:</p>"},{"location":"4-pack-and-unpack/#how-to-read-packing-patterns","title":"How to read packing patterns\u00b6","text":"<p>pattern <code>h w *</code> means that</p> <ul> <li>output is 3-dimensional</li> <li>first two axes (<code>h</code> and <code>w</code>) are shared across all inputs and also shared with output</li> <li>inputs, however do not have to be 3-dimensional. They can be 2-dim, 3-dim, 4-dim, etc.  Regardless of inputs dimensionality, they all will be packed into 3-dim output, and information about how they were packed is stored in <code>PS</code></li> </ul>"},{"location":"4-pack-and-unpack/#short-summary-so-far","title":"Short summary so far\u00b6","text":"<ul> <li><code>einops.pack</code> is a 'more generic concatenation' (that can stack too)</li> <li><code>einops.unpack</code> is a 'more generic split'</li> </ul> <p>And, of course, <code>einops</code> functions are more verbose, and reversing concatenation now is dead simple</p> <p>Compared to other <code>einops</code> functions, <code>pack</code> and <code>unpack</code> have a compact pattern without arrow, and the same pattern can be used in <code>pack</code> and <code>unpack</code>. These patterns are very simplistic: just a sequence of space-separated axes names. One axis is <code>*</code>, all other axes are valid identifiers.</p> <p>Now let's discuss some practical cases</p>"},{"location":"4-pack-and-unpack/#auto-batching","title":"Auto-batching\u00b6","text":"<p>ML models by default accept batches: batch of images, or batch of sentences, or batch of audios, etc.</p> <p>During debugging or inference, however, it is common to pass a single image instead (and thus output should be a single prediction)  In this example we'll write <code>universal_predict</code> that can handle both cases.</p>"},{"location":"4-pack-and-unpack/#class-token-in-vit","title":"Class token in VIT\u00b6","text":"<p>Let's assume we have a simple transformer model that works with <code>BTC</code>-shaped tensors.</p>"},{"location":"4-pack-and-unpack/#packing-different-modalities-together","title":"Packing different modalities together\u00b6","text":"<p>We can extend the previous example: it is quite common to mix elements of different types of inputs in transformers.</p> <p>The simples one is to mix tokens from all inputs:</p> <pre>all_inputs = [text_tokens_btc, image_bhwc, task_token_bc, static_tokens_bnc]\ninputs_packed, ps = pack(all_inputs, 'b * c')\n</pre> <p>and you can <code>unpack</code> resulting tokens to the same structure.</p>"},{"location":"4-pack-and-unpack/#packing-data-coming-from-different-sources-together","title":"Packing data coming from different sources together\u00b6","text":"<p>Most notable example is of course GANs:</p> <pre>input_ims, ps = pack([true_images, fake_images], '* h w c')\ntrue_pred, fake_pred = unpack(model(input_ims), ps, '* c')\n</pre> <p><code>true_pred</code> and <code>fake_pred</code> are handled differently, that's why we separated them</p>"},{"location":"4-pack-and-unpack/#predicting-multiple-outputs-at-the-same-time","title":"Predicting multiple outputs at the same time\u00b6","text":"<p>It is quite common to pack prediction of multiple target values into a single layer.</p> <p>This is more efficient, but code is less readable. For example, that's how detection code may look like:</p>"},{"location":"4-pack-and-unpack/#thats-all-for-today","title":"That's all for today!\u00b6","text":"<p>happy packing and unpacking!</p>"},{"location":"api/asnumpy/","title":"einops.asnumpy","text":"<p>Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/jax/etc.) to <code>numpy.ndarray</code></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>tensor of any known imperative framework</p> required <p>Returns:</p> Type Description <code>np_ndarray</code> <p><code>numpy.ndarray</code>, converted to numpy</p> Source code in <code>einops/einops.py</code> <pre><code>def asnumpy(tensor: Tensor) -&gt; np_ndarray:\n    \"\"\"\n    Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/jax/etc.) to `numpy.ndarray`\n\n    Parameters:\n        tensor: tensor of any known imperative framework\n\n    Returns:\n        `numpy.ndarray`, converted to numpy\n    \"\"\"\n    return get_backend(tensor).to_numpy(tensor)\n</code></pre>"},{"location":"api/einsum/","title":"einops.einsum","text":"<p>einops.einsum calls einsum operations with einops-style named axes indexing, computing tensor products with an arbitrary number of tensors. Unlike typical einsum syntax, here you must pass tensors first, and then the pattern.</p> <p>Also, note that rearrange operations such as <code>\"(batch chan) out\"</code>, or singleton axes <code>()</code>, are not currently supported.</p> <p>Examples:</p> <p>For a given pattern such as:</p> <pre><code>&gt;&gt;&gt; x, y, z = np.random.randn(3, 20, 20, 20)\n&gt;&gt;&gt; output = einsum(x, y, z, \"a b c, c b d, a g k -&gt; a b k\")\n</code></pre> <p>the following formula is computed:</p> <pre><code>output[a, b, k] = \\sum_{c, d, g} x[a, b, c] * y[c, b, d] * z[a, g, k]\n</code></pre> <p>where the summation over <code>c</code>, <code>d</code>, and <code>g</code> is performed because those axes names do not appear on the right-hand side.</p> <p>Let's see some additional examples:</p> <pre><code># Filter a set of images:\n&gt;&gt;&gt; batched_images = np.random.randn(128, 16, 16)\n&gt;&gt;&gt; filters = np.random.randn(16, 16, 30)\n&gt;&gt;&gt; result = einsum(batched_images, filters,\n...                 \"batch h w, h w channel -&gt; batch channel\")\n&gt;&gt;&gt; result.shape\n(128, 30)\n\n# Matrix multiplication, with an unknown input shape:\n&gt;&gt;&gt; batch_shape = (50, 30)\n&gt;&gt;&gt; data = np.random.randn(*batch_shape, 20)\n&gt;&gt;&gt; weights = np.random.randn(10, 20)\n&gt;&gt;&gt; result = einsum(weights, data,\n...                 \"out_dim in_dim, ... in_dim -&gt; ... out_dim\")\n&gt;&gt;&gt; result.shape\n(50, 30, 10)\n\n# Matrix trace on a single tensor:\n&gt;&gt;&gt; matrix = np.random.randn(10, 10)\n&gt;&gt;&gt; result = einsum(matrix, \"i i -&gt;\")\n&gt;&gt;&gt; result.shape\n()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tensors_and_pattern</code> <code>Union[Tensor, str]</code> <p>tensors: tensors of any supported library (numpy, tensorflow, pytorch, jax). pattern: string, einsum pattern, with commas     separating specifications for each tensor.     pattern should be provided after all tensors.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of the same type as input, after processing with einsum.</p> Source code in <code>einops/einops.py</code> <pre><code>def einsum(*tensors_and_pattern: Union[Tensor, str]) -&gt; Tensor:\n    r\"\"\"\n    einops.einsum calls einsum operations with einops-style named\n    axes indexing, computing tensor products with an arbitrary\n    number of tensors. Unlike typical einsum syntax, here you must\n    pass tensors first, and then the pattern.\n\n    Also, note that rearrange operations such as `\"(batch chan) out\"`,\n    or singleton axes `()`, are not currently supported.\n\n    Examples:\n\n    For a given pattern such as:\n    ```python\n    &gt;&gt;&gt; x, y, z = np.random.randn(3, 20, 20, 20)\n    &gt;&gt;&gt; output = einsum(x, y, z, \"a b c, c b d, a g k -&gt; a b k\")\n\n    ```\n    the following formula is computed:\n    ```tex\n    output[a, b, k] = \\sum_{c, d, g} x[a, b, c] * y[c, b, d] * z[a, g, k]\n    ```\n    where the summation over `c`, `d`, and `g` is performed\n    because those axes names do not appear on the right-hand side.\n\n    Let's see some additional examples:\n    ```python\n    # Filter a set of images:\n    &gt;&gt;&gt; batched_images = np.random.randn(128, 16, 16)\n    &gt;&gt;&gt; filters = np.random.randn(16, 16, 30)\n    &gt;&gt;&gt; result = einsum(batched_images, filters,\n    ...                 \"batch h w, h w channel -&gt; batch channel\")\n    &gt;&gt;&gt; result.shape\n    (128, 30)\n\n    # Matrix multiplication, with an unknown input shape:\n    &gt;&gt;&gt; batch_shape = (50, 30)\n    &gt;&gt;&gt; data = np.random.randn(*batch_shape, 20)\n    &gt;&gt;&gt; weights = np.random.randn(10, 20)\n    &gt;&gt;&gt; result = einsum(weights, data,\n    ...                 \"out_dim in_dim, ... in_dim -&gt; ... out_dim\")\n    &gt;&gt;&gt; result.shape\n    (50, 30, 10)\n\n    # Matrix trace on a single tensor:\n    &gt;&gt;&gt; matrix = np.random.randn(10, 10)\n    &gt;&gt;&gt; result = einsum(matrix, \"i i -&gt;\")\n    &gt;&gt;&gt; result.shape\n    ()\n\n    ```\n\n    Parameters:\n        tensors_and_pattern:\n            tensors: tensors of any supported library (numpy, tensorflow, pytorch, jax).\n            pattern: string, einsum pattern, with commas\n                separating specifications for each tensor.\n                pattern should be provided after all tensors.\n\n    Returns:\n        Tensor of the same type as input, after processing with einsum.\n\n    \"\"\"\n    if len(tensors_and_pattern) &lt;= 1:\n        raise ValueError(\n            \"`einops.einsum` takes at minimum two arguments: the tensors (at least one), followed by the pattern.\"\n        )\n    pattern = tensors_and_pattern[-1]\n    if not isinstance(pattern, str):\n        raise ValueError(\n            \"The last argument passed to `einops.einsum` must be a string, representing the einsum pattern.\"\n        )\n    tensors = tensors_and_pattern[:-1]\n    pattern = _compactify_pattern_for_einsum(pattern)\n    return get_backend(tensors[0]).einsum(pattern, *tensors)\n</code></pre>"},{"location":"api/pack_unpack/","title":"einops.pack and einops.unpack","text":""},{"location":"api/pack_unpack/#einopspack","title":"einops.pack","text":"<p>Packs several tensors into one. See einops tutorial for introduction into packing (and how it replaces stack and concatenation).</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Sequence[Tensor]</code> <p>tensors to be packed, can be of different dimensionality</p> required <code>pattern</code> <code>str</code> <p>pattern that is shared for all inputs and output, e.g. \"i j * k\" or \"batch seq *\"</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, list[Shape]]</code> <p>(packed_tensor, packed_shapes aka PS)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from numpy import zeros as Z\n&gt;&gt;&gt; inputs = [Z([2, 3, 5]), Z([2, 3, 7, 5]), Z([2, 3, 7, 9, 5])]\n&gt;&gt;&gt; packed, ps = pack(inputs, 'i j * k')\n&gt;&gt;&gt; packed.shape, ps\n((2, 3, 71, 5), [(), (7,), (7, 9)])\n</code></pre> <p>In this example, axes were matched to: i=2, j=3, k=5 based on order (first, second, and last). All other axes were 'packed' and concatenated. PS (packed shapes) contains information about axes that were matched to '*' in every input. Resulting tensor has as many elements as all inputs in total.</p> <p>Packing can be reversed with unpack, which additionally needs PS (packed shapes) to reconstruct order.</p> <pre><code>&gt;&gt;&gt; inputs_unpacked = unpack(packed, ps, 'i j * k')\n&gt;&gt;&gt; [x.shape for x in inputs_unpacked]\n[(2, 3, 5), (2, 3, 7, 5), (2, 3, 7, 9, 5)]\n</code></pre> <p>Read the tutorial for introduction and application scenarios.</p> Source code in <code>einops/packing.py</code> <pre><code>def pack(tensors: Sequence[Tensor], pattern: str) -&gt; tuple[Tensor, list[Shape]]:\n    \"\"\"\n    Packs several tensors into one.\n    See einops tutorial for introduction into packing (and how it replaces stack and concatenation).\n\n    Parameters:\n        tensors: tensors to be packed, can be of different dimensionality\n        pattern: pattern that is shared for all inputs and output, e.g. \"i j * k\" or \"batch seq *\"\n\n    Returns:\n        (packed_tensor, packed_shapes aka PS)\n\n    Example:\n    ```python\n    &gt;&gt;&gt; from numpy import zeros as Z\n    &gt;&gt;&gt; inputs = [Z([2, 3, 5]), Z([2, 3, 7, 5]), Z([2, 3, 7, 9, 5])]\n    &gt;&gt;&gt; packed, ps = pack(inputs, 'i j * k')\n    &gt;&gt;&gt; packed.shape, ps\n    ((2, 3, 71, 5), [(), (7,), (7, 9)])\n    ```\n\n    In this example, axes were matched to: i=2, j=3, k=5 based on order (first, second, and last).\n    All other axes were 'packed' and concatenated.\n    PS (packed shapes) contains information about axes that were matched to '*' in every input.\n    Resulting tensor has as many elements as all inputs in total.\n\n    Packing can be reversed with unpack, which additionally needs PS (packed shapes) to reconstruct order.\n\n    ```python\n    &gt;&gt;&gt; inputs_unpacked = unpack(packed, ps, 'i j * k')\n    &gt;&gt;&gt; [x.shape for x in inputs_unpacked]\n    [(2, 3, 5), (2, 3, 7, 5), (2, 3, 7, 9, 5)]\n    ```\n\n    Read the tutorial for introduction and application scenarios.\n    \"\"\"\n    n_axes_before, n_axes_after, min_axes = analyze_pattern(pattern, \"pack\")\n\n    # packing zero tensors is illegal\n    backend = get_backend(tensors[0])\n\n    reshaped_tensors: list[Tensor] = []\n    packed_shapes: list[Shape] = []\n    for i, tensor in enumerate(tensors):\n        shape = backend.shape(tensor)\n        if len(shape) &lt; min_axes:\n            raise EinopsError(\n                f\"packed tensor #{i} (enumeration starts with 0) has shape {shape}, \"\n                f\"while pattern {pattern} assumes at least {min_axes} axes\"\n            )\n        axis_after_packed_axes = len(shape) - n_axes_after\n        packed_shapes.append(shape[n_axes_before:axis_after_packed_axes])\n        reshaped_tensors.append(backend.reshape(tensor, (*shape[:n_axes_before], -1, *shape[axis_after_packed_axes:])))\n\n    return backend.concat(reshaped_tensors, axis=n_axes_before), packed_shapes\n</code></pre> <p></p>"},{"location":"api/pack_unpack/#einopsunpack","title":"einops.unpack","text":"<p>Unpacks a single tensor into several by splitting over a selected axes. See einops tutorial for introduction into packing (and how it replaces stack and concatenation).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>tensor to be unpacked</p> required <code>packed_shapes</code> <code>list[Shape]</code> <p>packed_shapes (aka PS) is a list of shapes that take place of '*' in each output. output will contain a single tensor for every provided shape</p> required <code>pattern</code> <code>str</code> <p>pattern that is shared for input and all outputs, e.g. \"i j * k\" or \"batch seq *\", where * designates an axis to be unpacked</p> required <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>list of tensors</p> <p>If framework supports views, results are views to the original tensor.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from numpy import zeros as Z\n&gt;&gt;&gt; inputs = [Z([2, 3, 5]), Z([2, 3, 7, 5]), Z([2, 3, 7, 9, 5])]\n&gt;&gt;&gt; packed, ps = pack(inputs, 'i j * k')\n&gt;&gt;&gt; packed.shape, ps\n((2, 3, 71, 5), [(), (7,), (7, 9)])\n</code></pre> <p>In this example, axes were matched to: i=2, j=3, k=5 based on order (first, second, and last). All other axes were 'packed' and concatenated. PS (packed shapes) contains information about axes that were matched to '*' in every input. Resulting tensor has as many elements as all inputs in total.</p> <p>Packing can be reversed with unpack, which additionally needs PS (packed shapes) to reconstruct order.</p> <pre><code>&gt;&gt;&gt; inputs_unpacked = unpack(packed, ps, 'i j * k')\n&gt;&gt;&gt; [x.shape for x in inputs_unpacked]\n[(2, 3, 5), (2, 3, 7, 5), (2, 3, 7, 9, 5)]\n</code></pre> <p>Read the tutorial for introduction and application scenarios.</p> Source code in <code>einops/packing.py</code> <pre><code>def unpack(tensor: Tensor, packed_shapes: list[Shape], pattern: str) -&gt; list[Tensor]:\n    \"\"\"\n    Unpacks a single tensor into several by splitting over a selected axes.\n    See einops tutorial for introduction into packing (and how it replaces stack and concatenation).\n\n    Parameters:\n        tensor: tensor to be unpacked\n        packed_shapes: packed_shapes (aka PS) is a list of shapes that take place of '*' in each output.\n            output will contain a single tensor for every provided shape\n        pattern: pattern that is shared for input and all outputs, e.g. \"i j * k\" or \"batch seq *\",\n            where * designates an axis to be unpacked\n\n    Returns:\n        list of tensors\n\n    If framework supports views, results are views to the original tensor.\n\n    Example:\n    ```python\n    &gt;&gt;&gt; from numpy import zeros as Z\n    &gt;&gt;&gt; inputs = [Z([2, 3, 5]), Z([2, 3, 7, 5]), Z([2, 3, 7, 9, 5])]\n    &gt;&gt;&gt; packed, ps = pack(inputs, 'i j * k')\n    &gt;&gt;&gt; packed.shape, ps\n    ((2, 3, 71, 5), [(), (7,), (7, 9)])\n    ```\n\n    In this example, axes were matched to: i=2, j=3, k=5 based on order (first, second, and last).\n    All other axes were 'packed' and concatenated.\n    PS (packed shapes) contains information about axes that were matched to '*' in every input.\n    Resulting tensor has as many elements as all inputs in total.\n\n    Packing can be reversed with unpack, which additionally needs PS (packed shapes) to reconstruct order.\n\n    ```python\n    &gt;&gt;&gt; inputs_unpacked = unpack(packed, ps, 'i j * k')\n    &gt;&gt;&gt; [x.shape for x in inputs_unpacked]\n    [(2, 3, 5), (2, 3, 7, 5), (2, 3, 7, 9, 5)]\n    ```\n\n    Read the tutorial for introduction and application scenarios.\n    \"\"\"\n    n_axes_before, n_axes_after, min_axes = analyze_pattern(pattern, opname=\"unpack\")\n\n    backend = get_backend(tensor)\n    input_shape = backend.shape(tensor)\n    if len(input_shape) != n_axes_before + 1 + n_axes_after:\n        raise EinopsError(f\"unpack(..., {pattern}) received input of wrong dim with shape {input_shape}\")\n\n    unpacked_axis: int = n_axes_before\n\n    lengths_of_composed_axes: list[int] = [-1 if -1 in p_shape else prod(p_shape) for p_shape in packed_shapes]\n\n    n_unknown_composed_axes = sum(int(x == -1) for x in lengths_of_composed_axes)\n    if n_unknown_composed_axes &gt; 1:\n        raise EinopsError(\n            f\"unpack(..., {pattern}) received more than one -1 in {packed_shapes} and can't infer dimensions\"\n        )\n\n    # following manipulations allow to skip some shape verifications\n    # and leave it to backends\n\n    # [[], [2, 3], [4], [-1, 5], [6]] &lt; examples of packed_axis\n    # split positions when computed should be\n    # [0,   1,      7,   11,      N-6 , N ], where N = length of axis\n    split_positions = [0] * len(packed_shapes) + [input_shape[unpacked_axis]]\n    if n_unknown_composed_axes == 0:\n        for i, x in enumerate(lengths_of_composed_axes[:-1]):\n            split_positions[i + 1] = split_positions[i] + x\n    else:\n        unknown_composed_axis: int = lengths_of_composed_axes.index(-1)\n        for i in range(unknown_composed_axis):\n            split_positions[i + 1] = split_positions[i] + lengths_of_composed_axes[i]\n        for j in range(unknown_composed_axis + 1, len(lengths_of_composed_axes))[::-1]:\n            split_positions[j] = split_positions[j + 1] - lengths_of_composed_axes[j]\n\n    shape_start = input_shape[:unpacked_axis]\n    shape_end = input_shape[unpacked_axis + 1 :]\n    slice_filler = (slice(None, None),) * unpacked_axis\n    try:\n        return [\n            backend.reshape(\n                # shortest way slice arbitrary axis\n                tensor[(*slice_filler, slice(split_positions[i], split_positions[i + 1]))],\n                (*shape_start, *element_shape, *shape_end),\n            )\n            for i, element_shape in enumerate(packed_shapes)\n        ]\n    except Exception as e:\n        # this hits if there is an error during reshapes, which means passed shapes were incorrect\n        raise EinopsError(\n            f'Error during unpack(..., \"{pattern}\"): could not split axis of size {split_positions[-1]}'\n            f\" into requested {packed_shapes}\"\n        ) from e\n</code></pre>"},{"location":"api/parse_shape/","title":"einops.parse_shape","text":"<p>Parse a tensor shape to dictionary mapping axes names to their lengths.</p> <pre><code># Use underscore to skip the dimension in parsing.\n&gt;&gt;&gt; x = np.zeros([2, 3, 5, 7])\n&gt;&gt;&gt; parse_shape(x, 'batch _ h w')\n{'batch': 2, 'h': 5, 'w': 7}\n\n# `parse_shape` output can be used to specify axes_lengths for other operations:\n&gt;&gt;&gt; y = np.zeros([700])\n&gt;&gt;&gt; rearrange(y, '(b c h w) -&gt; b c h w', **parse_shape(x, 'b _ h w')).shape\n(2, 10, 5, 7)\n</code></pre> <p>For symbolic frameworks may return symbols, not integers.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>tensor of any supported framework</p> required <code>pattern</code> <code>str</code> <p>str, space separated names for axes, underscore means skip axis</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict, maps axes names to their lengths</p> Source code in <code>einops/einops.py</code> <pre><code>def parse_shape(x: Tensor, pattern: str) -&gt; dict:\n    \"\"\"\n    Parse a tensor shape to dictionary mapping axes names to their lengths.\n\n    ```python\n    # Use underscore to skip the dimension in parsing.\n    &gt;&gt;&gt; x = np.zeros([2, 3, 5, 7])\n    &gt;&gt;&gt; parse_shape(x, 'batch _ h w')\n    {'batch': 2, 'h': 5, 'w': 7}\n\n    # `parse_shape` output can be used to specify axes_lengths for other operations:\n    &gt;&gt;&gt; y = np.zeros([700])\n    &gt;&gt;&gt; rearrange(y, '(b c h w) -&gt; b c h w', **parse_shape(x, 'b _ h w')).shape\n    (2, 10, 5, 7)\n\n    ```\n\n    For symbolic frameworks may return symbols, not integers.\n\n    Parameters:\n        x: tensor of any supported framework\n        pattern: str, space separated names for axes, underscore means skip axis\n\n    Returns:\n        dict, maps axes names to their lengths\n    \"\"\"\n    exp = ParsedExpression(pattern, allow_underscore=True)\n    shape = get_backend(x).shape(x)\n    if exp.has_composed_axes():\n        raise RuntimeError(f\"Can't parse shape with composite axes: {pattern} {shape}\")\n    if len(shape) != len(exp.composition):\n        if exp.has_ellipsis:\n            if len(shape) &lt; len(exp.composition) - 1:\n                raise RuntimeError(f\"Can't parse shape with this number of dimensions: {pattern} {shape}\")\n        else:\n            raise RuntimeError(f\"Can't parse shape with different number of dimensions: {pattern} {shape}\")\n    if exp.has_ellipsis:\n        ellipsis_idx = exp.composition.index(_ellipsis)\n        composition = (\n            exp.composition[:ellipsis_idx]\n            + [\"_\"] * (len(shape) - len(exp.composition) + 1)\n            + exp.composition[ellipsis_idx + 1 :]\n        )\n    else:\n        composition = exp.composition\n    result = {}\n    for axes, axis_length in zip(composition, shape):  # type: ignore\n        # axes either [], or [AnonymousAxis] or ['axis_name']\n        if len(axes) == 0:\n            if axis_length != 1:\n                raise RuntimeError(f\"Length of axis is not 1: {pattern} {shape}\")\n        else:\n            [axis] = axes\n            if isinstance(axis, str):\n                if axis != \"_\":\n                    result[axis] = axis_length\n            else:\n                if axis.value != axis_length:\n                    raise RuntimeError(f\"Length of anonymous axis does not match: {pattern} {shape}\")\n    return result\n</code></pre>"},{"location":"api/rearrange/","title":"einops.rearrange","text":"<p>einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations.</p> <p>Examples:</p> <pre><code># suppose we have a set of 32 images in \"h w c\" format (height-width-channel)\n&gt;&gt;&gt; images = [np.random.randn(30, 40, 3) for _ in range(32)]\n\n# stack along first (batch) axis, output is a single array\n&gt;&gt;&gt; rearrange(images, 'b h w c -&gt; b h w c').shape\n(32, 30, 40, 3)\n\n# stacked and reordered axes to \"b c h w\" format\n&gt;&gt;&gt; rearrange(images, 'b h w c -&gt; b c h w').shape\n(32, 3, 30, 40)\n\n# concatenate images along height (vertical axis), 960 = 32 * 30\n&gt;&gt;&gt; rearrange(images, 'b h w c -&gt; (b h) w c').shape\n(960, 40, 3)\n\n# concatenated images along horizontal axis, 1280 = 32 * 40\n&gt;&gt;&gt; rearrange(images, 'b h w c -&gt; h (b w) c').shape\n(30, 1280, 3)\n\n# flattened each image into a vector, 3600 = 30 * 40 * 3\n&gt;&gt;&gt; rearrange(images, 'b h w c -&gt; b (c h w)').shape\n(32, 3600)\n\n# split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n&gt;&gt;&gt; rearrange(images, 'b (h1 h) (w1 w) c -&gt; (b h1 w1) h w c', h1=2, w1=2).shape\n(128, 15, 20, 3)\n\n# space-to-depth operation\n&gt;&gt;&gt; rearrange(images, 'b (h h1) (w w1) c -&gt; b h w (c h1 w1)', h1=2, w1=2).shape\n(32, 15, 20, 12)\n</code></pre> <p>When composing axes, C-order enumeration used (consecutive elements have different last axis). Find more examples in einops tutorial.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Union[Tensor, list[Tensor]]</code> <p>tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).     list of tensors is also accepted, those should be of the same type and shape</p> required <code>pattern</code> <code>str</code> <p>string, rearrangement pattern</p> required <code>axes_lengths</code> <code>Size</code> <p>any additional specifications for dimensions</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tensor of the same type as input. If possible, a view to the original tensor is returned.</p> Source code in <code>einops/einops.py</code> <pre><code>def rearrange(tensor: Union[Tensor, list[Tensor]], pattern: str, **axes_lengths: Size) -&gt; Tensor:\n    \"\"\"\n    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\n    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\n    stack, concatenate and other operations.\n\n    Examples:\n\n    ```python\n    # suppose we have a set of 32 images in \"h w c\" format (height-width-channel)\n    &gt;&gt;&gt; images = [np.random.randn(30, 40, 3) for _ in range(32)]\n\n    # stack along first (batch) axis, output is a single array\n    &gt;&gt;&gt; rearrange(images, 'b h w c -&gt; b h w c').shape\n    (32, 30, 40, 3)\n\n    # stacked and reordered axes to \"b c h w\" format\n    &gt;&gt;&gt; rearrange(images, 'b h w c -&gt; b c h w').shape\n    (32, 3, 30, 40)\n\n    # concatenate images along height (vertical axis), 960 = 32 * 30\n    &gt;&gt;&gt; rearrange(images, 'b h w c -&gt; (b h) w c').shape\n    (960, 40, 3)\n\n    # concatenated images along horizontal axis, 1280 = 32 * 40\n    &gt;&gt;&gt; rearrange(images, 'b h w c -&gt; h (b w) c').shape\n    (30, 1280, 3)\n\n    # flattened each image into a vector, 3600 = 30 * 40 * 3\n    &gt;&gt;&gt; rearrange(images, 'b h w c -&gt; b (c h w)').shape\n    (32, 3600)\n\n    # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n    &gt;&gt;&gt; rearrange(images, 'b (h1 h) (w1 w) c -&gt; (b h1 w1) h w c', h1=2, w1=2).shape\n    (128, 15, 20, 3)\n\n    # space-to-depth operation\n    &gt;&gt;&gt; rearrange(images, 'b (h h1) (w w1) c -&gt; b h w (c h1 w1)', h1=2, w1=2).shape\n    (32, 15, 20, 12)\n\n    ```\n\n    When composing axes, C-order enumeration used (consecutive elements have different last axis).\n    Find more examples in einops tutorial.\n\n    Parameters:\n        tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).\n                list of tensors is also accepted, those should be of the same type and shape\n        pattern: string, rearrangement pattern\n        axes_lengths: any additional specifications for dimensions\n\n    Returns:\n        tensor of the same type as input. If possible, a view to the original tensor is returned.\n\n    \"\"\"\n    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\n</code></pre>"},{"location":"api/reduce/","title":"einops.reduce","text":"<p>einops.reduce combines rearrangement and reduction using reader-friendly notation.</p> <p>Some examples:</p> <pre><code>&gt;&gt;&gt; x = np.random.randn(100, 32, 64)\n\n# perform max-reduction on the first axis\n# Axis t does not appear on RHS - thus we reduced over t\n&gt;&gt;&gt; y = reduce(x, 't b c -&gt; b c', 'max')\n\n# same as previous, but using verbose names for axes\n&gt;&gt;&gt; y = reduce(x, 'time batch channel -&gt; batch channel', 'max')\n\n# let's pretend now that x is a batch of images\n# with 4 dims: batch=10, height=20, width=30, channel=40\n&gt;&gt;&gt; x = np.random.randn(10, 20, 30, 40)\n\n# 2d max-pooling with kernel size = 2 * 2 for image processing\n&gt;&gt;&gt; y1 = reduce(x, 'b c (h1 h2) (w1 w2) -&gt; b c h1 w1', 'max', h2=2, w2=2)\n\n# same as previous, using anonymous axes,\n# note: only reduced axes can be anonymous\n&gt;&gt;&gt; y1 = reduce(x, 'b c (h1 2) (w1 2) -&gt; b c h1 w1', 'max')\n\n# adaptive 2d max-pooling to 3 * 4 grid,\n# each element is max of 10x10 tile in the original tensor.\n&gt;&gt;&gt; reduce(x, 'b c (h1 h2) (w1 w2) -&gt; b c h1 w1', 'max', h1=3, w1=4).shape\n(10, 20, 3, 4)\n\n# Global average pooling\n&gt;&gt;&gt; reduce(x, 'b c h w -&gt; b c', 'mean').shape\n(10, 20)\n\n# subtracting mean over batch for each channel;\n# similar to x - np.mean(x, axis=(0, 2, 3), keepdims=True)\n&gt;&gt;&gt; y = x - reduce(x, 'b c h w -&gt; 1 c 1 1', 'mean')\n\n# Subtracting per-image mean for each channel\n&gt;&gt;&gt; y = x - reduce(x, 'b c h w -&gt; b c 1 1', 'mean')\n\n# same as previous, but using empty compositions\n&gt;&gt;&gt; y = x - reduce(x, 'b c h w -&gt; b c () ()', 'mean')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Union[Tensor, list[Tensor]]</code> <p>tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch). list of tensors is also accepted, those should be of the same type and shape</p> required <code>pattern</code> <code>str</code> <p>string, reduction pattern</p> required <code>reduction</code> <code>Reduction</code> <p>one of available reductions ('min', 'max', 'sum', 'mean', 'prod', 'any', 'all'). Alternatively, a callable f(tensor, reduced_axes) -&gt; tensor can be provided. This allows using various reductions like: np.max, np.nanmean, tf.reduce_logsumexp, torch.var, etc.</p> required <code>axes_lengths</code> <code>Size</code> <p>any additional specifications for dimensions</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tensor of the same type as input</p> Source code in <code>einops/einops.py</code> <pre><code>def reduce(tensor: Union[Tensor, list[Tensor]], pattern: str, reduction: Reduction, **axes_lengths: Size) -&gt; Tensor:\n    \"\"\"\n    einops.reduce combines rearrangement and reduction using reader-friendly notation.\n\n    Some examples:\n\n    ```python\n    &gt;&gt;&gt; x = np.random.randn(100, 32, 64)\n\n    # perform max-reduction on the first axis\n    # Axis t does not appear on RHS - thus we reduced over t\n    &gt;&gt;&gt; y = reduce(x, 't b c -&gt; b c', 'max')\n\n    # same as previous, but using verbose names for axes\n    &gt;&gt;&gt; y = reduce(x, 'time batch channel -&gt; batch channel', 'max')\n\n    # let's pretend now that x is a batch of images\n    # with 4 dims: batch=10, height=20, width=30, channel=40\n    &gt;&gt;&gt; x = np.random.randn(10, 20, 30, 40)\n\n    # 2d max-pooling with kernel size = 2 * 2 for image processing\n    &gt;&gt;&gt; y1 = reduce(x, 'b c (h1 h2) (w1 w2) -&gt; b c h1 w1', 'max', h2=2, w2=2)\n\n    # same as previous, using anonymous axes,\n    # note: only reduced axes can be anonymous\n    &gt;&gt;&gt; y1 = reduce(x, 'b c (h1 2) (w1 2) -&gt; b c h1 w1', 'max')\n\n    # adaptive 2d max-pooling to 3 * 4 grid,\n    # each element is max of 10x10 tile in the original tensor.\n    &gt;&gt;&gt; reduce(x, 'b c (h1 h2) (w1 w2) -&gt; b c h1 w1', 'max', h1=3, w1=4).shape\n    (10, 20, 3, 4)\n\n    # Global average pooling\n    &gt;&gt;&gt; reduce(x, 'b c h w -&gt; b c', 'mean').shape\n    (10, 20)\n\n    # subtracting mean over batch for each channel;\n    # similar to x - np.mean(x, axis=(0, 2, 3), keepdims=True)\n    &gt;&gt;&gt; y = x - reduce(x, 'b c h w -&gt; 1 c 1 1', 'mean')\n\n    # Subtracting per-image mean for each channel\n    &gt;&gt;&gt; y = x - reduce(x, 'b c h w -&gt; b c 1 1', 'mean')\n\n    # same as previous, but using empty compositions\n    &gt;&gt;&gt; y = x - reduce(x, 'b c h w -&gt; b c () ()', 'mean')\n\n    ```\n\n    Parameters:\n        tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).\n            list of tensors is also accepted, those should be of the same type and shape\n        pattern: string, reduction pattern\n        reduction: one of available reductions ('min', 'max', 'sum', 'mean', 'prod', 'any', 'all').\n            Alternatively, a callable f(tensor, reduced_axes) -&gt; tensor can be provided.\n            This allows using various reductions like: np.max, np.nanmean, tf.reduce_logsumexp, torch.var, etc.\n        axes_lengths: any additional specifications for dimensions\n\n    Returns:\n        tensor of the same type as input\n    \"\"\"\n    try:\n        if isinstance(tensor, list):\n            if len(tensor) == 0:\n                raise TypeError(\"Rearrange/Reduce/Repeat can't be applied to an empty list\")\n            backend = get_backend(tensor[0])\n            tensor = backend.stack_on_zeroth_dimension(tensor)\n        else:\n            backend = get_backend(tensor)\n\n        hashable_axes_lengths = tuple(axes_lengths.items())\n        shape = backend.shape(tensor)\n        recipe = _prepare_transformation_recipe(pattern, reduction, axes_names=tuple(axes_lengths), ndim=len(shape))\n        return _apply_recipe(\n            backend, recipe, cast(Tensor, tensor), reduction_type=reduction, axes_lengths=hashable_axes_lengths\n        )\n    except EinopsError as e:\n        message = f' Error while processing {reduction}-reduction pattern \"{pattern}\".'\n        if not isinstance(tensor, list):\n            message += f\"\\n Input tensor shape: {shape}. \"\n        else:\n            message += \"\\n Input is list. \"\n        message += f\"Additional info: {axes_lengths}.\"\n        raise EinopsError(message + f\"\\n {e}\") from None\n</code></pre>"},{"location":"api/repeat/","title":"einops.repeat","text":"<p>einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, and broadcast functions.</p> <p>Examples for repeat operation:</p> <pre><code># a grayscale image (of shape height x width)\n&gt;&gt;&gt; image = np.random.randn(30, 40)\n\n# change it to RGB format by repeating in each channel\n&gt;&gt;&gt; repeat(image, 'h w -&gt; h w c', c=3).shape\n(30, 40, 3)\n\n# repeat image 2 times along height (vertical axis)\n&gt;&gt;&gt; repeat(image, 'h w -&gt; (repeat h) w', repeat=2).shape\n(60, 40)\n\n# repeat image 2 time along height and 3 times along width\n&gt;&gt;&gt; repeat(image, 'h w -&gt; (h2 h) (w3 w)', h2=2, w3=3).shape\n(60, 120)\n\n# convert each pixel to a small square 2x2, i.e. upsample an image by 2x\n&gt;&gt;&gt; repeat(image, 'h w -&gt; (h h2) (w w2)', h2=2, w2=2).shape\n(60, 80)\n\n# 'pixelate' an image first by downsampling by 2x, then upsampling\n&gt;&gt;&gt; downsampled = reduce(image, '(h h2) (w w2) -&gt; h w', 'mean', h2=2, w2=2)\n&gt;&gt;&gt; repeat(downsampled, 'h w -&gt; (h h2) (w w2)', h2=2, w2=2).shape\n(30, 40)\n</code></pre> <p>When composing axes, C-order enumeration used (consecutive elements have different last axis). Find more examples in einops tutorial.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Union[Tensor, list[Tensor]]</code> <p>tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch). list of tensors is also accepted, those should be of the same type and shape</p> required <code>pattern</code> <code>str</code> <p>string, rearrangement pattern</p> required <code>axes_lengths</code> <code>Size</code> <p>any additional specifications for dimensions</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of the same type as input. If possible, a view to the original tensor is returned.</p> Source code in <code>einops/einops.py</code> <pre><code>def repeat(tensor: Union[Tensor, list[Tensor]], pattern: str, **axes_lengths: Size) -&gt; Tensor:\n    \"\"\"\n    einops.repeat allows reordering elements and repeating them in arbitrary combinations.\n    This operation includes functionality of repeat, tile, and broadcast functions.\n\n    Examples for repeat operation:\n\n    ```python\n    # a grayscale image (of shape height x width)\n    &gt;&gt;&gt; image = np.random.randn(30, 40)\n\n    # change it to RGB format by repeating in each channel\n    &gt;&gt;&gt; repeat(image, 'h w -&gt; h w c', c=3).shape\n    (30, 40, 3)\n\n    # repeat image 2 times along height (vertical axis)\n    &gt;&gt;&gt; repeat(image, 'h w -&gt; (repeat h) w', repeat=2).shape\n    (60, 40)\n\n    # repeat image 2 time along height and 3 times along width\n    &gt;&gt;&gt; repeat(image, 'h w -&gt; (h2 h) (w3 w)', h2=2, w3=3).shape\n    (60, 120)\n\n    # convert each pixel to a small square 2x2, i.e. upsample an image by 2x\n    &gt;&gt;&gt; repeat(image, 'h w -&gt; (h h2) (w w2)', h2=2, w2=2).shape\n    (60, 80)\n\n    # 'pixelate' an image first by downsampling by 2x, then upsampling\n    &gt;&gt;&gt; downsampled = reduce(image, '(h h2) (w w2) -&gt; h w', 'mean', h2=2, w2=2)\n    &gt;&gt;&gt; repeat(downsampled, 'h w -&gt; (h h2) (w w2)', h2=2, w2=2).shape\n    (30, 40)\n\n    ```\n\n    When composing axes, C-order enumeration used (consecutive elements have different last axis).\n    Find more examples in einops tutorial.\n\n    Parameters:\n        tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).\n            list of tensors is also accepted, those should be of the same type and shape\n        pattern: string, rearrangement pattern\n        axes_lengths: any additional specifications for dimensions\n\n    Returns:\n        Tensor of the same type as input. If possible, a view to the original tensor is returned.\n\n    \"\"\"\n    return reduce(tensor, pattern, reduction=\"repeat\", **axes_lengths)\n</code></pre>"},{"location":"docs/","title":"Einops tutorial","text":"<p>You can read notebooks from github by clicking on files above.</p> <p>If you prefer nbviewer, use the links below:</p> <ul> <li>Part 1 notebook at nbviewer</li> <li>Part 2 notebook at nbviewer</li> <li>Part 3: EinMix for great MLPs at nbviewer</li> <li>Part 4: einops.pack and einops.unpack nbviewer</li> <li>Writing better code with einops and pytorch: web link</li> </ul>"},{"location":"docs/1-einops-basics/","title":"Einops tutorial, part 1: basics","text":"In\u00a0[1]: Copied! <pre># we need some libraries for this demo\n%pip install einops numpy pillow -q\n</pre> # we need some libraries for this demo %pip install einops numpy pillow -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre># Examples are given for numpy. This code also setups ipython/jupyter/jupyterlite\n# so that numpy arrays in the output are displayed as images\nimport numpy\nfrom utils import display_np_arrays_as_images\n\ndisplay_np_arrays_as_images()\n</pre> # Examples are given for numpy. This code also setups ipython/jupyter/jupyterlite # so that numpy arrays in the output are displayed as images import numpy from utils import display_np_arrays_as_images  display_np_arrays_as_images() In\u00a0[3]: Copied! <pre>ims = numpy.load(\"./resources/test_images.npy\", allow_pickle=False)\n# There are 6 images of shape 96x96 with 3 color channels packed into tensor\nprint(ims.shape, ims.dtype)\n</pre> ims = numpy.load(\"./resources/test_images.npy\", allow_pickle=False) # There are 6 images of shape 96x96 with 3 color channels packed into tensor print(ims.shape, ims.dtype) <pre>(6, 96, 96, 3) float64\n</pre> In\u00a0[4]: Copied! <pre># display the first image (whole 4d tensor can't be rendered)\nims[0]\n</pre> # display the first image (whole 4d tensor can't be rendered) ims[0] Out[4]: In\u00a0[5]: Copied! <pre># second image in a batch\nims[1]\n</pre> # second image in a batch ims[1] Out[5]: In\u00a0[6]: Copied! <pre># we'll use three operations\nfrom einops import rearrange, reduce, repeat\n</pre> # we'll use three operations from einops import rearrange, reduce, repeat In\u00a0[7]: Copied! <pre># rearrange, as the name suggests, rearranges elements\n# below we swapped height and width.\n# In other words, transposed first two axes (dimensions)\nrearrange(ims[0], \"h w c -&gt; w h c\")\n</pre> # rearrange, as the name suggests, rearranges elements # below we swapped height and width. # In other words, transposed first two axes (dimensions) rearrange(ims[0], \"h w c -&gt; w h c\") Out[7]: In\u00a0[8]: Copied! <pre># we could use more verbose names for axes, and result is the same:\nrearrange(ims[0], \"height width color -&gt; width height color\")\n# when you operate on same set of axes many times,\n# you usually come up with short names.\n# That's what we do throughout tutorial - we'll use b (for batch), h, w, and c\n</pre> # we could use more verbose names for axes, and result is the same: rearrange(ims[0], \"height width color -&gt; width height color\") # when you operate on same set of axes many times, # you usually come up with short names. # That's what we do throughout tutorial - we'll use b (for batch), h, w, and c Out[8]: In\u00a0[9]: Copied! <pre># einops allows seamlessly composing batch and height to a new height dimension\n# We just rendered all images by collapsing to 3d tensor!\nrearrange(ims, \"b h w c -&gt; (b h) w c\")\n</pre> # einops allows seamlessly composing batch and height to a new height dimension # We just rendered all images by collapsing to 3d tensor! rearrange(ims, \"b h w c -&gt; (b h) w c\") Out[9]: In\u00a0[10]: Copied! <pre># or compose a new dimension of batch and width\nrearrange(ims, \"b h w c -&gt; h (b w) c\")\n</pre> # or compose a new dimension of batch and width rearrange(ims, \"b h w c -&gt; h (b w) c\") Out[10]: In\u00a0[11]: Copied! <pre># resulting dimensions are computed very simply\n# length of newly composed axis is a product of components\n# [6, 96, 96, 3] -&gt; [96, (6 * 96), 3]\nrearrange(ims, \"b h w c -&gt; h (b w) c\").shape\n</pre> # resulting dimensions are computed very simply # length of newly composed axis is a product of components # [6, 96, 96, 3] -&gt; [96, (6 * 96), 3] rearrange(ims, \"b h w c -&gt; h (b w) c\").shape Out[11]: <pre>(96, 576, 3)</pre> In\u00a0[12]: Copied! <pre># we can compose more than two axes.\n# let's flatten 4d array into 1d, resulting array has as many elements as the original\nrearrange(ims, \"b h w c -&gt; (b h w c)\").shape\n</pre> # we can compose more than two axes. # let's flatten 4d array into 1d, resulting array has as many elements as the original rearrange(ims, \"b h w c -&gt; (b h w c)\").shape Out[12]: <pre>(165888,)</pre> In\u00a0[13]: Copied! <pre># decomposition is the inverse process - represent an axis as a combination of new axes\n# several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3\nrearrange(ims, \"(b1 b2) h w c -&gt; b1 b2 h w c \", b1=2).shape\n</pre> # decomposition is the inverse process - represent an axis as a combination of new axes # several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3 rearrange(ims, \"(b1 b2) h w c -&gt; b1 b2 h w c \", b1=2).shape Out[13]: <pre>(2, 3, 96, 96, 3)</pre> In\u00a0[14]: Copied! <pre># finally, combine composition and decomposition:\nrearrange(ims, \"(b1 b2) h w c -&gt; (b1 h) (b2 w) c \", b1=2)\n</pre> # finally, combine composition and decomposition: rearrange(ims, \"(b1 b2) h w c -&gt; (b1 h) (b2 w) c \", b1=2) Out[14]: In\u00a0[15]: Copied! <pre># slightly different composition: b1 is merged with width, b2 with height\n# ... so letters are ordered by w then by h\nrearrange(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w) c \", b1=2)\n</pre> # slightly different composition: b1 is merged with width, b2 with height # ... so letters are ordered by w then by h rearrange(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w) c \", b1=2) Out[15]: In\u00a0[16]: Copied! <pre># move part of width dimension to height.\n# we should call this width-to-height as image width shrunk by 2 and height doubled.\n# but all pixels are the same!\n# Can you write reverse operation (height-to-width)?\nrearrange(ims, \"b h (w w2) c -&gt; (h w2) (b w) c\", w2=2)\n</pre> # move part of width dimension to height. # we should call this width-to-height as image width shrunk by 2 and height doubled. # but all pixels are the same! # Can you write reverse operation (height-to-width)? rearrange(ims, \"b h (w w2) c -&gt; (h w2) (b w) c\", w2=2) Out[16]: In\u00a0[17]: Copied! <pre># compare with the next example\nrearrange(ims, \"b h w c -&gt; h (b w) c\")\n</pre> # compare with the next example rearrange(ims, \"b h w c -&gt; h (b w) c\") Out[17]: In\u00a0[18]: Copied! <pre># order of axes in composition is different\n# rule is just as for digits in the number: leftmost digit is the most significant,\n# while neighboring numbers differ in the rightmost axis.\n\n# you can also think of this as lexicographic sort\nrearrange(ims, \"b h w c -&gt; h (w b) c\")\n</pre> # order of axes in composition is different # rule is just as for digits in the number: leftmost digit is the most significant, # while neighboring numbers differ in the rightmost axis.  # you can also think of this as lexicographic sort rearrange(ims, \"b h w c -&gt; h (w b) c\") Out[18]: In\u00a0[19]: Copied! <pre># what if b1 and b2 are reordered before composing to width?\nrearrange(ims, \"(b1 b2) h w c -&gt; h (b1 b2 w) c \", b1=2)  # produces 'einops'\nrearrange(ims, \"(b1 b2) h w c -&gt; h (b2 b1 w) c \", b1=2)  # produces 'eoipns'\n</pre> # what if b1 and b2 are reordered before composing to width? rearrange(ims, \"(b1 b2) h w c -&gt; h (b1 b2 w) c \", b1=2)  # produces 'einops' rearrange(ims, \"(b1 b2) h w c -&gt; h (b2 b1 w) c \", b1=2)  # produces 'eoipns' Out[19]: In\u00a0[20]: Copied! <pre># average over batch\nreduce(ims, \"b h w c -&gt; h w c\", \"mean\")\n</pre> # average over batch reduce(ims, \"b h w c -&gt; h w c\", \"mean\") Out[20]: In\u00a0[21]: Copied! <pre># the previous is identical to familiar:\nims.mean(axis=0)\n# but is so much more readable\n</pre> # the previous is identical to familiar: ims.mean(axis=0) # but is so much more readable Out[21]: In\u00a0[22]: Copied! <pre># Example of reducing of several axes\n# besides mean, there are also min, max, sum, prod\nreduce(ims, \"b h w c -&gt; h w\", \"min\")\n</pre> # Example of reducing of several axes # besides mean, there are also min, max, sum, prod reduce(ims, \"b h w c -&gt; h w\", \"min\") Out[22]: In\u00a0[23]: Copied! <pre># this is mean-pooling with 2x2 kernel\n# image is split into 2x2 patches, each patch is averaged\nreduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"mean\", h2=2, w2=2)\n</pre> # this is mean-pooling with 2x2 kernel # image is split into 2x2 patches, each patch is averaged reduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"mean\", h2=2, w2=2) Out[23]: In\u00a0[24]: Copied! <pre># max-pooling is similar\n# result is not as smooth as for mean-pooling\nreduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"max\", h2=2, w2=2)\n</pre> # max-pooling is similar # result is not as smooth as for mean-pooling reduce(ims, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"max\", h2=2, w2=2) Out[24]: In\u00a0[25]: Copied! <pre># yet another example. Can you compute result shape?\nreduce(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w)\", \"mean\", b1=2)\n</pre> # yet another example. Can you compute result shape? reduce(ims, \"(b1 b2) h w c -&gt; (b2 h) (b1 w)\", \"mean\", b1=2) Out[25]: In\u00a0[26]: Copied! <pre># rearrange can also take care of lists of arrays with the same shape\nx = list(ims)\nprint(type(x), \"with\", len(x), \"tensors of shape\", x[0].shape)\n# that's how we can stack inputs\n# \"list axis\" becomes first (\"b\" in this case), and we left it there\nrearrange(x, \"b h w c -&gt; b h w c\").shape\n</pre> # rearrange can also take care of lists of arrays with the same shape x = list(ims) print(type(x), \"with\", len(x), \"tensors of shape\", x[0].shape) # that's how we can stack inputs # \"list axis\" becomes first (\"b\" in this case), and we left it there rearrange(x, \"b h w c -&gt; b h w c\").shape <pre>&lt;class 'list'&gt; with 6 tensors of shape (96, 96, 3)\n</pre> Out[26]: <pre>(6, 96, 96, 3)</pre> In\u00a0[27]: Copied! <pre># but new axis can appear in the other place:\nrearrange(x, \"b h w c -&gt; h w c b\").shape\n</pre> # but new axis can appear in the other place: rearrange(x, \"b h w c -&gt; h w c b\").shape Out[27]: <pre>(96, 96, 3, 6)</pre> In\u00a0[28]: Copied! <pre># that's equivalent to numpy stacking, but written more explicitly\nnumpy.array_equal(rearrange(x, \"b h w c -&gt; h w c b\"), numpy.stack(x, axis=3))\n</pre> # that's equivalent to numpy stacking, but written more explicitly numpy.array_equal(rearrange(x, \"b h w c -&gt; h w c b\"), numpy.stack(x, axis=3)) Out[28]: <pre>True</pre> In\u00a0[29]: Copied! <pre># ... or we can concatenate along axes\nrearrange(x, \"b h w c -&gt; h (b w) c\").shape\n</pre> # ... or we can concatenate along axes rearrange(x, \"b h w c -&gt; h (b w) c\").shape Out[29]: <pre>(96, 576, 3)</pre> In\u00a0[30]: Copied! <pre># which is equivalent to concatenation\nnumpy.array_equal(rearrange(x, \"b h w c -&gt; h (b w) c\"), numpy.concatenate(x, axis=1))\n</pre> # which is equivalent to concatenation numpy.array_equal(rearrange(x, \"b h w c -&gt; h (b w) c\"), numpy.concatenate(x, axis=1)) Out[30]: <pre>True</pre> In\u00a0[31]: Copied! <pre>x = rearrange(ims, \"b h w c -&gt; b 1 h w 1 c\")  # functionality of numpy.expand_dims\nprint(x.shape)\nprint(rearrange(x, \"b 1 h w 1 c -&gt; b h w c\").shape)  # functionality of numpy.squeeze\n</pre> x = rearrange(ims, \"b h w c -&gt; b 1 h w 1 c\")  # functionality of numpy.expand_dims print(x.shape) print(rearrange(x, \"b 1 h w 1 c -&gt; b h w c\").shape)  # functionality of numpy.squeeze <pre>(6, 1, 96, 96, 1, 3)\n(6, 96, 96, 3)\n</pre> In\u00a0[32]: Copied! <pre># compute max in each image individually, then show a difference\nx = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims\nrearrange(x, \"b h w c -&gt; h (b w) c\")\n</pre> # compute max in each image individually, then show a difference x = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims rearrange(x, \"b h w c -&gt; h (b w) c\") Out[32]: In\u00a0[33]: Copied! <pre># repeat along a new axis. New axis can be placed anywhere\nrepeat(ims[0], \"h w c -&gt; h new_axis w c\", new_axis=5).shape\n</pre> # repeat along a new axis. New axis can be placed anywhere repeat(ims[0], \"h w c -&gt; h new_axis w c\", new_axis=5).shape Out[33]: <pre>(96, 5, 96, 3)</pre> In\u00a0[34]: Copied! <pre># shortcut\nrepeat(ims[0], \"h w c -&gt; h 5 w c\").shape\n</pre> # shortcut repeat(ims[0], \"h w c -&gt; h 5 w c\").shape Out[34]: <pre>(96, 5, 96, 3)</pre> In\u00a0[35]: Copied! <pre># repeat along w (existing axis)\nrepeat(ims[0], \"h w c -&gt; h (repeat w) c\", repeat=3)\n</pre> # repeat along w (existing axis) repeat(ims[0], \"h w c -&gt; h (repeat w) c\", repeat=3) Out[35]: In\u00a0[36]: Copied! <pre># repeat along two existing axes\nrepeat(ims[0], \"h w c -&gt; (2 h) (2 w) c\")\n</pre> # repeat along two existing axes repeat(ims[0], \"h w c -&gt; (2 h) (2 w) c\") Out[36]: In\u00a0[37]: Copied! <pre># order of axes matters as usual - you can repeat each element (pixel) 3 times\n# by changing order in parenthesis\nrepeat(ims[0], \"h w c -&gt; h (w repeat) c\", repeat=3)\n</pre> # order of axes matters as usual - you can repeat each element (pixel) 3 times # by changing order in parenthesis repeat(ims[0], \"h w c -&gt; h (w repeat) c\", repeat=3) Out[37]: <p>Note: <code>repeat</code> operation covers functionality identical to <code>numpy.repeat</code>, <code>numpy.tile</code> and actually more than that.</p> In\u00a0[38]: Copied! <pre>repeated = repeat(ims, \"b h w c -&gt; b h new_axis w c\", new_axis=2)\nreduced = reduce(repeated, \"b h new_axis w c -&gt; b h w c\", \"min\")\nassert numpy.array_equal(ims, reduced)\n</pre> repeated = repeat(ims, \"b h w c -&gt; b h new_axis w c\", new_axis=2) reduced = reduce(repeated, \"b h new_axis w c -&gt; b h w c\", \"min\") assert numpy.array_equal(ims, reduced) In\u00a0[39]: Copied! <pre># interweaving pixels of different pictures\n# all letters are observable\nrearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (w b2) c \", b1=2)\n</pre> # interweaving pixels of different pictures # all letters are observable rearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (w b2) c \", b1=2) Out[39]: In\u00a0[40]: Copied! <pre># interweaving along vertical for couples of images\nrearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (b2 w) c\", b1=2)\n</pre> # interweaving along vertical for couples of images rearrange(ims, \"(b1 b2) h w c -&gt; (h b1) (b2 w) c\", b1=2) Out[40]: In\u00a0[41]: Copied! <pre># interweaving lines for couples of images\n# exercise: achieve the same result without einops in your favourite framework\nreduce(ims, \"(b1 b2) h w c -&gt; h (b2 w) c\", \"max\", b1=2)\n</pre> # interweaving lines for couples of images # exercise: achieve the same result without einops in your favourite framework reduce(ims, \"(b1 b2) h w c -&gt; h (b2 w) c\", \"max\", b1=2) Out[41]: In\u00a0[42]: Copied! <pre># color can be also composed into dimension\n# ... while image is downsampled\nreduce(ims, \"b (h 2) (w 2) c -&gt; (c h) (b w)\", \"mean\")\n</pre> # color can be also composed into dimension # ... while image is downsampled reduce(ims, \"b (h 2) (w 2) c -&gt; (c h) (b w)\", \"mean\") Out[42]: In\u00a0[43]: Copied! <pre># disproportionate resize\nreduce(ims, \"b (h 4) (w 3) c -&gt; (h) (b w)\", \"mean\")\n</pre> # disproportionate resize reduce(ims, \"b (h 4) (w 3) c -&gt; (h) (b w)\", \"mean\") Out[43]: In\u00a0[44]: Copied! <pre># spilt each image in two halves, compute mean of the two\nreduce(ims, \"b (h1 h2) w c -&gt; h2 (b w)\", \"mean\", h1=2)\n</pre> # spilt each image in two halves, compute mean of the two reduce(ims, \"b (h1 h2) w c -&gt; h2 (b w)\", \"mean\", h1=2) Out[44]: In\u00a0[45]: Copied! <pre># split in small patches and transpose each patch\nrearrange(ims, \"b (h1 h2) (w1 w2) c -&gt; (h1 w2) (b w1 h2) c\", h2=8, w2=8)\n</pre> # split in small patches and transpose each patch rearrange(ims, \"b (h1 h2) (w1 w2) c -&gt; (h1 w2) (b w1 h2) c\", h2=8, w2=8) Out[45]: In\u00a0[46]: Copied! <pre># stop me someone!\nrearrange(ims, \"b (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w2 h3) (b w1 h2 w3) c\", h2=2, w2=2, w3=2, h3=2)\n</pre> # stop me someone! rearrange(ims, \"b (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w2 h3) (b w1 h2 w3) c\", h2=2, w2=2, w3=2, h3=2) Out[46]: In\u00a0[47]: Copied! <pre>rearrange(ims, \"(b1 b2) (h1 h2) (w1 w2) c -&gt; (h1 b1 h2) (w1 b2 w2) c\", h1=3, w1=3, b2=3)\n</pre> rearrange(ims, \"(b1 b2) (h1 h2) (w1 w2) c -&gt; (h1 b1 h2) (w1 b2 w2) c\", h1=3, w1=3, b2=3) Out[47]: In\u00a0[48]: Copied! <pre># patterns can be arbitrarily complicated\nreduce(ims, \"(b1 b2) (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w1 h3) (b1 w2 h2 w3 b2) c\", \"mean\", h2=2, w1=2, w3=2, h3=2, b2=2)\n</pre> # patterns can be arbitrarily complicated reduce(ims, \"(b1 b2) (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w1 h3) (b1 w2 h2 w3 b2) c\", \"mean\", h2=2, w1=2, w3=2, h3=2, b2=2) Out[48]: In\u00a0[49]: Copied! <pre># subtract background in each image individually and normalize\n# pay attention to () - this is composition of 0 axis, a dummy axis with 1 element.\nim2 = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims\nim2 /= reduce(im2, \"b h w c -&gt; b () () c\", \"max\")\nrearrange(im2, \"b h w c -&gt; h (b w) c\")\n</pre> # subtract background in each image individually and normalize # pay attention to () - this is composition of 0 axis, a dummy axis with 1 element. im2 = reduce(ims, \"b h w c -&gt; b () () c\", \"max\") - ims im2 /= reduce(im2, \"b h w c -&gt; b () () c\", \"max\") rearrange(im2, \"b h w c -&gt; h (b w) c\") Out[49]: In\u00a0[50]: Copied! <pre># pixelate: first downscale by averaging, then upscale back using the same pattern\naveraged = reduce(ims, \"b (h h2) (w w2) c -&gt; b h w c\", \"mean\", h2=6, w2=8)\nrepeat(averaged, \"b h w c -&gt; (h h2) (b w w2) c\", h2=6, w2=8)\n</pre> # pixelate: first downscale by averaging, then upscale back using the same pattern averaged = reduce(ims, \"b (h h2) (w w2) c -&gt; b h w c\", \"mean\", h2=6, w2=8) repeat(averaged, \"b h w c -&gt; (h h2) (b w w2) c\", h2=6, w2=8) Out[50]: In\u00a0[51]: Copied! <pre>rearrange(ims, \"b h w c -&gt; w (b h) c\")\n</pre> rearrange(ims, \"b h w c -&gt; w (b h) c\") Out[51]: In\u00a0[52]: Copied! <pre># let's bring color dimension as part of horizontal axis\n# at the same time horizontal axis is downsampled by 2x\nreduce(ims, \"b (h h2) (w w2) c -&gt; (h w2) (b w c)\", \"mean\", h2=3, w2=3)\n</pre> # let's bring color dimension as part of horizontal axis # at the same time horizontal axis is downsampled by 2x reduce(ims, \"b (h h2) (w w2) c -&gt; (h w2) (b w c)\", \"mean\", h2=3, w2=3) Out[52]:"},{"location":"docs/1-einops-basics/#einops-tutorial-part-1-basics","title":"Einops tutorial, part 1: basics\u00b6","text":""},{"location":"docs/1-einops-basics/#welcome-to-einops-land","title":"Welcome to einops-land!\u00b6","text":"<p>We don't write</p> <pre>y = x.transpose(0, 2, 3, 1)\n</pre> <p>We write comprehensible code</p> <pre>y = rearrange(x, 'b c h w -&gt; b h w c')\n</pre> <p><code>einops</code> supports widely used tensor packages (such as <code>numpy</code>, <code>pytorch</code>, <code>jax</code>, <code>tensorflow</code>), and extends them.</p>"},{"location":"docs/1-einops-basics/#whats-in-this-tutorial","title":"What's in this tutorial?\u00b6","text":"<ul> <li>fundamentals: reordering, composition and decomposition of axes</li> <li>operations: <code>rearrange</code>, <code>reduce</code>, <code>repeat</code></li> <li>how much you can do with a single operation!</li> </ul>"},{"location":"docs/1-einops-basics/#preparations","title":"Preparations\u00b6","text":""},{"location":"docs/1-einops-basics/#load-a-batch-of-images-to-play-with","title":"Load a batch of images to play with\u00b6","text":""},{"location":"docs/1-einops-basics/#composition-of-axes","title":"Composition of axes\u00b6","text":"<p>transposition is very common and useful, but let's move to other capabilities provided by einops</p>"},{"location":"docs/1-einops-basics/#decomposition-of-axis","title":"Decomposition of axis\u00b6","text":""},{"location":"docs/1-einops-basics/#order-of-axes-matters","title":"Order of axes matters\u00b6","text":""},{"location":"docs/1-einops-basics/#meet-einopsreduce","title":"Meet einops.reduce\u00b6","text":"<p>In einops-land you don't need to guess what happened</p> <pre>x.mean(-1)\n</pre> <p>Because you write what the operation does</p> <pre>reduce(x, 'b h w c -&gt; b h w', 'mean')\n</pre> <p>if axis is not present in the output \u2014 you guessed it \u2014 axis was reduced.</p>"},{"location":"docs/1-einops-basics/#stack-and-concatenate","title":"Stack and concatenate\u00b6","text":""},{"location":"docs/1-einops-basics/#addition-or-removal-of-axes","title":"Addition or removal of axes\u00b6","text":"<p>You can write 1 to create a new axis of length 1. Similarly you can remove such axis.</p> <p>There is also a synonym <code>()</code> that you can use. That's a composition of zero axes and it also has a unit length.</p>"},{"location":"docs/1-einops-basics/#repeating-elements","title":"Repeating elements\u00b6","text":"<p>Third operation we introduce is <code>repeat</code></p>"},{"location":"docs/1-einops-basics/#reduce-repeat","title":"Reduce \u21c6 repeat\u00b6","text":"<p>reduce and repeat are like opposite of each other: first one reduces amount of elements, second one increases.</p> <p>In the following example each image is repeated first, then we reduce over new axis to get back original tensor. Notice that operation patterns are \"reverse\" of each other</p>"},{"location":"docs/1-einops-basics/#fancy-examples-in-random-order","title":"Fancy examples in random order\u00b6","text":"<p>(a.k.a. mad designer gallery)</p>"},{"location":"docs/1-einops-basics/#ok-numpy-is-fun-but-how-do-i-use-einops-with-some-other-framework","title":"Ok, numpy is fun, but how do I use einops with some other framework?\u00b6","text":"<p>If that's what you've done with <code>ims</code> being numpy array:</p> <pre>rearrange(ims, 'b h w c -&gt; w (b h) c')\n</pre> <p>That's how you adapt the code for other frameworks:</p> <pre># pytorch:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# tensorflow:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# gluon:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# cupy:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# jax:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n# paddle:\nrearrange(ims, 'b h w c -&gt; w (b h) c')\n\n...well, you got the idea.\n</pre> <p>Einops allows backpropagation as if all operations were native to framework. Operations do not change when moving to another framework - einops notation is universal</p>"},{"location":"docs/1-einops-basics/#summary","title":"Summary\u00b6","text":"<ul> <li><p><code>rearrange</code> doesn't change number of elements and covers different numpy functions (like <code>transpose</code>, <code>reshape</code>, <code>stack</code>, <code>concatenate</code>,  <code>squeeze</code> and <code>expand_dims</code>)</p> </li> <li><p><code>reduce</code> combines same reordering syntax with reductions (<code>mean</code>, <code>min</code>, <code>max</code>, <code>sum</code>, <code>prod</code>, and any others)</p> </li> <li><p><code>repeat</code> additionally covers repeating and tiling</p> </li> <li><p>composition and decomposition of axes are a corner stone, they can and should be used together</p> </li> <li><p>Second part of tutorial shows how einops works with other frameworks</p> </li> <li><p>Third part of tutorial shows how to improve your DL code with einops</p> </li> </ul>"},{"location":"docs/2-einops-for-deep-learning/","title":"Einops tutorial, part 2: deep learning","text":"In\u00a0[1]: Copied! <pre>from einops import rearrange, reduce\n</pre> from einops import rearrange, reduce In\u00a0[2]: Copied! <pre>import numpy as np\n\nx = np.random.RandomState(42).normal(size=[10, 32, 100, 200])\n</pre> import numpy as np  x = np.random.RandomState(42).normal(size=[10, 32, 100, 200]) In\u00a0[3]: Copied! <pre># utility to hide answers\nfrom utils import guess\n</pre> # utility to hide answers from utils import guess In\u00a0[4]: Copied! <pre># select \"tensorflow\" or \"pytorch\"\nflavour = \"pytorch\"\n</pre> # select \"tensorflow\" or \"pytorch\" flavour = \"pytorch\" In\u00a0[5]: Copied! <pre>print(f\"selected {flavour} backend\")\nif flavour == \"tensorflow\":\n    import tensorflow as tf\n\n    tape = tf.GradientTape(persistent=True)\n    tape.__enter__()\n    x = tf.Variable(x) + 0\nelse:\n    assert flavour == \"pytorch\"\n    import torch\n\n    x = torch.from_numpy(x)\n    x.requires_grad = True\n</pre> print(f\"selected {flavour} backend\") if flavour == \"tensorflow\":     import tensorflow as tf      tape = tf.GradientTape(persistent=True)     tape.__enter__()     x = tf.Variable(x) + 0 else:     assert flavour == \"pytorch\"     import torch      x = torch.from_numpy(x)     x.requires_grad = True <pre>selected pytorch backend\n</pre> In\u00a0[6]: Copied! <pre>type(x), x.shape\n</pre> type(x), x.shape Out[6]: <pre>(torch.Tensor, torch.Size([10, 32, 100, 200]))</pre> In\u00a0[7]: Copied! <pre>y = rearrange(x, \"b c h w -&gt; b h w c\")\nguess(y.shape)\n</pre> y = rearrange(x, \"b c h w -&gt; b h w c\") guess(y.shape) Answer is: (10, 100, 200, 32) (hover to see) In\u00a0[8]: Copied! <pre>y0 = x\ny1 = reduce(y0, \"b c h w -&gt; b c\", \"max\")\ny2 = rearrange(y1, \"b c -&gt; c b\")\ny3 = reduce(y2, \"c b -&gt; \", \"sum\")\n\nif flavour == \"tensorflow\":\n    print(reduce(tape.gradient(y3, x), \"b c h w -&gt; \", \"sum\"))\nelse:\n    y3.backward()\n    print(reduce(x.grad, \"b c h w -&gt; \", \"sum\"))\n</pre> y0 = x y1 = reduce(y0, \"b c h w -&gt; b c\", \"max\") y2 = rearrange(y1, \"b c -&gt; c b\") y3 = reduce(y2, \"c b -&gt; \", \"sum\")  if flavour == \"tensorflow\":     print(reduce(tape.gradient(y3, x), \"b c h w -&gt; \", \"sum\")) else:     y3.backward()     print(reduce(x.grad, \"b c h w -&gt; \", \"sum\")) <pre>tensor(320., dtype=torch.float64)\n</pre> In\u00a0[9]: Copied! <pre>from einops import asnumpy\n\ny3_numpy = asnumpy(y3)\n\nprint(type(y3_numpy))\n</pre> from einops import asnumpy  y3_numpy = asnumpy(y3)  print(type(y3_numpy)) <pre>&lt;class 'numpy.ndarray'&gt;\n</pre> In\u00a0[10]: Copied! <pre>y = rearrange(x, \"b c h w -&gt; b (c h w)\")\nguess(y.shape)\n</pre> y = rearrange(x, \"b c h w -&gt; b (c h w)\") guess(y.shape) Answer is: (10, 640000) (hover to see) <p>space-to-depth</p> In\u00a0[11]: Copied! <pre>y = rearrange(x, \"b c (h h1) (w w1) -&gt; b (h1 w1 c) h w\", h1=2, w1=2)\nguess(y.shape)\n</pre> y = rearrange(x, \"b c (h h1) (w w1) -&gt; b (h1 w1 c) h w\", h1=2, w1=2) guess(y.shape) Answer is: (10, 128, 50, 100) (hover to see) <p>depth-to-space (notice that it's reverse of the previous)</p> In\u00a0[12]: Copied! <pre>y = rearrange(x, \"b (h1 w1 c) h w -&gt; b c (h h1) (w w1)\", h1=2, w1=2)\nguess(y.shape)\n</pre> y = rearrange(x, \"b (h1 w1 c) h w -&gt; b c (h h1) (w w1)\", h1=2, w1=2) guess(y.shape) Answer is: (10, 8, 200, 400) (hover to see) In\u00a0[13]: Copied! <pre>y = reduce(x, \"b c h w -&gt; b c\", reduction=\"mean\")\nguess(y.shape)\n</pre> y = reduce(x, \"b c h w -&gt; b c\", reduction=\"mean\") guess(y.shape) Answer is: (10, 32) (hover to see) <p>max-pooling with a kernel 2x2</p> In\u00a0[14]: Copied! <pre>y = reduce(x, \"b c (h h1) (w w1) -&gt; b c h w\", reduction=\"max\", h1=2, w1=2)\nguess(y.shape)\n</pre> y = reduce(x, \"b c (h h1) (w w1) -&gt; b c h w\", reduction=\"max\", h1=2, w1=2) guess(y.shape) Answer is: (10, 32, 50, 100) (hover to see) In\u00a0[15]: Copied! <pre># you can skip names for reduced axes\ny = reduce(x, \"b c (h 2) (w 2) -&gt; b c h w\", reduction=\"max\")\nguess(y.shape)\n</pre> # you can skip names for reduced axes y = reduce(x, \"b c (h 2) (w 2) -&gt; b c h w\", reduction=\"max\") guess(y.shape) Answer is: (10, 32, 50, 100) (hover to see) In\u00a0[16]: Copied! <pre># models typically work only with batches,\n# so to predict a single image ...\nimage = rearrange(x[0, :3], \"c h w -&gt; h w c\")\n# ... create a dummy 1-element axis ...\ny = rearrange(image, \"h w c -&gt; () c h w\")\n# ... imagine you predicted this with a convolutional network for classification,\n# we'll just flatten axes ...\npredictions = rearrange(y, \"b c h w -&gt; b (c h w)\")\n# ... finally, decompose (remove) dummy axis\npredictions = rearrange(predictions, \"() classes -&gt; classes\")\n</pre> # models typically work only with batches, # so to predict a single image ... image = rearrange(x[0, :3], \"c h w -&gt; h w c\") # ... create a dummy 1-element axis ... y = rearrange(image, \"h w c -&gt; () c h w\") # ... imagine you predicted this with a convolutional network for classification, # we'll just flatten axes ... predictions = rearrange(y, \"b c h w -&gt; b (c h w)\") # ... finally, decompose (remove) dummy axis predictions = rearrange(predictions, \"() classes -&gt; classes\") In\u00a0[17]: Copied! <pre>y = x - reduce(x, \"b c h w -&gt; b c 1 1\", \"mean\")\nguess(y.shape)\n</pre> y = x - reduce(x, \"b c h w -&gt; b c 1 1\", \"mean\") guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) <p>per-channel mean-normalization for whole batch:</p> In\u00a0[18]: Copied! <pre>y = x - reduce(y, \"b c h w -&gt; 1 c 1 1\", \"mean\")\nguess(y.shape)\n</pre> y = x - reduce(y, \"b c h w -&gt; 1 c 1 1\", \"mean\") guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) In\u00a0[19]: Copied! <pre>list_of_tensors = list(x)\n</pre> list_of_tensors = list(x) <p>New axis (one that enumerates tensors) appears first on the left side of expression. Just as if you were indexing list - first you'd get tensor by index</p> In\u00a0[20]: Copied! <pre>tensors = rearrange(list_of_tensors, \"b c h w -&gt; b h w c\")\nguess(tensors.shape)\n</pre> tensors = rearrange(list_of_tensors, \"b c h w -&gt; b h w c\") guess(tensors.shape) Answer is: (10, 100, 200, 32) (hover to see) In\u00a0[21]: Copied! <pre># or maybe stack along last dimension?\ntensors = rearrange(list_of_tensors, \"b c h w -&gt; h w c b\")\nguess(tensors.shape)\n</pre> # or maybe stack along last dimension? tensors = rearrange(list_of_tensors, \"b c h w -&gt; h w c b\") guess(tensors.shape) Answer is: (100, 200, 32, 10) (hover to see) In\u00a0[22]: Copied! <pre>tensors = rearrange(list_of_tensors, \"b c h w -&gt; (b h) w c\")\nguess(tensors.shape)\n</pre> tensors = rearrange(list_of_tensors, \"b c h w -&gt; (b h) w c\") guess(tensors.shape) Answer is: (1000, 200, 32) (hover to see) <p>or maybe concatenate along last dimension?</p> In\u00a0[23]: Copied! <pre>tensors = rearrange(list_of_tensors, \"b c h w -&gt; h w (b c)\")\nguess(tensors.shape)\n</pre> tensors = rearrange(list_of_tensors, \"b c h w -&gt; h w (b c)\") guess(tensors.shape) Answer is: (100, 200, 320) (hover to see) In\u00a0[24]: Copied! <pre>y = rearrange(x, \"b (g1 g2 c) h w-&gt; b (g2 g1 c) h w\", g1=4, g2=4)\nguess(y.shape)\n</pre> y = rearrange(x, \"b (g1 g2 c) h w-&gt; b (g2 g1 c) h w\", g1=4, g2=4) guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) <p>simpler version of channel shuffle</p> In\u00a0[25]: Copied! <pre>y = rearrange(x, \"b (g c) h w-&gt; b (c g) h w\", g=4)\nguess(y.shape)\n</pre> y = rearrange(x, \"b (g c) h w-&gt; b (c g) h w\", g=4) guess(y.shape) Answer is: (10, 32, 100, 200) (hover to see) In\u00a0[26]: Copied! <pre>bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, \"b (coord bbox) h w -&gt; coord b bbox h w\", coord=4, bbox=8)\n# now you can operate on individual variables\nmax_bbox_area = reduce(bbox_w * bbox_h, \"b bbox h w -&gt; b h w\", \"max\")\nguess(bbox_x.shape)\nguess(max_bbox_area.shape)\n</pre> bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, \"b (coord bbox) h w -&gt; coord b bbox h w\", coord=4, bbox=8) # now you can operate on individual variables max_bbox_area = reduce(bbox_w * bbox_h, \"b bbox h w -&gt; b h w\", \"max\") guess(bbox_x.shape) guess(max_bbox_area.shape) Answer is: (10, 8, 100, 200) (hover to see) Answer is: (10, 100, 200) (hover to see) In\u00a0[27]: Copied! <pre>from einops import parse_shape\n</pre> from einops import parse_shape In\u00a0[28]: Copied! <pre>def convolve_2d(x):\n    # imagine we have a simple 2d convolution with padding,\n    # so output has same shape as input.\n    # Sorry for laziness, use imagination!\n    return x\n</pre> def convolve_2d(x):     # imagine we have a simple 2d convolution with padding,     # so output has same shape as input.     # Sorry for laziness, use imagination!     return x In\u00a0[29]: Copied! <pre># imagine we are working with 3d data\nx_5d = rearrange(x, \"b c x (y z) -&gt; b c x y z\", z=20)\n# but we have only 2d convolutions.\n# That's not a problem, since we can apply\ny = rearrange(x_5d, \"b c x y z -&gt; (b z) c x y\")\ny = convolve_2d(y)\n# not just specifies additional information, but verifies that all dimensions match\ny = rearrange(y, \"(b z) c x y -&gt; b c x y z\", **parse_shape(x_5d, \"b c x y z\"))\n</pre> # imagine we are working with 3d data x_5d = rearrange(x, \"b c x (y z) -&gt; b c x y z\", z=20) # but we have only 2d convolutions. # That's not a problem, since we can apply y = rearrange(x_5d, \"b c x y z -&gt; (b z) c x y\") y = convolve_2d(y) # not just specifies additional information, but verifies that all dimensions match y = rearrange(y, \"(b z) c x y -&gt; b c x y z\", **parse_shape(x_5d, \"b c x y z\")) In\u00a0[30]: Copied! <pre>parse_shape(x_5d, \"b c x y z\")\n</pre> parse_shape(x_5d, \"b c x y z\") Out[30]: <pre>{'b': 10, 'c': 32, 'x': 100, 'y': 10, 'z': 20}</pre> In\u00a0[31]: Copied! <pre># we can skip some dimensions by writing underscore\nparse_shape(x_5d, \"batch c _ _ _\")\n</pre> # we can skip some dimensions by writing underscore parse_shape(x_5d, \"batch c _ _ _\") Out[31]: <pre>{'batch': 10, 'c': 32}</pre> In\u00a0[32]: Copied! <pre># each image is split into subgrids, each subgrid now is a separate \"image\"\ny = rearrange(x, \"b c (h hs) (w ws) -&gt; (hs ws b) c h w\", hs=2, ws=2)\ny = convolve_2d(y)\n# pack subgrids back to an image\ny = rearrange(y, \"(hs ws b) c h w -&gt; b c (h hs) (w ws)\", hs=2, ws=2)\n\nassert y.shape == x.shape\n</pre> # each image is split into subgrids, each subgrid now is a separate \"image\" y = rearrange(x, \"b c (h hs) (w ws) -&gt; (hs ws b) c h w\", hs=2, ws=2) y = convolve_2d(y) # pack subgrids back to an image y = rearrange(y, \"(hs ws b) c h w -&gt; b c (h hs) (w ws)\", hs=2, ws=2)  assert y.shape == x.shape"},{"location":"docs/2-einops-for-deep-learning/#einops-tutorial-part-2-deep-learning","title":"Einops tutorial, part 2: deep learning\u00b6","text":"<p>Previous part of tutorial provides visual examples with numpy.</p>"},{"location":"docs/2-einops-for-deep-learning/#whats-in-this-tutorial","title":"What's in this tutorial?\u00b6","text":"<ul> <li>working with deep learning packages</li> <li>important cases for deep learning models</li> <li><code>einops.asnumpy</code> and <code>einops.layers</code></li> </ul>"},{"location":"docs/2-einops-for-deep-learning/#select-your-flavour","title":"Select your flavour\u00b6","text":"<p>Switch to the framework you're most comfortable with.</p>"},{"location":"docs/2-einops-for-deep-learning/#simple-computations","title":"Simple computations\u00b6","text":"<ul> <li>converting bchw to bhwc format and back is a common operation in CV</li> <li>try to predict output shape and then check your guess!</li> </ul>"},{"location":"docs/2-einops-for-deep-learning/#worked","title":"Worked!\u00b6","text":"<p>Did you notice? Code above worked for you backend of choice.  Einops functions work with any tensor like they are native to the framework.</p>"},{"location":"docs/2-einops-for-deep-learning/#backpropagation","title":"Backpropagation\u00b6","text":"<ul> <li>gradients are a corner stone of deep learning</li> <li>You can back-propagate through einops operations  (just as with framework native operations)</li> </ul>"},{"location":"docs/2-einops-for-deep-learning/#meet-einopsasnumpy","title":"Meet <code>einops.asnumpy</code>\u00b6","text":"<p>Just converts tensors to numpy (and pulls from gpu if necessary)</p>"},{"location":"docs/2-einops-for-deep-learning/#common-building-blocks-of-deep-learning","title":"Common building blocks of deep learning\u00b6","text":"<p>Let's check how some familiar operations can be written with <code>einops</code></p> <p>Flattening is common operation, frequently appears at the boundary between convolutional layers and fully connected layers</p>"},{"location":"docs/2-einops-for-deep-learning/#reductions","title":"Reductions\u00b6","text":"<p>Simple global average pooling.</p>"},{"location":"docs/2-einops-for-deep-learning/#1d-2d-and-3d-pooling-are-defined-in-a-similar-way","title":"1d, 2d and 3d pooling are defined in a similar way\u00b6","text":"<p>for sequential 1-d models, you'll probably want pooling over time</p> <pre>reduce(x, '(t 2) b c -&gt; t b c', reduction='max')\n</pre> <p>for volumetric models, all three dimensions are pooled</p> <pre>reduce(x, 'b c (x 2) (y 2) (z 2) -&gt; b c x y z', reduction='max')\n</pre> <p>Uniformity is a strong point of <code>einops</code>, and you don't need specific operation for each particular case.</p>"},{"location":"docs/2-einops-for-deep-learning/#good-exercises","title":"Good exercises\u00b6","text":"<ul> <li>write a version of space-to-depth for 1d and 3d (2d is provided above)</li> <li>write an average / max pooling for 1d models.</li> </ul>"},{"location":"docs/2-einops-for-deep-learning/#squeeze-and-unsqueeze-expand_dims","title":"Squeeze and unsqueeze (expand_dims)\u00b6","text":""},{"location":"docs/2-einops-for-deep-learning/#keepdims-like-behavior-for-reductions","title":"keepdims-like behavior for reductions\u00b6","text":"<ul> <li>empty composition <code>()</code> provides dimensions of length 1, which are broadcastable.</li> <li>alternatively, you can use just <code>1</code> to introduce new axis, that's a synonym to <code>()</code></li> </ul> <p>per-channel mean-normalization for each image:</p>"},{"location":"docs/2-einops-for-deep-learning/#stacking","title":"Stacking\u00b6","text":"<p>let's take a list of tensors</p>"},{"location":"docs/2-einops-for-deep-learning/#concatenation","title":"Concatenation\u00b6","text":"<p>concatenate over the first dimension?</p>"},{"location":"docs/2-einops-for-deep-learning/#shuffling-within-a-dimension","title":"Shuffling within a dimension\u00b6","text":"<p>channel shuffle (as it is drawn in shufflenet paper)</p>"},{"location":"docs/2-einops-for-deep-learning/#split-a-dimension","title":"Split a dimension\u00b6","text":"<p>Here's a super-convenient trick.</p> <p>Example: when a network predicts several bboxes for each position</p> <p>Assume we got 8 bboxes, 4 coordinates each.  To get coordinated into 4 separate variables, you move corresponding dimension to front and unpack tuple.</p>"},{"location":"docs/2-einops-for-deep-learning/#getting-into-the-weeds-of-tensor-packing","title":"Getting into the weeds of tensor packing\u00b6","text":"<p>you can skip this part - it explains why taking a habit of defining splits and packs explicitly</p> <p>when implementing custom gated activation (like GLU), split is needed:</p> <pre>y1, y2 = rearrange(x, 'b (split c) h w -&gt; split b c h w', split=2)\nresult = y2 * sigmoid(y2) # or tanh\n</pre> <p>... but we could split differently</p> <pre>y1, y2 = rearrange(x, 'b (c split) h w -&gt; split b c h w', split=2)\n</pre> <ul> <li>first one splits channels into consequent groups: <code>y1 = x[:, :x.shape[1] // 2, :, :]</code></li> <li>while second takes channels with a step: <code>y1 = x[:, 0::2, :, :]</code></li> </ul> <p>This may drive to very surprising results when input is</p> <ul> <li>a result of group convolution</li> <li>a result of bidirectional LSTM/RNN</li> <li>multi-head attention</li> </ul> <p>Let's focus on the second case (LSTM/RNN), since it is less obvious.</p> <p>For instance, cudnn concatenates LSTM outputs for forward-in-time and backward-in-time</p> <p>Also in pytorch GLU splits channels into consequent groups (first way) So when LSTM's output comes to GLU,</p> <ul> <li>forward-in-time produces linear part, and backward-in-time produces activation ...</li> <li>and role of directions is different, and gradients coming to two parts are different<ul> <li>that's not what you expect from simple <code>GLU(BLSTM(x))</code>, right?</li> </ul> </li> </ul> <p><code>einops</code> notation makes such inconsistencies explicit and easy-detectable</p>"},{"location":"docs/2-einops-for-deep-learning/#shape-parsing","title":"Shape parsing\u00b6","text":"<p>just a handy utility</p>"},{"location":"docs/2-einops-for-deep-learning/#striding-anything","title":"Striding anything\u00b6","text":"<p>Finally, how to convert any operation into a strided operation?  (like convolution with strides, aka dilated/atrous convolution)</p>"},{"location":"docs/2-einops-for-deep-learning/#layers","title":"Layers\u00b6","text":"<p>For frameworks that prefer operating with layers, layers are available.</p> <p>You'll need to import a proper one depending on your backend:</p> <pre>from einops.layers.torch import Rearrange, Reduce\nfrom einops.layers.flax import Rearrange, Reduce\nfrom einops.layers.tensorflow import Rearrange, Reduce\nfrom einops.layers.chainer import Rearrange, Reduce\n</pre> <p><code>Einops</code> layers are identical to operations, and have same parameters.  (for the exception of first argument, which should be passed during call)</p> <pre>layer = Rearrange(pattern, **axes_lengths)\nlayer = Reduce(pattern, reduction, **axes_lengths)\n\n# apply layer to tensor\nx = layer(x)\n</pre> <p>Usually it is more convenient to use layers, not operations, to build models</p> <pre># example given for pytorch, but code in other frameworks is almost identical\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Reduce\n\nmodel = Sequential(\n    Conv2d(3, 6, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    Conv2d(6, 16, kernel_size=5),\n    # combined pooling and flattening in a single step\n    Reduce('b c (h 2) (w 2) -&gt; b (c h w)', 'max'), \n    Linear(16*5*5, 120), \n    ReLU(),\n    Linear(120, 10), \n    # In flax, the {'axis': value} syntax for specifying values for axes is mandatory:\n    # Rearrange('(b1 b2) d -&gt; b1 b2 d', {'b1': 12}), \n)\n</pre>"},{"location":"docs/2-einops-for-deep-learning/#whats-now","title":"What's now?\u00b6","text":"<ul> <li>rush through writing better code with einops+pytorch</li> </ul> <p>Use different framework? Not a big issue, most recommendations transfer well to other frameworks.  <code>einops</code> works the same way in any framework.</p> <p>Finally - just write your code with einops!</p>"},{"location":"docs/3-einmix-layer/","title":"EinMix: universal toolkit for advanced MLP architectures","text":"In\u00a0[\u00a0]: Copied! <pre>from einops.layers.torch import EinMix as Mix\n\n# tutorial uses torch. EinMix is available for other frameworks too\nfrom torch import nn\nfrom torch.nn import functional as F\n</pre> from einops.layers.torch import EinMix as Mix  # tutorial uses torch. EinMix is available for other frameworks too from torch import nn from torch.nn import functional as F <p>Logic of EinMix is very close to the one of <code>einsum</code>. If you're not familiar with einsum, follow these guides first:</p> <ul> <li>https://rockt.github.io/2018/04/30/einsum</li> <li>https://towardsdatascience.com/einsum-an-underestimated-function-99ca96e2942e</li> </ul> <p>Einsum uniformly describes a number of operations. <code>EinMix</code> is a layer (not function) implementing a similar logic, it has some differences with <code>einsum</code>.</p> <p>Let's implement simple linear layer using einsum</p> <pre>weight = &lt;...create and initialize parameter...&gt;\nbias = &lt;...create and initialize parameter...&gt;\nresult = torch.einsum('tbc,cd-&gt;tbd', embeddings, weight) + bias\n</pre> <p>EinMix counter-part is:</p> <pre>mix_channels = Mix('t b c -&gt; t b c_out', weight_shape='c c_out', bias_shape='c_out', ...)\nresult = mix_channels(embeddings)\n</pre> <p>Main differences compared to plain <code>einsum</code> are:</p> <ul> <li>layer takes care of the parameter initialization &amp; management</li> <li>weight is not in the comprehension</li> <li>EinMix includes bias term</li> </ul> <p>We'll discuss other changes a bit later, now let's implement some elements from MLPMixer.</p> In\u00a0[2]: Copied! <pre>class MLP(nn.Module):\n    def __init__(self, num_features, expansion_factor, dropout):\n        super().__init__()\n        num_hidden = num_features * expansion_factor\n        self.fc1 = nn.Linear(num_features, num_hidden)\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(num_hidden, num_features)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.dropout1(F.gelu(self.fc1(x)))\n        x = self.dropout2(self.fc2(x))\n        return x\n\n\nclass TokenMixer(nn.Module):\n    def __init__(self, num_features, num_patches, expansion_factor, dropout):\n        super().__init__()\n        self.norm = nn.LayerNorm(num_features)\n        self.mlp = MLP(num_patches, expansion_factor, dropout)\n\n    def forward(self, x):\n        # x.shape == (batch_size, num_patches, num_features)\n        residual = x\n        x = self.norm(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_features, num_patches)\n        x = self.mlp(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_patches, num_features)\n        out = x + residual\n        return out\n</pre> class MLP(nn.Module):     def __init__(self, num_features, expansion_factor, dropout):         super().__init__()         num_hidden = num_features * expansion_factor         self.fc1 = nn.Linear(num_features, num_hidden)         self.dropout1 = nn.Dropout(dropout)         self.fc2 = nn.Linear(num_hidden, num_features)         self.dropout2 = nn.Dropout(dropout)      def forward(self, x):         x = self.dropout1(F.gelu(self.fc1(x)))         x = self.dropout2(self.fc2(x))         return x   class TokenMixer(nn.Module):     def __init__(self, num_features, num_patches, expansion_factor, dropout):         super().__init__()         self.norm = nn.LayerNorm(num_features)         self.mlp = MLP(num_patches, expansion_factor, dropout)      def forward(self, x):         # x.shape == (batch_size, num_patches, num_features)         residual = x         x = self.norm(x)         x = x.transpose(1, 2)         # x.shape == (batch_size, num_features, num_patches)         x = self.mlp(x)         x = x.transpose(1, 2)         # x.shape == (batch_size, num_patches, num_features)         out = x + residual         return out In\u00a0[3]: Copied! <pre>def TokenMixer(num_features: int, n_patches: int, expansion_factor: int, dropout: float):\n    n_hidden = n_patches * expansion_factor\n    return nn.Sequential(\n        nn.LayerNorm(num_features),\n        Mix(\"b hw c -&gt; b hid c\", weight_shape=\"hw hid\", bias_shape=\"hid\", hw=n_patches, hidden=n_hidden),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        Mix(\"b hid c -&gt; b hw c\", weight_shape=\"hid hw\", bias_shape=\"hw\",  hw=n_patches, hidden=n_hidden),\n        nn.Dropout(dropout),\n    )\n</pre> def TokenMixer(num_features: int, n_patches: int, expansion_factor: int, dropout: float):     n_hidden = n_patches * expansion_factor     return nn.Sequential(         nn.LayerNorm(num_features),         Mix(\"b hw c -&gt; b hid c\", weight_shape=\"hw hid\", bias_shape=\"hid\", hw=n_patches, hidden=n_hidden),         nn.GELU(),         nn.Dropout(dropout),         Mix(\"b hid c -&gt; b hw c\", weight_shape=\"hid hw\", bias_shape=\"hw\",  hw=n_patches, hidden=n_hidden),         nn.Dropout(dropout),     ) <p>You may also check another implementation of MLPMixer from Phil Wang.  Phil solves the issue by repurposing <code>nn.Conv1d</code> to mix on the second dimension. Hacky, but does the job</p> In\u00a0[4]: Copied! <pre>def check_sizes(image_size, patch_size):\n    sqrt_num_patches, remainder = divmod(image_size, patch_size)\n    assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"\n    num_patches = sqrt_num_patches ** 2\n    return num_patches\n\nclass Patcher(nn.Module):\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=16,\n        in_channels=3,\n        num_features=128,\n    ):\n        _num_patches = check_sizes(image_size, patch_size)\n        super().__init__()\n        # per-patch fully-connected is equivalent to strided conv2d\n        self.patcher = nn.Conv2d(\n            in_channels, num_features, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        patches = self.patcher(x)\n        batch_size, num_features, _, _ = patches.shape\n        patches = patches.permute(0, 2, 3, 1)\n        patches = patches.view(batch_size, -1, num_features)\n\n        return patches\n</pre> def check_sizes(image_size, patch_size):     sqrt_num_patches, remainder = divmod(image_size, patch_size)     assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"     num_patches = sqrt_num_patches ** 2     return num_patches  class Patcher(nn.Module):     def __init__(         self,         image_size=256,         patch_size=16,         in_channels=3,         num_features=128,     ):         _num_patches = check_sizes(image_size, patch_size)         super().__init__()         # per-patch fully-connected is equivalent to strided conv2d         self.patcher = nn.Conv2d(             in_channels, num_features, kernel_size=patch_size, stride=patch_size         )      def forward(self, x):         patches = self.patcher(x)         batch_size, num_features, _, _ = patches.shape         patches = patches.permute(0, 2, 3, 1)         patches = patches.view(batch_size, -1, num_features)          return patches In\u00a0[5]: Copied! <pre>def patcher(patch_size=16, in_channels=3, num_features=128):\n    return Mix(\"b c_in (h hp) (w wp) -&gt; b (h w) c\", weight_shape=\"c_in hp wp c\", bias_shape=\"c\",\n               c=num_features, hp=patch_size, wp=patch_size, c_in=in_channels)\n</pre> def patcher(patch_size=16, in_channels=3, num_features=128):     return Mix(\"b c_in (h hp) (w wp) -&gt; b (h w) c\", weight_shape=\"c_in hp wp c\", bias_shape=\"c\",                c=num_features, hp=patch_size, wp=patch_size, c_in=in_channels) In\u00a0[6]: Copied! <pre>class WeightedPermuteMLP(nn.Module):\n    def __init__(self, H, W, C, S):\n        super().__init__()\n\n        self.proj_h = nn.Linear(H * S, H * S)\n        self.proj_w = nn.Linear(W * S, W * S)\n        self.proj_c = nn.Linear(C, C)\n        self.proj = nn.Linear(C, C)\n        self.S = S\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        S = self.S\n        N = C // S\n        x_h = x.reshape(B, H, W, N, S).permute(0, 3, 2, 1, 4).reshape(B, N, W, H*S)\n        x_h = self.proj_h(x_h).reshape(B, N, W, H, S).permute(0, 3, 2, 1, 4).reshape(B, H, W, C)\n\n        x_w = x.reshape(B, H, W, N, S).permute(0, 1, 3, 2, 4).reshape(B, H, N, W*S)\n        x_w = self.proj_w(x_w).reshape(B, H, N, W, S).permute(0, 1, 3, 2, 4).reshape(B, H, W, C)\n\n        x_c = self.proj_c(x)\n\n        x = x_h + x_w + x_c\n        x = self.proj(x)\n        return x\n</pre> class WeightedPermuteMLP(nn.Module):     def __init__(self, H, W, C, S):         super().__init__()          self.proj_h = nn.Linear(H * S, H * S)         self.proj_w = nn.Linear(W * S, W * S)         self.proj_c = nn.Linear(C, C)         self.proj = nn.Linear(C, C)         self.S = S      def forward(self, x):         B, H, W, C = x.shape         S = self.S         N = C // S         x_h = x.reshape(B, H, W, N, S).permute(0, 3, 2, 1, 4).reshape(B, N, W, H*S)         x_h = self.proj_h(x_h).reshape(B, N, W, H, S).permute(0, 3, 2, 1, 4).reshape(B, H, W, C)          x_w = x.reshape(B, H, W, N, S).permute(0, 1, 3, 2, 4).reshape(B, H, N, W*S)         x_w = self.proj_w(x_w).reshape(B, H, N, W, S).permute(0, 1, 3, 2, 4).reshape(B, H, W, C)          x_c = self.proj_c(x)          x = x_h + x_w + x_c         x = self.proj(x)         return x <p>That didn't look readable, right?</p> <p>This code is also very inflexible: code in the paper did not support batch dimension, and multiple changes were necessary to allow batch processing.  This process is fragile and easily can result in virtually uncatchable bugs.</p> <p>Now good news: each of these long method chains can be replaced with a single <code>EinMix</code> layer:</p> In\u00a0[7]: Copied! <pre>class WeightedPermuteMLP_new(nn.Module):\n    def __init__(self, H, W, C, seg_len):\n        super().__init__()\n        assert C % seg_len == 0, f\"can't divide {C} into segments of length {seg_len}\"\n        self.mlp_c = Mix(\"b h w c -&gt; b h w c0\", weight_shape=\"c c0\", bias_shape=\"c0\",\n                         c=C, c0=C)\n        self.mlp_h = Mix(\"b h w (n c) -&gt; b h0 w (n c0)\", weight_shape=\"h c h0 c0\", bias_shape=\"h0 c0\",\n                         h=H, h0=H, c=seg_len, c0=seg_len)\n        self.mlp_w = Mix(\"b h w (n c) -&gt; b h w0 (n c0)\", weight_shape=\"w c w0 c0\", bias_shape=\"w0 c0\",\n                         w=W, w0=W, c=seg_len, c0=seg_len)\n        self.proj = nn.Linear(C, C)\n\n    def forward(self, x):\n        x = self.mlp_c(x) + self.mlp_h(x) + self.mlp_w(x)\n        return self.proj(x)\n</pre> class WeightedPermuteMLP_new(nn.Module):     def __init__(self, H, W, C, seg_len):         super().__init__()         assert C % seg_len == 0, f\"can't divide {C} into segments of length {seg_len}\"         self.mlp_c = Mix(\"b h w c -&gt; b h w c0\", weight_shape=\"c c0\", bias_shape=\"c0\",                          c=C, c0=C)         self.mlp_h = Mix(\"b h w (n c) -&gt; b h0 w (n c0)\", weight_shape=\"h c h0 c0\", bias_shape=\"h0 c0\",                          h=H, h0=H, c=seg_len, c0=seg_len)         self.mlp_w = Mix(\"b h w (n c) -&gt; b h w0 (n c0)\", weight_shape=\"w c w0 c0\", bias_shape=\"w0 c0\",                          w=W, w0=W, c=seg_len, c0=seg_len)         self.proj = nn.Linear(C, C)      def forward(self, x):         x = self.mlp_c(x) + self.mlp_h(x) + self.mlp_w(x)         return self.proj(x) In\u00a0[10]: Copied! <pre>class MultiheadAttention(nn.Module):\n    def __init__(self, dim_input, n_heads, head_dim):\n        super().__init__()\n        self.input_to_qkv = Mix(\"b t c -&gt; qkv b h t hid\", \"c qkv h hid\",\n                                c=dim_input, qkv=3, h=n_heads, hid=head_dim)\n        self.out_proj = Mix(\"b h t hid -&gt; b t c\", \"h hid c\",\n                            h=n_heads, hid=head_dim, c=dim_input)\n\n    def forward(self, x):\n        q, k, v = self.input_to_qkv(x) # fused projections, computed in one go\n        return self.out_proj(F.scaled_dot_product_attention(q, k, v)) # flash attention\n</pre> class MultiheadAttention(nn.Module):     def __init__(self, dim_input, n_heads, head_dim):         super().__init__()         self.input_to_qkv = Mix(\"b t c -&gt; qkv b h t hid\", \"c qkv h hid\",                                 c=dim_input, qkv=3, h=n_heads, hid=head_dim)         self.out_proj = Mix(\"b h t hid -&gt; b t c\", \"h hid c\",                             h=n_heads, hid=head_dim, c=dim_input)      def forward(self, x):         q, k, v = self.input_to_qkv(x) # fused projections, computed in one go         return self.out_proj(F.scaled_dot_product_attention(q, k, v)) # flash attention"},{"location":"docs/3-einmix-layer/#einmix-universal-toolkit-for-advanced-mlp-architectures","title":"EinMix: universal toolkit for advanced MLP architectures\u00b6","text":"<p>Recent progress in MLP-based architectures demonstrated that very specific MLPs can compete with convnets and transformers (and even outperform them).</p> <p>EinMix allows writing such architectures in a more uniform and readable way.</p>"},{"location":"docs/3-einmix-layer/#einmix-building-block-of-mlps","title":"EinMix \u2014 building block of MLPs\u00b6","text":""},{"location":"docs/3-einmix-layer/#tokenmixer-from-mlpmixer-original-code","title":"TokenMixer from MLPMixer \u2014 original code\u00b6","text":"<p>We start from pytorch implementation of MLPMixer by Jake Tae.</p> <p>We'll focus on two components of MLPMixer that don't exist in convnets. First component is TokenMixer:</p>"},{"location":"docs/3-einmix-layer/#tokenmixer-from-mlpmixer-reimplemented","title":"TokenMixer from MLPMixer \u2014 reimplemented\u00b6","text":"<p>We can significantly reduce amount of code by using <code>EinMix</code>.</p> <ul> <li>Main caveat addressed by original code is that <code>nn.Linear</code> mixes only last axis. <code>EinMix</code> can mix any axis (or set of axes).</li> <li>Sequential structure is always preferred as it is easier to follow</li> <li>Intentionally there is no residual connection in <code>TokenMixer</code>, because honestly it's not work of Mixer and should be done by caller</li> </ul>"},{"location":"docs/3-einmix-layer/#mlpmixers-patch-embeddings-aka-vit-patch-embeddings-original","title":"MLPMixer's patch embeddings (aka ViT patch embeddings) \u2014 original\u00b6","text":"<p>Second interesting part of MLPMixer is derived from vision transformers.</p> <p>In the very beginning an image is split into patches, and each patch is linearly projected into embedding:</p>"},{"location":"docs/3-einmix-layer/#mlpmixers-patch-embeddings-reimplemented","title":"MLPMixer's patch embeddings \u2014 reimplemented\u00b6","text":"<p><code>EinMix</code> does this in a single operation. This may require some training at first to understand.</p> <p>Let's go step-by-step:</p> <ul> <li><code>b c_in (h hp) (w wp) -&gt;</code> - 4-dimensional input tensor (BCHW-ordered) is split into patches of shape <code>hp x wp</code></li> <li><code>weight_shape='c_in hp wp c'</code>. Axes <code>c_in</code>, <code>hp</code> and <code>wp</code> are all absent in the output: three dimensional patch tensor was mixed to produce a vector of length <code>c</code></li> <li><code>-&gt; b (h w) c</code> - output is 3-dimensional. All patches were reorganized from <code>h x w</code> grid to one-dimensional sequence of vectors</li> </ul> <p>We don't need to provide image_size beforehead, new implementation handles images of different dimensions as long as they can be divided into patches</p>"},{"location":"docs/3-einmix-layer/#vision-permutator","title":"Vision Permutator\u00b6","text":"<p>As a third example we consider pytorch-like code from ViP paper.</p> <p>Vision permutator is only slightly more nuanced than previous models, because</p> <ol> <li>it operates on spatial dimensions separately, while MLPMixer and its friends just pack all spatial info into one axis.</li> <li>it splits channels into groups called 'segments'</li> </ol>"},{"location":"docs/3-einmix-layer/#multi-head-attention-once-again","title":"Multi-head attention, once again\u00b6","text":"<p>EinMix can be (mis)used to compute multiple projections and perform transpositions along the way.</p> <p>For example, F.scaled_dot_product_attention wants a specific order of axes, and an explicit head axis. We can combine linear projection with providing desired order of arguments in a single operation.</p>"},{"location":"docs/3-einmix-layer/#exercises","title":"Exercises\u00b6","text":"<ol> <li><p>Many normalizations (batch norm, layer norm, etc) use affine scaling afterwards. Implement this scaling using <code>EinMix</code>.</p> </li> <li><p>let's assume you have an input tensor of shape <code>[b, t, n_groups, n_channels]</code>, and you want to apply a separate linear layer to every group of channels.</p> <p>This will introduce <code>n_groups</code> matrices of shape <code>[n_channels, n_channels]</code> and <code>n_groups</code> biases of shape <code>[n_channels]</code>. Can you perform this operation with just one <code>EinMix</code>?</p> </li> </ol>"},{"location":"docs/3-einmix-layer/#final-remarks","title":"Final remarks\u00b6","text":"<p><code>EinMix</code> helps with MLPs that don't fit into a limited 'mix all in the last axis' paradigm, and specially helpful for non-1d inputs (images, videos, etc).</p> <p>However existing research does not cover real possibilities of densely connected architectures.</p> <p>Most of its systematic novelty is \"mix along spatial axes actually works\". But <code>EinMix</code> provides an astonishing amount of other possibilities!. Let me mention some examples:</p>"},{"location":"docs/3-einmix-layer/#mixing-within-a-patch-on-a-grid","title":"Mixing within a patch on a grid\u00b6","text":"<p>What if you make mixing 'local' in space? Completely doable:</p> <pre>'b c (h hI) (w wI) -&gt; b c (h hO) (w wO)', weight_shape='c hI wI hO wO'\n</pre> <p>We split tensor into patches of shape <code>hI wI</code> and mixed per-channel.</p>"},{"location":"docs/3-einmix-layer/#mixing-in-subgrids","title":"Mixing in subgrids\u00b6","text":"<p>Opposite question: how to collect information from the whole image (without attention)? </p> <p>Well, you can again densely connect all the tokens, but all-to-all connection can be too expensive.</p> <p>Here is EinMix-way: split the image into subgrids (each subgrid has steps <code>h</code> and <code>w</code>), and connect densely tokens within each subgrid</p> <pre>'b c (hI h) (wI w) -&gt; b c (hO h) (wO w)', weight_shape='c hI wI hO wO'\n</pre>"},{"location":"docs/3-einmix-layer/#going-deeper","title":"Going deeper\u00b6","text":"<p>And that's very top of the iceberg.</p> <ul> <li>Want to mix part of axis? \u2014 No problems!</li> <li>... in a grid-like manner \u2014 Supported!</li> <li>... while mixing channels within group? \u2014 Welcome!</li> <li>In 2d/3d/4d? \u2014 Sure!</li> <li>Don't use pytorch? \u2014 EinMix is available for multiple frameworks!</li> </ul> <p>Hopefully this guide helped you to find MLPs more interesting and intriguing. And simpler to experiment with.</p>"},{"location":"docs/4-pack-and-unpack/","title":"einops.pack and einops.unpack","text":"In\u00a0[1]: Copied! <pre># install necessary libraries\n%pip install numpy einops -q\n</pre> # install necessary libraries %pip install numpy einops -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre># we'll use numpy for demo purposes\n# operations work the same way with other frameworks\nimport numpy as np\n</pre> # we'll use numpy for demo purposes # operations work the same way with other frameworks import numpy as np In\u00a0[3]: Copied! <pre>from einops import pack, unpack\n\nh, w = 100, 200\n# image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w)\nimage_rgb = np.random.random([h, w, 3])\nimage_depth = np.random.random([h, w])\n# but we can stack them\nimage_rgbd, ps = pack([image_rgb, image_depth], \"h w *\")\n</pre> from einops import pack, unpack  h, w = 100, 200 # image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w) image_rgb = np.random.random([h, w, 3]) image_depth = np.random.random([h, w]) # but we can stack them image_rgbd, ps = pack([image_rgb, image_depth], \"h w *\") In\u00a0[4]: Copied! <pre># as you see, pack properly appended depth as one more layer\n# and correctly aligned axes!\n# this won't work off the shelf with np.concatenate or torch.cat or alike\nimage_rgb.shape, image_depth.shape, image_rgbd.shape\n</pre> # as you see, pack properly appended depth as one more layer # and correctly aligned axes! # this won't work off the shelf with np.concatenate or torch.cat or alike image_rgb.shape, image_depth.shape, image_rgbd.shape Out[4]: <pre>((100, 200, 3), (100, 200), (100, 200, 4))</pre> In\u00a0[5]: Copied! <pre># now let's see what PS keeps.\n# PS means Packed Shapes, not PlayStation or Post Script\nps\n</pre> # now let's see what PS keeps. # PS means Packed Shapes, not PlayStation or Post Script ps Out[5]: <pre>[(3,), ()]</pre> <p>which reads: first tensor had shape <code>h, w, *and 3*</code>, while second tensor had shape <code>h, w *and nothing more*</code>. That's just enough to reverse packing:</p> In\u00a0[6]: Copied! <pre># remove 1-axis in depth image during unpacking. Results are (h, w, 3) and (h, w)\nunpacked_rgb, unpacked_depth = unpack(image_rgbd, ps, \"h w *\")\nunpacked_rgb.shape, unpacked_depth.shape\n</pre> # remove 1-axis in depth image during unpacking. Results are (h, w, 3) and (h, w) unpacked_rgb, unpacked_depth = unpack(image_rgbd, ps, \"h w *\") unpacked_rgb.shape, unpacked_depth.shape Out[6]: <pre>((100, 200, 3), (100, 200))</pre> <p>we can unpack tensor in different ways manually:</p> In\u00a0[7]: Copied! <pre># simple unpack by splitting the axis. Results are (h, w, 3) and (h, w, 1)\nrgb, depth = unpack(image_rgbd, [[3], [1]], \"h w *\")\n# different split, both outputs have shape (h, w, 2)\nrg, bd = unpack(image_rgbd, [[2], [2]], \"h w *\")\n# unpack to 4 tensors of shape (h, w). More like 'unstack over last axis'\n[r, g, b, d] = unpack(image_rgbd, [[], [], [], []], \"h w *\")\n</pre> # simple unpack by splitting the axis. Results are (h, w, 3) and (h, w, 1) rgb, depth = unpack(image_rgbd, [[3], [1]], \"h w *\") # different split, both outputs have shape (h, w, 2) rg, bd = unpack(image_rgbd, [[2], [2]], \"h w *\") # unpack to 4 tensors of shape (h, w). More like 'unstack over last axis' [r, g, b, d] = unpack(image_rgbd, [[], [], [], []], \"h w *\") In\u00a0[8]: Copied! <pre>from einops import reduce\ndef image_classifier(images_bhwc):\n    # mock for image classifier\n    predictions = reduce(images_bhwc, \"b h w c -&gt; b c\", \"mean\", h=100, w=200, c=3)\n    return predictions\n\n\ndef universal_predict(x):\n    x_packed, ps = pack([x], \"* h w c\")\n    predictions_packed = image_classifier(x_packed)\n    [predictions] = unpack(predictions_packed, ps, \"* cls\")\n    return predictions\n</pre> from einops import reduce def image_classifier(images_bhwc):     # mock for image classifier     predictions = reduce(images_bhwc, \"b h w c -&gt; b c\", \"mean\", h=100, w=200, c=3)     return predictions   def universal_predict(x):     x_packed, ps = pack([x], \"* h w c\")     predictions_packed = image_classifier(x_packed)     [predictions] = unpack(predictions_packed, ps, \"* cls\")     return predictions In\u00a0[9]: Copied! <pre># works with a single image\nprint(universal_predict(np.zeros([h, w, 3])).shape)\n# works with a batch of images\nbatch = 5\nprint(universal_predict(np.zeros([batch, h, w, 3])).shape)\n# or even a batch of videos\nn_frames = 7\nprint(universal_predict(np.zeros([batch, n_frames, h, w, 3])).shape)\n</pre> # works with a single image print(universal_predict(np.zeros([h, w, 3])).shape) # works with a batch of images batch = 5 print(universal_predict(np.zeros([batch, h, w, 3])).shape) # or even a batch of videos n_frames = 7 print(universal_predict(np.zeros([batch, n_frames, h, w, 3])).shape) <pre>(3,)\n(5, 3)\n(5, 7, 3)\n</pre> <p>what we can learn from this example:</p> <ul> <li><code>pack</code> and <code>unpack</code> play nicely together. That's not a coincidence :)</li> <li>patterns in <code>pack</code> and <code>unpack</code> may differ, and that's quite common for applications</li> <li>unlike other operations in <code>einops</code>, <code>(un)pack</code> does not provide arbitrary reordering of axes</li> </ul> In\u00a0[10]: Copied! <pre>def transformer_mock(x_btc):\n    # imagine this is a transformer model, a very efficient one\n    assert len(x_btc.shape) == 3\n    return x_btc\n</pre> def transformer_mock(x_btc):     # imagine this is a transformer model, a very efficient one     assert len(x_btc.shape) == 3     return x_btc <p>Let's implement vision transformer (ViT) with a class token (i.e. static token, corresponding output is used to classify an image)</p> In\u00a0[11]: Copied! <pre># below it is assumed that you already\n# 1) split batch of images into patches 2) applied linear projection and 3) used positional embedding.\n\n# We'll skip that here. But hey, here is an einops-style way of doing all of that in a single shot!\n# from einops.layers.torch import EinMix\n# patcher_and_posembedder = EinMix('b (h h2) (w w2) c -&gt; b h w c_out', weight_shape='h2 w2 c c_out',\n#                                  bias_shape='h w c_out', h2=..., w2=...)\n# patch_tokens_bhwc = patcher_and_posembedder(images_bhwc)\n</pre> # below it is assumed that you already # 1) split batch of images into patches 2) applied linear projection and 3) used positional embedding.  # We'll skip that here. But hey, here is an einops-style way of doing all of that in a single shot! # from einops.layers.torch import EinMix # patcher_and_posembedder = EinMix('b (h h2) (w w2) c -&gt; b h w c_out', weight_shape='h2 w2 c c_out', #                                  bias_shape='h w c_out', h2=..., w2=...) # patch_tokens_bhwc = patcher_and_posembedder(images_bhwc) In\u00a0[12]: Copied! <pre># preparations\nbatch, height, width, c = 6, 16, 16, 256\npatch_tokens = np.random.random([batch, height, width, c])\nclass_tokens = np.zeros([batch, c])\n</pre> # preparations batch, height, width, c = 6, 16, 16, 256 patch_tokens = np.random.random([batch, height, width, c]) class_tokens = np.zeros([batch, c]) In\u00a0[13]: Copied! <pre>def vit_einops(class_tokens, patch_tokens):\n    input_packed, ps = pack([class_tokens, patch_tokens], \"b * c\")\n    output_packed = transformer_mock(input_packed)\n    return unpack(output_packed, ps, \"b * c_out\")\n\nclass_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)\n\nclass_token_emb.shape, patch_tokens_emb.shape\n</pre> def vit_einops(class_tokens, patch_tokens):     input_packed, ps = pack([class_tokens, patch_tokens], \"b * c\")     output_packed = transformer_mock(input_packed)     return unpack(output_packed, ps, \"b * c_out\")  class_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)  class_token_emb.shape, patch_tokens_emb.shape Out[13]: <pre>((6, 256), (6, 16, 16, 256))</pre> <p>At this point, let's make a small pause and understand conveniences of this pipeline, by contrasting it to more 'standard' code</p> In\u00a0[14]: Copied! <pre>def vit_vanilla(class_tokens, patch_tokens):\n    b, h, w, c = patch_tokens.shape\n    class_tokens_b1c = class_tokens[:, np.newaxis, :]\n    patch_tokens_btc = np.reshape(patch_tokens, [b, -1, c])\n    input_packed = np.concatenate([class_tokens_b1c, patch_tokens_btc], axis=1)\n    output_packed = transformer_mock(input_packed)\n    class_token_emb = np.squeeze(output_packed[:, :1, :], 1)\n    patch_tokens_emb = np.reshape(output_packed[:, 1:, :], [b, h, w, -1])\n    return class_token_emb, patch_tokens_emb\n\nclass_token_emb2, patch_tokens_emb2 = vit_vanilla(class_tokens, patch_tokens)\nassert np.allclose(class_token_emb, class_token_emb2)\nassert np.allclose(patch_tokens_emb, patch_tokens_emb2)\n</pre> def vit_vanilla(class_tokens, patch_tokens):     b, h, w, c = patch_tokens.shape     class_tokens_b1c = class_tokens[:, np.newaxis, :]     patch_tokens_btc = np.reshape(patch_tokens, [b, -1, c])     input_packed = np.concatenate([class_tokens_b1c, patch_tokens_btc], axis=1)     output_packed = transformer_mock(input_packed)     class_token_emb = np.squeeze(output_packed[:, :1, :], 1)     patch_tokens_emb = np.reshape(output_packed[:, 1:, :], [b, h, w, -1])     return class_token_emb, patch_tokens_emb  class_token_emb2, patch_tokens_emb2 = vit_vanilla(class_tokens, patch_tokens) assert np.allclose(class_token_emb, class_token_emb2) assert np.allclose(patch_tokens_emb, patch_tokens_emb2) <p>Notably, we have put all packing and unpacking, reshapes, adding and removing of dummy axes into a couple of lines.</p> In\u00a0[15]: Copied! <pre>def loss_detection(model_output_bhwc, mask_h: int, mask_w: int, n_classes: int):\n    output = model_output_bhwc\n\n    confidence = output[..., 0].sigmoid()\n    bbox_x_shift = output[..., 1].sigmoid()\n    bbox_y_shift = output[..., 2].sigmoid()\n    bbox_w = output[..., 3]\n    bbox_h = output[..., 4]\n    mask_logits = output[..., 5: 5 + mask_h * mask_w]\n    mask_logits = mask_logits.reshape([*mask_logits.shape[:-1], mask_h, mask_w])\n    class_logits = output[..., 5 + mask_h * mask_w:]\n    assert class_logits.shape[-1] == n_classes, class_logits.shape[-1]\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n</pre> def loss_detection(model_output_bhwc, mask_h: int, mask_w: int, n_classes: int):     output = model_output_bhwc      confidence = output[..., 0].sigmoid()     bbox_x_shift = output[..., 1].sigmoid()     bbox_y_shift = output[..., 2].sigmoid()     bbox_w = output[..., 3]     bbox_h = output[..., 4]     mask_logits = output[..., 5: 5 + mask_h * mask_w]     mask_logits = mask_logits.reshape([*mask_logits.shape[:-1], mask_h, mask_w])     class_logits = output[..., 5 + mask_h * mask_w:]     assert class_logits.shape[-1] == n_classes, class_logits.shape[-1]      # downstream computations     return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits <p>When the same logic is implemented in einops, there is no need to memorize offsets.  Additionally, reshapes and shape checks are automatic:</p> In\u00a0[16]: Copied! <pre>def loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):\n    confidence, bbox_x_shift, bbox_y_shift, bbox_w, bbox_h, mask_logits, class_logits \\\n        = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], \"b h w *\")\n\n    confidence = confidence.sigmoid()\n    bbox_x_shift = bbox_x_shift.sigmoid()\n    bbox_y_shift = bbox_y_shift.sigmoid()\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n</pre> def loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):     confidence, bbox_x_shift, bbox_y_shift, bbox_w, bbox_h, mask_logits, class_logits \\         = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], \"b h w *\")      confidence = confidence.sigmoid()     bbox_x_shift = bbox_x_shift.sigmoid()     bbox_y_shift = bbox_y_shift.sigmoid()      # downstream computations     return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits In\u00a0[17]: Copied! <pre># check that results are identical\nimport torch\ndims = dict(mask_h=6, mask_w=8, n_classes=19)\nmodel_output = torch.randn([3, 5, 7, 5 + dims[\"mask_h\"] * dims[\"mask_w\"] + dims[\"n_classes\"]])\nfor a, b in zip(loss_detection(model_output, **dims), loss_detection_einops(model_output, **dims)):\n    assert torch.allclose(a, b)\n</pre> # check that results are identical import torch dims = dict(mask_h=6, mask_w=8, n_classes=19) model_output = torch.randn([3, 5, 7, 5 + dims[\"mask_h\"] * dims[\"mask_w\"] + dims[\"n_classes\"]]) for a, b in zip(loss_detection(model_output, **dims), loss_detection_einops(model_output, **dims)):     assert torch.allclose(a, b) <p>Or maybe reinforcement learning is closer to your mind?</p> <p>If so, predicting multiple outputs is valuable there too:</p> <pre>action_logits, reward_expectation, q_values, expected_entropy_after_action = \\\n    unpack(predictions_btc, [[n_actions], [], [n_actions], [n_actions]], 'b step *')\n</pre>"},{"location":"docs/4-pack-and-unpack/#einopspack-and-einopsunpack","title":"einops.pack and einops.unpack\u00b6","text":"<p>einops 0.6 introduces two more functions to the family: <code>pack</code> and <code>unpack</code>.</p> <p>Here is what they do:</p> <ul> <li><code>unpack</code> reverses <code>pack</code></li> <li><code>pack</code> reverses <code>unpack</code></li> </ul> <p>Enlightened with this exhaustive description, let's move to examples.</p>"},{"location":"docs/4-pack-and-unpack/#stacking-data-layers","title":"Stacking data layers\u00b6","text":"<p>Assume we have RGB image along with a corresponding depth image that we want to stack:</p>"},{"location":"docs/4-pack-and-unpack/#how-to-read-packing-patterns","title":"How to read packing patterns\u00b6","text":"<p>pattern <code>h w *</code> means that</p> <ul> <li>output is 3-dimensional</li> <li>first two axes (<code>h</code> and <code>w</code>) are shared across all inputs and also shared with output</li> <li>inputs, however do not have to be 3-dimensional. They can be 2-dim, 3-dim, 4-dim, etc.  Regardless of inputs dimensionality, they all will be packed into 3-dim output, and information about how they were packed is stored in <code>PS</code></li> </ul>"},{"location":"docs/4-pack-and-unpack/#short-summary-so-far","title":"Short summary so far\u00b6","text":"<ul> <li><code>einops.pack</code> is a 'more generic concatenation' (that can stack too)</li> <li><code>einops.unpack</code> is a 'more generic split'</li> </ul> <p>And, of course, <code>einops</code> functions are more verbose, and reversing concatenation now is dead simple</p> <p>Compared to other <code>einops</code> functions, <code>pack</code> and <code>unpack</code> have a compact pattern without arrow, and the same pattern can be used in <code>pack</code> and <code>unpack</code>. These patterns are very simplistic: just a sequence of space-separated axes names. One axis is <code>*</code>, all other axes are valid identifiers.</p> <p>Now let's discuss some practical cases</p>"},{"location":"docs/4-pack-and-unpack/#auto-batching","title":"Auto-batching\u00b6","text":"<p>ML models by default accept batches: batch of images, or batch of sentences, or batch of audios, etc.</p> <p>During debugging or inference, however, it is common to pass a single image instead (and thus output should be a single prediction)  In this example we'll write <code>universal_predict</code> that can handle both cases.</p>"},{"location":"docs/4-pack-and-unpack/#class-token-in-vit","title":"Class token in VIT\u00b6","text":"<p>Let's assume we have a simple transformer model that works with <code>BTC</code>-shaped tensors.</p>"},{"location":"docs/4-pack-and-unpack/#packing-different-modalities-together","title":"Packing different modalities together\u00b6","text":"<p>We can extend the previous example: it is quite common to mix elements of different types of inputs in transformers.</p> <p>The simples one is to mix tokens from all inputs:</p> <pre>all_inputs = [text_tokens_btc, image_bhwc, task_token_bc, static_tokens_bnc]\ninputs_packed, ps = pack(all_inputs, 'b * c')\n</pre> <p>and you can <code>unpack</code> resulting tokens to the same structure.</p>"},{"location":"docs/4-pack-and-unpack/#packing-data-coming-from-different-sources-together","title":"Packing data coming from different sources together\u00b6","text":"<p>Most notable example is of course GANs:</p> <pre>input_ims, ps = pack([true_images, fake_images], '* h w c')\ntrue_pred, fake_pred = unpack(model(input_ims), ps, '* c')\n</pre> <p><code>true_pred</code> and <code>fake_pred</code> are handled differently, that's why we separated them</p>"},{"location":"docs/4-pack-and-unpack/#predicting-multiple-outputs-at-the-same-time","title":"Predicting multiple outputs at the same time\u00b6","text":"<p>It is quite common to pack prediction of multiple target values into a single layer.</p> <p>This is more efficient, but code is less readable. For example, that's how detection code may look like:</p>"},{"location":"docs/4-pack-and-unpack/#thats-all-for-today","title":"That's all for today!\u00b6","text":"<p>happy packing and unpacking!</p>"},{"location":"docs/utils/__init__/","title":"init","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom IPython import get_ipython\nfrom IPython.display import display_html\nfrom PIL.Image import fromarray\n</pre> import numpy as np from IPython import get_ipython from IPython.display import display_html from PIL.Image import fromarray In\u00a0[\u00a0]: Copied! <pre>def display_np_arrays_as_images():\n    def np_to_png(a):\n        if 2 &lt;= len(a.shape) &lt;= 3:\n            return fromarray(np.array(np.clip(a, 0, 1) * 255, dtype=\"uint8\"))._repr_png_()\n        else:\n            return fromarray(np.zeros([1, 1], dtype=\"uint8\"))._repr_png_()\n\n    def np_to_text(obj, p, cycle):\n        if len(obj.shape) &lt; 2:\n            print(repr(obj))\n        if 2 &lt;= len(obj.shape) &lt;= 3:\n            pass\n        else:\n            print(f\"&lt;array of shape {obj.shape}&gt;\")\n\n    get_ipython().display_formatter.formatters[\"image/png\"].for_type(np.ndarray, np_to_png)\n    get_ipython().display_formatter.formatters[\"text/plain\"].for_type(np.ndarray, np_to_text)\n</pre> def display_np_arrays_as_images():     def np_to_png(a):         if 2 &lt;= len(a.shape) &lt;= 3:             return fromarray(np.array(np.clip(a, 0, 1) * 255, dtype=\"uint8\"))._repr_png_()         else:             return fromarray(np.zeros([1, 1], dtype=\"uint8\"))._repr_png_()      def np_to_text(obj, p, cycle):         if len(obj.shape) &lt; 2:             print(repr(obj))         if 2 &lt;= len(obj.shape) &lt;= 3:             pass         else:             print(f\"\")      get_ipython().display_formatter.formatters[\"image/png\"].for_type(np.ndarray, np_to_png)     get_ipython().display_formatter.formatters[\"text/plain\"].for_type(np.ndarray, np_to_text) In\u00a0[\u00a0]: Copied! <pre>_style_inline = \"\"\"&lt;style&gt;\n.einops-answer {\n    color: transparent;\n    padding: 5px 15px;\n    background-color: #def;\n}\n.einops-answer:hover { color: blue; }\n&lt;/style&gt;\n\"\"\"\n</pre> _style_inline = \"\"\" \"\"\" In\u00a0[\u00a0]: Copied! <pre>def guess(x):\n    display_html(\n        _style_inline + f\"&lt;h4&gt;Answer is: &lt;span class='einops-answer'&gt;{tuple(x)}&lt;/span&gt; (hover to see)&lt;/h4&gt;\",\n        raw=True,\n    )\n</pre> def guess(x):     display_html(         _style_inline + f\"Answer is: {tuple(x)} (hover to see)\",         raw=True,     )"},{"location":"pages/projects/","title":"Community/Ecosystem","text":""},{"location":"pages/projects/#tools-for-einops-users","title":"Tools for einops users","text":"<ul> <li>Ein Color \u2014 a VS Code extension to color axes in einops/einsum patterns</li> <li>Sonar codechecker provides a rule to statically check einops patterns: </li> </ul>"},{"location":"pages/projects/#selected-projects-implemented-with-einops","title":"Selected projects implemented with einops","text":"<p>Einops tutorials cover many common usages (cover tutorials first!), but it is also useful to see real projects that apply einops in practice. The projects below illustrate how einops can simplify code in various domains.</p> <ul> <li> <p>@lucidrains has a dramatic collection of vision transformers</p> <ul> <li>there is a plenty of good examples how to use einops efficiently in your projects</li> </ul> </li> <li> <p>lambda networks (non-conventional architecture) implemented by @lucidrains</p> <ul> <li>nice demonstration how clearer code can be with einops, even compared to description in the paper </li> <li>implementation and video</li> </ul> </li> <li> <p>capsule networks (aka capsnets) implemented in einops</p> <ul> <li>this implementation is blazingly fast, concise (3-10 times less code), and memory efficient</li> </ul> </li> <li> <p>NuX \u2014 normalizing flows in Jax</p> <ul> <li>different rearrangement patterns in normalizing flows have nice mapping to einops</li> </ul> </li> <li> <p>For video recognition, look at MotionFormer    and TimeSFormer implementations</p> </li> <li> <p>For protein folding, see alphafold3-pytorch and implementation of invariant point attention from AlphaFold 2</p> </li> </ul>"},{"location":"pages/projects/#community-introductions-to-einops","title":"Community introductions to einops","text":"<p>Tutorial in the AI summer about einops and einsum: https://theaisummer.com/einsum-attention/</p> <p>Introduction to einops by Kapil Sachdeva https://www.youtube.com/watch?v=xGy75Pjsqzo</p> <p>Implementing visual transformer in pytorch: https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632</p> <p>Refactoring machine learning code, one of posts in a series is devoted to einops: https://www.paepper.com/blog/posts/refactoring-machine-learning-code-einops/</p> <p>ML TLDR thread on einops: https://twitter.com/mlsummaries/status/1400505282543955970</p> <p>Book \"Deep Reinforcement Learning in Action\" by Brandon Brown &amp; Alexander Zai contains an introduction into einops in chapter 10.</p>"},{"location":"pages/projects/#related-projects","title":"Related projects","text":"<ul> <li>numpy.einsum \u2014 grand-dad of einops, this operation is now available in all mainstream DL frameworks </li> <li>einops in Rust language: https://docs.rs/einops/0.1.0/einops</li> <li>einops in C++ for torch: https://github.com/dorpxam/einops-cpp</li> <li>tensorcast in Julia language: https://juliahub.com/ui/Packages/TensorCast</li> <li>one-to-one einops implementation in Julia language: https://murrellgroup.github.io/Einops.jl/stable/</li> <li>einops in R language: https://qile0317.github.io/einops/</li> <li>for those chasing an extreme compactness of API, https://github.com/cgarciae/einop provides 'one op', as the name suggests</li> <li>https://github.com/fferflo/einx goes in opposite direction and creates einops-style operation for anything</li> </ul>"},{"location":"pages/testimonials/","title":"Testimonials","text":"<p>Einops was created three years ago, and never hit big ML news. However, little by little and step by step it sneaked into every major AI lab.</p> <p>This all happened by word of mouth and by sharing code snippets:</p> <p>Einops simplifies and clarifies array/tensor manipulation. \ud83d\udc4d  You really have to try it out, you'll love it: https://github.com/arogozhnikov/einops  Plus it supports NumPy, TensorFlow, PyTorch, and more.</p> <p>Aur\u00e9lien Geron,  author of \"Hands-On Machine Learning with Scikit-Learn and TensorFlow.\"   Former PM of YouTube video classification. (ref)</p> <p>This is your daily reminder to never trust pytorch's native reshape/view.   Always use einops! </p> <p>(Just spend 1 h debugging code and it turned out tensor.view was shuffling the tensor in a weird way) </p> <p>Tom Lieberum, University of Amsterdam (ref)</p> <p>TIL einops can be faster than raw PyTorch \ud83e\udd2f</p> <p>Zach Mueller, Novetta, author of \"walk with fastai\" (ref)</p> <p>einops are also a joy to work with!</p> <p>Norman Casagrande, Deepmind (ref)</p> <p>\u2014 And btw I estimate that AI research suffers globally from a 5% loss of productivity because einops are not included in  @PyTorch by default.</p> <p>\u2014 Might be true for research, but not likely to be true for engineering. I don't think a lot of people in the industry use PyTorch directly [...]</p> <p>\u2014 That\u2019s why it\u2019s 5% and not 30%</p> <p>\u2014 E-xa-ctly</p> <p>Discussion thread</p> <p>After a while, it feels like einsum+einops is all you need ;) [...]</p> <p>Tim Rockt\u00e4schel, Facebook AI Research  (ref)</p> <p>Yes, I am also using einops in that snippet! It's great!   I wished I knew about it from the start when it was created</p> <p>Tim Dettmers, PhD Student at UoW and visiting researcher at Facebook AI (ref)</p> <p>A little late to the party, but einops (https://github.com/arogozhnikov/einops) is a massive improvement to deep learning code readability. I love this!</p> <p>Daniel Havir, Apple (ref)</p> <p>I recently discovered the beauty of torch.einsum and einops.rearrange   and at this point I'm confused why I even bothered with other tensor operations in the first place.</p> <p>Robin M. Schmidt, AI/ML Resident at Apple (ref)</p> <p>I end up using einops for ridiculously simple things,  simply to be nice to my future self (because the code takes so little effort to read).</p> <p>Nasim Rahaman, MILA (ref)</p> <p>I love Einops for this kind of stuff, it makes code very readable,   even if you are just doing a simple squeeze or expand_dims. [...]</p> <p>Cristian Garcia, ML Engineer @quansightai, (ref)</p> <p>i might be late to the party, but einsum and the einops package are unbelievably useful</p> <p>Samson Koelle, Stat PhD candidate at UoW, (ref)</p> <p>I really recommend einops for tensor shape manipulation</p> <p>Alex Mordvintsev, DeepDream creator, (ref)</p> <p>The einops Python package is worth checking out:  it provides a powerful declarative interface  for manipulating and reshaping multi-dimensional arrays https://github.com/arogozhnikov/einops</p> <p>Jake VanderPlas,   Google Research, core contributor to Altair, AstroML, scikit-learn, etc. (ref)</p> <p>I can't believe after this many years of programming with NumPy/PyTorch/TensorFlow, I didn't know about \ud835\ude8e\ud835\ude92\ud835\ude97\ud835\ude9c\ud835\ude9e\ud835\ude96. [...]  \u2014 Arash Vahdat</p> <p>They smell like regexp to me -- concise, but I know it is going to take effort to understand or modify them in the future.   \u2014 John Carmack</p> <p>I have a much easier time to read einsum than any equivalent combinations of matmul, reshape, broadcasting... you name it. Regexps are ad-hoc, subtle and cryptic.</p> <p>Einstein summation is uniform, succinct with simple, clear semantics.  Ein sum to rule them all ;)  \u2014 Christian Szegedy</p> <p>The einops library, in particular, is magical. Best thing since baked bread and brewed beer.  \u2014 @aertherks</p> <p>Discussion thread </p>"}]}